{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import sklearn.model_selection\n",
    "import sklearn.dummy\n",
    "import sklearn.metrics\n",
    "import sklearn.neural_network\n",
    "import sklearn.ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetch the data frames from the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read both the matches and the frames\n",
    "training_df = pd.read_csv('../data/processed/diff_train.csv').drop(labels=['tier','Unnamed: 0'],axis=1)\n",
    "test_df = pd.read_csv('../data/processed/diff_test.csv').drop(labels=['tier','Unnamed: 0'],axis=1)\n",
    "# Reinterpret all values as int32s\n",
    "training_df = training_df.astype({\n",
    "    'winner': 'int32',\n",
    "    'first_kill': 'int32',\n",
    "    'first_tower': 'int32',\n",
    "    'first_inhibitor': 'int32',\n",
    "    'first_baron': 'int32',\n",
    "    'first_dragon': 'int32',\n",
    "    'first_rift_herald': 'int32',\n",
    "})\n",
    "test_df = test_df.astype({\n",
    "    'winner': 'int32',\n",
    "    'first_kill': 'int32',\n",
    "    'first_tower': 'int32',\n",
    "    'first_inhibitor': 'int32',\n",
    "    'first_baron': 'int32',\n",
    "    'first_dragon': 'int32',\n",
    "    'first_rift_herald': 'int32',\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions to format the data prior to training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_data(X, y):\n",
    "    \"\"\"Plots the data from Exercise 1\"\"\"\n",
    "    plt.scatter(*X[y==-1].T, marker=\"x\", c=\"r\")\n",
    "    plt.scatter(*X[y==1].T, marker=\"x\", c=\"b\")\n",
    "    plt.xlim(0,1)\n",
    "    plt.ylim(0,1)\n",
    "    plt.gca().set_aspect('equal')  \n",
    "\n",
    "def plot_decision_function(model):\n",
    "    \"\"\"\n",
    "    Plots the decision function of a model as a red-blue heatmap.\n",
    "    The region evaluated, along with x and y axis limits, are determined by 'extent'.\n",
    "    \"\"\"\n",
    "    extent = (0,1,0,1)\n",
    "    x1min, x1max ,x2min, x2max = extent\n",
    "    x1, x2 = np.meshgrid(np.linspace(x1min, x1max, 200), np.linspace(x2min, x2max, 200))\n",
    "    X = np.column_stack([x1.ravel(), x2.ravel()])\n",
    "    y = model.decision_function(X).reshape(x1.shape)\n",
    "    plt.imshow(-y, extent=extent, origin='lower', vmin=-1, vmax=1, cmap='bwr', alpha=0.5, interpolation='nearest')\n",
    "    if y.min() < 0 and y.max() > 0:\n",
    "        plt.contour(x1, x2, y, levels=[0], colors=['k'])  # Decision boundary\n",
    "    plt.xlim([x1min, x1max])\n",
    "    plt.ylim([x2min, x2max])\n",
    "    \n",
    "def plot_rf_prediction(model):\n",
    "    \"\"\"\n",
    "    Plots the model's predictions over all points in range 2D [-3, 3].\n",
    "    Assumes at most 3 classes.\n",
    "    \"\"\"\n",
    "    extent = (0, 1, 0, 1)\n",
    "    x1min, x1max ,x2min, x2max = extent\n",
    "    x1, x2 = np.meshgrid(np.linspace(x1min, x1max, 100), np.linspace(x2min, x2max, 100))\n",
    "    X = np.column_stack([x1.ravel(), x2.ravel()])\n",
    "    y = model.predict(X).reshape(x1.shape)\n",
    "    cmap = matplotlib.colors.ListedColormap(['r', 'b', 'g'])\n",
    "    plt.imshow(y, extent=extent, origin='lower', alpha=0.4, vmin=0, vmax=2, cmap=cmap, interpolation='nearest')\n",
    "    plt.xlim([x1min, x1max])\n",
    "    plt.ylim([x2min, x2max])\n",
    "    plt.gca().set_aspect('equal')\n",
    "\n",
    "def scale_data(X_trn, X_tst):\n",
    "    # Scale the data with MinMax to avoid negative values\n",
    "    scaler = sklearn.preprocessing.MinMaxScaler()\n",
    "    scaler.fit(X_trn)\n",
    "    X_trn = scaler.transform(X_trn)\n",
    "    X_tst = scaler.transform(X_tst)\n",
    "\n",
    "    return X_trn, X_tst\n",
    "\n",
    "def score_estimators(X, y, estimators):\n",
    "    \"\"\"Scores each estimator on (X, y), returning a list of scores.\"\"\"\n",
    "    # Your implementation here. Aim for 1-4 lines.\n",
    "    scores = [0 for _ in range(len(estimators))]\n",
    "    for x in range(len(estimators)):\n",
    "        scores[x] = sklearn.metrics.accuracy_score(y, estimators[x].predict(X))\n",
    "        print(sklearn.metrics.precision_recall_fscore_support(y, estimators[x].predict(X), average='binary'))\n",
    "    return scores\n",
    "\n",
    "def plot_estimator_scores(estimators, param_name, param_vals):\n",
    "    \"\"\"\n",
    "    Plots the training, validation, and testing scores of a list of estimators,\n",
    "    where `param_name` and `param_vals` are the same as for `train_estimators`.\n",
    "    The estimator with best validation score will be highlighted with an 'x'.\n",
    "    \"\"\"\n",
    "    # Your implementation here. Use as many lines as you need\n",
    "    plt.figure()\n",
    "    X = np.arange(0, len(param_vals))\n",
    "    trn_scores = score_estimators(X_trn, y_trn, estimators)\n",
    "    tst_scores = score_estimators(X_tst, y_tst, estimators)\n",
    "    index = np.argmin(trn_scores - tst_scores)\n",
    "    print(tst_scores[index])\n",
    "    plt.title(estimators[0].__class__.__name__ + \" score vs \" + param_name)\n",
    "    plt.xlabel(param_name)\n",
    "    plt.ylabel(\"score\")\n",
    "    plt.ylim(0.0, 1.05)\n",
    "    plt.scatter(X[index], tst_scores[index], marker='x', color='black', s=200)\n",
    "    plt.plot(X, trn_scores, marker='o', color='green', markerfacecolor='green', label=\"train\")\n",
    "    plt.plot(X, tst_scores, marker='o', color='red', markerfacecolor='red', label=\"test\")\n",
    "    plt.text(0, 0.4,\n",
    "             \"Optimal Test Accuracy = %.2f%% with %s = %d \" % (tst_scores[index] * 100, param_name, param_vals[index]))\n",
    "    plt.legend()\n",
    "    plt.xticks(X, param_vals)\n",
    "    \n",
    "\n",
    "def train_estimators(X, y, estimator, param_grid, cv=3):\n",
    "    gscv = sklearn.model_selection.GridSearchCV(estimator, param_grid=param_grid, verbose=1, cv=3)\n",
    "    gscv.fit(X, y)\n",
    "    print(gscv.cv_results_)\n",
    "    print(gscv.best_params_)\n",
    "    return gscv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature selection: Select an X and y we want to train on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trn = training_df[['kill_advantage','gold_advantage']].values\n",
    "y_trn = training_df[['winner']].values.T[0]\n",
    "X_tst = test_df[['kill_advantage','gold_advantage']].values\n",
    "y_tst = test_df[['winner']].values.T[0]\n",
    "X_trn, X_tst = scale_data(X_trn, X_tst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_data(X_tst[:100], y_tst[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dummy Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Here we train a dummy classifier to compare performance\n",
    "def train_dummy_classifier(X, y):\n",
    "    dummy_clf = sklearn.dummy.DummyClassifier(strategy='uniform', random_state=0)\n",
    "    # Scale the data with MinMax to avoid negative values\n",
    "    dummy_clf.fit(X, y)\n",
    "    return dummy_clf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train_kNN_estimators(X, y):\n",
    "    param_grid = {\n",
    "        'n_neighbors': [1, 5, 10, 15, 20]\n",
    "    }\n",
    "    clf = sklearn.neighbors.KNeighborsClassifier()\n",
    "    knn_estimators = train_estimators(X, y, estimator=clf, param_grid=param_grid)\n",
    "    \n",
    "    return knn_estimators.best_estimator_\n",
    "    plot_estimator_scores(knn_estimators, param_name='n_neighbors', param_vals=param_vals)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train_random_forests(X, y):\n",
    "    estimator = np.arange(1, 5) * 50\n",
    "    depths = np.arange(1, 5) * 5\n",
    "    param_grid = {\n",
    "        'n_estimators': estimator, \n",
    "        'max_depth': depths\n",
    "    }\n",
    "    clf = sklearn.ensemble.RandomForestClassifier(random_state=0)\n",
    "    gscv = train_estimators(X, y, estimator=clf, param_grid=param_grid)\n",
    "    return gscv.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_adaboost(X, y):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_logistic(X, y):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_svm(X, y):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This is the neural network trained in Lab 8\n",
    "\n",
    "def train_neural_network(X, y):\n",
    "    net = sklearn.neural_network.MLPClassifier(        \n",
    "        learning_rate_init=0.01,\n",
    "        momentum=0.9,\n",
    "        solver='sgd',\n",
    "        random_state=0,\n",
    "        verbose=True\n",
    "    )\n",
    "    param_grid = [\n",
    "        {\n",
    "            'activation' : ['logistic', 'tanh'],\n",
    "            'hidden_layer_sizes': [\n",
    "             (), (1,),(2,),(3,),\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    gscv = train_estimators(X, y, estimator=net, param_grid=param_grid)\n",
    "    return gscv.best_estimator_\n",
    "# train_neural_network(X_trn,y_trn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "Iteration 1, loss = 0.67776433\n",
      "Iteration 2, loss = 0.66063632\n",
      "Iteration 3, loss = 0.64652522\n",
      "Iteration 4, loss = 0.63331269\n",
      "Iteration 5, loss = 0.62099495\n",
      "Iteration 6, loss = 0.60948657\n",
      "Iteration 7, loss = 0.59877332\n",
      "Iteration 8, loss = 0.58876504\n",
      "Iteration 9, loss = 0.57935828\n",
      "Iteration 10, loss = 0.57054885\n",
      "Iteration 11, loss = 0.56233498\n",
      "Iteration 12, loss = 0.55456744\n",
      "Iteration 13, loss = 0.54725492\n",
      "Iteration 14, loss = 0.54044482\n",
      "Iteration 15, loss = 0.53397881\n",
      "Iteration 16, loss = 0.52794604\n",
      "Iteration 17, loss = 0.52221220\n",
      "Iteration 18, loss = 0.51681651\n",
      "Iteration 19, loss = 0.51166502\n",
      "Iteration 20, loss = 0.50683707\n",
      "Iteration 21, loss = 0.50223881\n",
      "Iteration 22, loss = 0.49788550\n",
      "Iteration 23, loss = 0.49377467\n",
      "Iteration 24, loss = 0.48990196\n",
      "Iteration 25, loss = 0.48609963\n",
      "Iteration 26, loss = 0.48251458\n",
      "Iteration 27, loss = 0.47914464\n",
      "Iteration 28, loss = 0.47590354\n",
      "Iteration 29, loss = 0.47280768\n",
      "Iteration 30, loss = 0.46987168\n",
      "Iteration 31, loss = 0.46710015\n",
      "Iteration 32, loss = 0.46440287\n",
      "Iteration 33, loss = 0.46183968\n",
      "Iteration 34, loss = 0.45938156\n",
      "Iteration 35, loss = 0.45700412\n",
      "Iteration 36, loss = 0.45478186\n",
      "Iteration 37, loss = 0.45259425\n",
      "Iteration 38, loss = 0.45053171\n",
      "Iteration 39, loss = 0.44851967\n",
      "Iteration 40, loss = 0.44659524\n",
      "Iteration 41, loss = 0.44474659\n",
      "Iteration 42, loss = 0.44297037\n",
      "Iteration 43, loss = 0.44125683\n",
      "Iteration 44, loss = 0.43960147\n",
      "Iteration 45, loss = 0.43801912\n",
      "Iteration 46, loss = 0.43650491\n",
      "Iteration 47, loss = 0.43501999\n",
      "Iteration 48, loss = 0.43358443\n",
      "Iteration 49, loss = 0.43221532\n",
      "Iteration 50, loss = 0.43088285\n",
      "Iteration 51, loss = 0.42962541\n",
      "Iteration 52, loss = 0.42840236\n",
      "Iteration 53, loss = 0.42720670\n",
      "Iteration 54, loss = 0.42603171\n",
      "Iteration 55, loss = 0.42492394\n",
      "Iteration 56, loss = 0.42384430\n",
      "Iteration 57, loss = 0.42283626\n",
      "Iteration 58, loss = 0.42178173\n",
      "Iteration 59, loss = 0.42080328\n",
      "Iteration 60, loss = 0.41986894\n",
      "Iteration 61, loss = 0.41895692\n",
      "Iteration 62, loss = 0.41803256\n",
      "Iteration 63, loss = 0.41716676\n",
      "Iteration 64, loss = 0.41631973\n",
      "Iteration 65, loss = 0.41549342\n",
      "Iteration 66, loss = 0.41469583\n",
      "Iteration 67, loss = 0.41394136\n",
      "Iteration 68, loss = 0.41317517\n",
      "Iteration 69, loss = 0.41246991\n",
      "Iteration 70, loss = 0.41175049\n",
      "Iteration 71, loss = 0.41104936\n",
      "Iteration 72, loss = 0.41038046\n",
      "Iteration 73, loss = 0.40971676\n",
      "Iteration 74, loss = 0.40909474\n",
      "Iteration 75, loss = 0.40847052\n",
      "Iteration 76, loss = 0.40786412\n",
      "Iteration 77, loss = 0.40728490\n",
      "Iteration 78, loss = 0.40670071\n",
      "Iteration 79, loss = 0.40615710\n",
      "Iteration 80, loss = 0.40564772\n",
      "Iteration 81, loss = 0.40511254\n",
      "Iteration 82, loss = 0.40457028\n",
      "Iteration 83, loss = 0.40408120\n",
      "Iteration 84, loss = 0.40358898\n",
      "Iteration 85, loss = 0.40310793\n",
      "Iteration 86, loss = 0.40265852\n",
      "Iteration 87, loss = 0.40219655\n",
      "Iteration 88, loss = 0.40178864\n",
      "Iteration 89, loss = 0.40131225\n",
      "Iteration 90, loss = 0.40088550\n",
      "Iteration 91, loss = 0.40048377\n",
      "Iteration 92, loss = 0.40007001\n",
      "Iteration 93, loss = 0.39968484\n",
      "Iteration 94, loss = 0.39930127\n",
      "Iteration 95, loss = 0.39892842\n",
      "Iteration 96, loss = 0.39854888\n",
      "Iteration 97, loss = 0.39820963\n",
      "Iteration 98, loss = 0.39786071\n",
      "Iteration 99, loss = 0.39754024\n",
      "Iteration 100, loss = 0.39718179\n",
      "Iteration 101, loss = 0.39684150\n",
      "Iteration 102, loss = 0.39654589\n",
      "Iteration 103, loss = 0.39629722\n",
      "Iteration 104, loss = 0.39591461\n",
      "Iteration 105, loss = 0.39560121\n",
      "Iteration 106, loss = 0.39535156\n",
      "Iteration 107, loss = 0.39502459\n",
      "Iteration 108, loss = 0.39477089\n",
      "Iteration 109, loss = 0.39447865\n",
      "Iteration 110, loss = 0.39421159\n",
      "Iteration 111, loss = 0.39392623\n",
      "Iteration 112, loss = 0.39368000\n",
      "Iteration 113, loss = 0.39341042\n",
      "Iteration 114, loss = 0.39317467\n",
      "Iteration 115, loss = 0.39292443\n",
      "Iteration 116, loss = 0.39271219\n",
      "Iteration 117, loss = 0.39246449\n",
      "Iteration 118, loss = 0.39224510\n",
      "Iteration 119, loss = 0.39202432\n",
      "Iteration 120, loss = 0.39181726\n",
      "Iteration 121, loss = 0.39158953\n",
      "Iteration 122, loss = 0.39136315\n",
      "Iteration 123, loss = 0.39114571\n",
      "Iteration 124, loss = 0.39094563\n",
      "Iteration 125, loss = 0.39073500\n",
      "Iteration 126, loss = 0.39057604\n",
      "Iteration 127, loss = 0.39039745\n",
      "Iteration 128, loss = 0.39017014\n",
      "Iteration 129, loss = 0.38999396\n",
      "Iteration 130, loss = 0.38983815\n",
      "Iteration 131, loss = 0.38962273\n",
      "Iteration 132, loss = 0.38945639\n",
      "Iteration 133, loss = 0.38927352\n",
      "Iteration 134, loss = 0.38910766\n",
      "Iteration 135, loss = 0.38895139\n",
      "Iteration 136, loss = 0.38877342\n",
      "Iteration 137, loss = 0.38864632\n",
      "Iteration 138, loss = 0.38847025\n",
      "Iteration 139, loss = 0.38835148\n",
      "Iteration 140, loss = 0.38816969\n",
      "Iteration 141, loss = 0.38801733\n",
      "Iteration 142, loss = 0.38786785\n",
      "Iteration 143, loss = 0.38772734\n",
      "Iteration 144, loss = 0.38760399\n",
      "Iteration 145, loss = 0.38747156\n",
      "Iteration 146, loss = 0.38732179\n",
      "Iteration 147, loss = 0.38720053\n",
      "Iteration 148, loss = 0.38706528\n",
      "Iteration 149, loss = 0.38697199\n",
      "Iteration 150, loss = 0.38679789\n",
      "Iteration 151, loss = 0.38666618\n",
      "Iteration 152, loss = 0.38656590\n",
      "Iteration 153, loss = 0.38643398\n",
      "Iteration 154, loss = 0.38630760\n",
      "Iteration 155, loss = 0.38619406\n",
      "Iteration 156, loss = 0.38609176\n",
      "Iteration 157, loss = 0.38596669\n",
      "Iteration 158, loss = 0.38586753\n",
      "Iteration 159, loss = 0.38575970\n",
      "Iteration 160, loss = 0.38566525\n",
      "Iteration 161, loss = 0.38553390\n",
      "Iteration 162, loss = 0.38544266\n",
      "Iteration 163, loss = 0.38534956\n",
      "Iteration 164, loss = 0.38523519\n",
      "Iteration 165, loss = 0.38516624\n",
      "Iteration 166, loss = 0.38502745\n",
      "Iteration 167, loss = 0.38495444\n",
      "Iteration 168, loss = 0.38485734\n",
      "Iteration 169, loss = 0.38478608\n",
      "Iteration 170, loss = 0.38466439\n",
      "Iteration 171, loss = 0.38458218\n",
      "Iteration 172, loss = 0.38447559\n",
      "Iteration 173, loss = 0.38438394\n",
      "Iteration 174, loss = 0.38430582\n",
      "Iteration 175, loss = 0.38421676\n",
      "Iteration 176, loss = 0.38412863\n",
      "Iteration 177, loss = 0.38404799\n",
      "Iteration 178, loss = 0.38397198\n",
      "Iteration 179, loss = 0.38389312\n",
      "Iteration 180, loss = 0.38381293\n",
      "Iteration 181, loss = 0.38374205\n",
      "Iteration 182, loss = 0.38366970\n",
      "Iteration 183, loss = 0.38358445\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.67842286\n",
      "Iteration 2, loss = 0.66118009\n",
      "Iteration 3, loss = 0.64709378\n",
      "Iteration 4, loss = 0.63403520\n",
      "Iteration 5, loss = 0.62191247\n",
      "Iteration 6, loss = 0.61048913\n",
      "Iteration 7, loss = 0.59995457\n",
      "Iteration 8, loss = 0.59006204\n",
      "Iteration 9, loss = 0.58084302\n",
      "Iteration 10, loss = 0.57217690\n",
      "Iteration 11, loss = 0.56408570\n",
      "Iteration 12, loss = 0.55650059\n",
      "Iteration 13, loss = 0.54937666\n",
      "Iteration 14, loss = 0.54270253\n",
      "Iteration 15, loss = 0.53638752\n",
      "Iteration 16, loss = 0.53048329\n",
      "Iteration 17, loss = 0.52489866\n",
      "Iteration 18, loss = 0.51961874\n",
      "Iteration 19, loss = 0.51466351\n",
      "Iteration 20, loss = 0.50992529\n",
      "Iteration 21, loss = 0.50552050\n",
      "Iteration 22, loss = 0.50130394\n",
      "Iteration 23, loss = 0.49724506\n",
      "Iteration 24, loss = 0.49343476\n",
      "Iteration 25, loss = 0.48983606\n",
      "Iteration 26, loss = 0.48638220\n",
      "Iteration 27, loss = 0.48314153\n",
      "Iteration 28, loss = 0.48001560\n",
      "Iteration 29, loss = 0.47700763\n",
      "Iteration 30, loss = 0.47420877\n",
      "Iteration 31, loss = 0.47148471\n",
      "Iteration 32, loss = 0.46888863\n",
      "Iteration 33, loss = 0.46640674\n",
      "Iteration 34, loss = 0.46400130\n",
      "Iteration 35, loss = 0.46173003\n",
      "Iteration 36, loss = 0.45955865\n",
      "Iteration 37, loss = 0.45746876\n",
      "Iteration 38, loss = 0.45548242\n",
      "Iteration 39, loss = 0.45357913\n",
      "Iteration 40, loss = 0.45174056\n",
      "Iteration 41, loss = 0.44998917\n",
      "Iteration 42, loss = 0.44826816\n",
      "Iteration 43, loss = 0.44660501\n",
      "Iteration 44, loss = 0.44510641\n",
      "Iteration 45, loss = 0.44349463\n",
      "Iteration 46, loss = 0.44204690\n",
      "Iteration 47, loss = 0.44064173\n",
      "Iteration 48, loss = 0.43926781\n",
      "Iteration 49, loss = 0.43796354\n",
      "Iteration 50, loss = 0.43669220\n",
      "Iteration 51, loss = 0.43551167\n",
      "Iteration 52, loss = 0.43428536\n",
      "Iteration 53, loss = 0.43315886\n",
      "Iteration 54, loss = 0.43204535\n",
      "Iteration 55, loss = 0.43101336\n",
      "Iteration 56, loss = 0.42993613\n",
      "Iteration 57, loss = 0.42895219\n",
      "Iteration 58, loss = 0.42798013\n",
      "Iteration 59, loss = 0.42703398\n",
      "Iteration 60, loss = 0.42614566\n",
      "Iteration 61, loss = 0.42529175\n",
      "Iteration 62, loss = 0.42438652\n",
      "Iteration 63, loss = 0.42356772\n",
      "Iteration 64, loss = 0.42276897\n",
      "Iteration 65, loss = 0.42198312\n",
      "Iteration 66, loss = 0.42122804\n",
      "Iteration 67, loss = 0.42052613\n",
      "Iteration 68, loss = 0.41980283\n",
      "Iteration 69, loss = 0.41910936\n",
      "Iteration 70, loss = 0.41843095\n",
      "Iteration 71, loss = 0.41778881\n",
      "Iteration 72, loss = 0.41712035\n",
      "Iteration 73, loss = 0.41648896\n",
      "Iteration 74, loss = 0.41590072\n",
      "Iteration 75, loss = 0.41529499\n",
      "Iteration 76, loss = 0.41476032\n",
      "Iteration 77, loss = 0.41421794\n",
      "Iteration 78, loss = 0.41363672\n",
      "Iteration 79, loss = 0.41310187\n",
      "Iteration 80, loss = 0.41261515\n",
      "Iteration 81, loss = 0.41213400\n",
      "Iteration 82, loss = 0.41161268\n",
      "Iteration 83, loss = 0.41113078\n",
      "Iteration 84, loss = 0.41067446\n",
      "Iteration 85, loss = 0.41020425\n",
      "Iteration 86, loss = 0.40977748\n",
      "Iteration 87, loss = 0.40934245\n",
      "Iteration 88, loss = 0.40895906\n",
      "Iteration 89, loss = 0.40850759\n",
      "Iteration 90, loss = 0.40814252\n",
      "Iteration 91, loss = 0.40773285\n",
      "Iteration 92, loss = 0.40733422\n",
      "Iteration 93, loss = 0.40696372\n",
      "Iteration 94, loss = 0.40663332\n",
      "Iteration 95, loss = 0.40624799\n",
      "Iteration 96, loss = 0.40591216\n",
      "Iteration 97, loss = 0.40561203\n",
      "Iteration 98, loss = 0.40525734\n",
      "Iteration 99, loss = 0.40494970\n",
      "Iteration 100, loss = 0.40460782\n",
      "Iteration 101, loss = 0.40428718\n",
      "Iteration 102, loss = 0.40398389\n",
      "Iteration 103, loss = 0.40367816\n",
      "Iteration 104, loss = 0.40344754\n",
      "Iteration 105, loss = 0.40310574\n",
      "Iteration 106, loss = 0.40284546\n",
      "Iteration 107, loss = 0.40257616\n",
      "Iteration 108, loss = 0.40231959\n",
      "Iteration 109, loss = 0.40207047\n",
      "Iteration 110, loss = 0.40183718\n",
      "Iteration 111, loss = 0.40154793\n",
      "Iteration 112, loss = 0.40128956\n",
      "Iteration 113, loss = 0.40105688\n",
      "Iteration 114, loss = 0.40081065\n",
      "Iteration 115, loss = 0.40061983\n",
      "Iteration 116, loss = 0.40043122\n",
      "Iteration 117, loss = 0.40013816\n",
      "Iteration 118, loss = 0.39994165\n",
      "Iteration 119, loss = 0.39975910\n",
      "Iteration 120, loss = 0.39952452\n",
      "Iteration 121, loss = 0.39931801\n",
      "Iteration 122, loss = 0.39912785\n",
      "Iteration 123, loss = 0.39892112\n",
      "Iteration 124, loss = 0.39872371\n",
      "Iteration 125, loss = 0.39853847\n",
      "Iteration 126, loss = 0.39836239\n",
      "Iteration 127, loss = 0.39818278\n",
      "Iteration 128, loss = 0.39798875\n",
      "Iteration 129, loss = 0.39783536\n",
      "Iteration 130, loss = 0.39765507\n",
      "Iteration 131, loss = 0.39748357\n",
      "Iteration 132, loss = 0.39730952\n",
      "Iteration 133, loss = 0.39715948\n",
      "Iteration 134, loss = 0.39700202\n",
      "Iteration 135, loss = 0.39684141\n",
      "Iteration 136, loss = 0.39671418\n",
      "Iteration 137, loss = 0.39653484\n",
      "Iteration 138, loss = 0.39639101\n",
      "Iteration 139, loss = 0.39627779\n",
      "Iteration 140, loss = 0.39610949\n",
      "Iteration 141, loss = 0.39596966\n",
      "Iteration 142, loss = 0.39583800\n",
      "Iteration 143, loss = 0.39572510\n",
      "Iteration 144, loss = 0.39558373\n",
      "Iteration 145, loss = 0.39547241\n",
      "Iteration 146, loss = 0.39532981\n",
      "Iteration 147, loss = 0.39519616\n",
      "Iteration 148, loss = 0.39513663\n",
      "Iteration 149, loss = 0.39498539\n",
      "Iteration 150, loss = 0.39483069\n",
      "Iteration 151, loss = 0.39473530\n",
      "Iteration 152, loss = 0.39460494\n",
      "Iteration 153, loss = 0.39449023\n",
      "Iteration 154, loss = 0.39437480\n",
      "Iteration 155, loss = 0.39428436\n",
      "Iteration 156, loss = 0.39419915\n",
      "Iteration 157, loss = 0.39404752\n",
      "Iteration 158, loss = 0.39394974\n",
      "Iteration 159, loss = 0.39385373\n",
      "Iteration 160, loss = 0.39374875\n",
      "Iteration 161, loss = 0.39364120\n",
      "Iteration 162, loss = 0.39356205\n",
      "Iteration 163, loss = 0.39347084\n",
      "Iteration 164, loss = 0.39336814\n",
      "Iteration 165, loss = 0.39327065\n",
      "Iteration 166, loss = 0.39317573\n",
      "Iteration 167, loss = 0.39307986\n",
      "Iteration 168, loss = 0.39299184\n",
      "Iteration 169, loss = 0.39290955\n",
      "Iteration 170, loss = 0.39282553\n",
      "Iteration 171, loss = 0.39272333\n",
      "Iteration 172, loss = 0.39265047\n",
      "Iteration 173, loss = 0.39258341\n",
      "Iteration 174, loss = 0.39248991\n",
      "Iteration 175, loss = 0.39242216\n",
      "Iteration 176, loss = 0.39235994\n",
      "Iteration 177, loss = 0.39225855\n",
      "Iteration 178, loss = 0.39218902\n",
      "Iteration 179, loss = 0.39211731\n",
      "Iteration 180, loss = 0.39206413\n",
      "Iteration 181, loss = 0.39202550\n",
      "Iteration 182, loss = 0.39188102\n",
      "Iteration 183, loss = 0.39180972\n",
      "Iteration 184, loss = 0.39173781\n",
      "Iteration 185, loss = 0.39167444\n",
      "Iteration 186, loss = 0.39161666\n",
      "Iteration 187, loss = 0.39155008\n",
      "Iteration 188, loss = 0.39148133\n",
      "Iteration 189, loss = 0.39141231\n",
      "Iteration 190, loss = 0.39136440\n",
      "Iteration 191, loss = 0.39129908\n",
      "Iteration 192, loss = 0.39122614\n",
      "Iteration 193, loss = 0.39116941\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.67874501\n",
      "Iteration 2, loss = 0.66173364\n",
      "Iteration 3, loss = 0.64800290\n",
      "Iteration 4, loss = 0.63519636\n",
      "Iteration 5, loss = 0.62326522\n",
      "Iteration 6, loss = 0.61216538\n",
      "Iteration 7, loss = 0.60180466\n",
      "Iteration 8, loss = 0.59219783\n",
      "Iteration 9, loss = 0.58317566\n",
      "Iteration 10, loss = 0.57474821\n",
      "Iteration 11, loss = 0.56681271\n",
      "Iteration 12, loss = 0.55940499\n",
      "Iteration 13, loss = 0.55246691\n",
      "Iteration 14, loss = 0.54593780\n",
      "Iteration 15, loss = 0.53979130\n",
      "Iteration 16, loss = 0.53400183\n",
      "Iteration 17, loss = 0.52857679\n",
      "Iteration 18, loss = 0.52341133\n",
      "Iteration 19, loss = 0.51858167\n",
      "Iteration 20, loss = 0.51401116\n",
      "Iteration 21, loss = 0.50965721\n",
      "Iteration 22, loss = 0.50561335\n",
      "Iteration 23, loss = 0.50166312\n",
      "Iteration 24, loss = 0.49800827\n",
      "Iteration 25, loss = 0.49442495\n",
      "Iteration 26, loss = 0.49110019\n",
      "Iteration 27, loss = 0.48789690\n",
      "Iteration 28, loss = 0.48488801\n",
      "Iteration 29, loss = 0.48197818\n",
      "Iteration 30, loss = 0.47923166\n",
      "Iteration 31, loss = 0.47663306\n",
      "Iteration 32, loss = 0.47411134\n",
      "Iteration 33, loss = 0.47171206\n",
      "Iteration 34, loss = 0.46942504\n",
      "Iteration 35, loss = 0.46723024\n",
      "Iteration 36, loss = 0.46512376\n",
      "Iteration 37, loss = 0.46317870\n",
      "Iteration 38, loss = 0.46119994\n",
      "Iteration 39, loss = 0.45936851\n",
      "Iteration 40, loss = 0.45757869\n",
      "Iteration 41, loss = 0.45585369\n",
      "Iteration 42, loss = 0.45420829\n",
      "Iteration 43, loss = 0.45261947\n",
      "Iteration 44, loss = 0.45109449\n",
      "Iteration 45, loss = 0.44968506\n",
      "Iteration 46, loss = 0.44821601\n",
      "Iteration 47, loss = 0.44688862\n",
      "Iteration 48, loss = 0.44558301\n",
      "Iteration 49, loss = 0.44432022\n",
      "Iteration 50, loss = 0.44309208\n",
      "Iteration 51, loss = 0.44195314\n",
      "Iteration 52, loss = 0.44084871\n",
      "Iteration 53, loss = 0.43968641\n",
      "Iteration 54, loss = 0.43862143\n",
      "Iteration 55, loss = 0.43760822\n",
      "Iteration 56, loss = 0.43659906\n",
      "Iteration 57, loss = 0.43564487\n",
      "Iteration 58, loss = 0.43475162\n",
      "Iteration 59, loss = 0.43381893\n",
      "Iteration 60, loss = 0.43295597\n",
      "Iteration 61, loss = 0.43215303\n",
      "Iteration 62, loss = 0.43129029\n",
      "Iteration 63, loss = 0.43051863\n",
      "Iteration 64, loss = 0.42974041\n",
      "Iteration 65, loss = 0.42898796\n",
      "Iteration 66, loss = 0.42828614\n",
      "Iteration 67, loss = 0.42757845\n",
      "Iteration 68, loss = 0.42688380\n",
      "Iteration 69, loss = 0.42623340\n",
      "Iteration 70, loss = 0.42562285\n",
      "Iteration 71, loss = 0.42495288\n",
      "Iteration 72, loss = 0.42436750\n",
      "Iteration 73, loss = 0.42376714\n",
      "Iteration 74, loss = 0.42318505\n",
      "Iteration 75, loss = 0.42263816\n",
      "Iteration 76, loss = 0.42208974\n",
      "Iteration 77, loss = 0.42156258\n",
      "Iteration 78, loss = 0.42106519\n",
      "Iteration 79, loss = 0.42056068\n",
      "Iteration 80, loss = 0.42006051\n",
      "Iteration 81, loss = 0.41958130\n",
      "Iteration 82, loss = 0.41917738\n",
      "Iteration 83, loss = 0.41867084\n",
      "Iteration 84, loss = 0.41824718\n",
      "Iteration 85, loss = 0.41783418\n",
      "Iteration 86, loss = 0.41739017\n",
      "Iteration 87, loss = 0.41698052\n",
      "Iteration 88, loss = 0.41658507\n",
      "Iteration 89, loss = 0.41624203\n",
      "Iteration 90, loss = 0.41583122\n",
      "Iteration 91, loss = 0.41545095\n",
      "Iteration 92, loss = 0.41512291\n",
      "Iteration 93, loss = 0.41473021\n",
      "Iteration 94, loss = 0.41439999\n",
      "Iteration 95, loss = 0.41408232\n",
      "Iteration 96, loss = 0.41376593\n",
      "Iteration 97, loss = 0.41341281\n",
      "Iteration 98, loss = 0.41312076\n",
      "Iteration 99, loss = 0.41281053\n",
      "Iteration 100, loss = 0.41249097\n",
      "Iteration 101, loss = 0.41219678\n",
      "Iteration 102, loss = 0.41193620\n",
      "Iteration 103, loss = 0.41162957\n",
      "Iteration 104, loss = 0.41137562\n",
      "Iteration 105, loss = 0.41109381\n",
      "Iteration 106, loss = 0.41085516\n",
      "Iteration 107, loss = 0.41058099\n",
      "Iteration 108, loss = 0.41035817\n",
      "Iteration 109, loss = 0.41007731\n",
      "Iteration 110, loss = 0.40985605\n",
      "Iteration 111, loss = 0.40961247\n",
      "Iteration 112, loss = 0.40938632\n",
      "Iteration 113, loss = 0.40915958\n",
      "Iteration 114, loss = 0.40893336\n",
      "Iteration 115, loss = 0.40870813\n",
      "Iteration 116, loss = 0.40850911\n",
      "Iteration 117, loss = 0.40832004\n",
      "Iteration 118, loss = 0.40808887\n",
      "Iteration 119, loss = 0.40792147\n",
      "Iteration 120, loss = 0.40771514\n",
      "Iteration 121, loss = 0.40752871\n",
      "Iteration 122, loss = 0.40732612\n",
      "Iteration 123, loss = 0.40717833\n",
      "Iteration 124, loss = 0.40700580\n",
      "Iteration 125, loss = 0.40677250\n",
      "Iteration 126, loss = 0.40661132\n",
      "Iteration 127, loss = 0.40644870\n",
      "Iteration 128, loss = 0.40627815\n",
      "Iteration 129, loss = 0.40611429\n",
      "Iteration 130, loss = 0.40597668\n",
      "Iteration 131, loss = 0.40582088\n",
      "Iteration 132, loss = 0.40564071\n",
      "Iteration 133, loss = 0.40554144\n",
      "Iteration 134, loss = 0.40535742\n",
      "Iteration 135, loss = 0.40520039\n",
      "Iteration 136, loss = 0.40509489\n",
      "Iteration 137, loss = 0.40492082\n",
      "Iteration 138, loss = 0.40481223\n",
      "Iteration 139, loss = 0.40464237\n",
      "Iteration 140, loss = 0.40450260\n",
      "Iteration 141, loss = 0.40437558\n",
      "Iteration 142, loss = 0.40425793\n",
      "Iteration 143, loss = 0.40413527\n",
      "Iteration 144, loss = 0.40401057\n",
      "Iteration 145, loss = 0.40390495\n",
      "Iteration 146, loss = 0.40381400\n",
      "Iteration 147, loss = 0.40369560\n",
      "Iteration 148, loss = 0.40351612\n",
      "Iteration 149, loss = 0.40342058\n",
      "Iteration 150, loss = 0.40329610\n",
      "Iteration 151, loss = 0.40324935\n",
      "Iteration 152, loss = 0.40311460\n",
      "Iteration 153, loss = 0.40299409\n",
      "Iteration 154, loss = 0.40291306\n",
      "Iteration 155, loss = 0.40278273\n",
      "Iteration 156, loss = 0.40264848\n",
      "Iteration 157, loss = 0.40261342\n",
      "Iteration 158, loss = 0.40248141\n",
      "Iteration 159, loss = 0.40237226\n",
      "Iteration 160, loss = 0.40228339\n",
      "Iteration 161, loss = 0.40218129\n",
      "Iteration 162, loss = 0.40212451\n",
      "Iteration 163, loss = 0.40203084\n",
      "Iteration 164, loss = 0.40193795\n",
      "Iteration 165, loss = 0.40183749\n",
      "Iteration 166, loss = 0.40173782\n",
      "Iteration 167, loss = 0.40165596\n",
      "Iteration 168, loss = 0.40157305\n",
      "Iteration 169, loss = 0.40149247\n",
      "Iteration 170, loss = 0.40140231\n",
      "Iteration 171, loss = 0.40132240\n",
      "Iteration 172, loss = 0.40125712\n",
      "Iteration 173, loss = 0.40118906\n",
      "Iteration 174, loss = 0.40110589\n",
      "Iteration 175, loss = 0.40100776\n",
      "Iteration 176, loss = 0.40095634\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69314023\n",
      "Iteration 2, loss = 0.69228889\n",
      "Iteration 3, loss = 0.69207904\n",
      "Iteration 4, loss = 0.69189603\n",
      "Iteration 5, loss = 0.69165909\n",
      "Iteration 6, loss = 0.69144683\n",
      "Iteration 7, loss = 0.69119664\n",
      "Iteration 8, loss = 0.69090602\n",
      "Iteration 9, loss = 0.69052024\n",
      "Iteration 10, loss = 0.69011167\n",
      "Iteration 11, loss = 0.68970003\n",
      "Iteration 12, loss = 0.68923660\n",
      "Iteration 13, loss = 0.68868356\n",
      "Iteration 14, loss = 0.68806994\n",
      "Iteration 15, loss = 0.68742811\n",
      "Iteration 16, loss = 0.68663536\n",
      "Iteration 17, loss = 0.68579617\n",
      "Iteration 18, loss = 0.68492685\n",
      "Iteration 19, loss = 0.68391248\n",
      "Iteration 20, loss = 0.68281603\n",
      "Iteration 21, loss = 0.68162192\n",
      "Iteration 22, loss = 0.68026523\n",
      "Iteration 23, loss = 0.67874998\n",
      "Iteration 24, loss = 0.67715760\n",
      "Iteration 25, loss = 0.67533835\n",
      "Iteration 26, loss = 0.67330939\n",
      "Iteration 27, loss = 0.67114160\n",
      "Iteration 28, loss = 0.66875213\n",
      "Iteration 29, loss = 0.66608944\n",
      "Iteration 30, loss = 0.66318286\n",
      "Iteration 31, loss = 0.66000803\n",
      "Iteration 32, loss = 0.65645788\n",
      "Iteration 33, loss = 0.65259094\n",
      "Iteration 34, loss = 0.64832949\n",
      "Iteration 35, loss = 0.64370810\n",
      "Iteration 36, loss = 0.63866537\n",
      "Iteration 37, loss = 0.63320799\n",
      "Iteration 38, loss = 0.62735163\n",
      "Iteration 39, loss = 0.62103123\n",
      "Iteration 40, loss = 0.61434793\n",
      "Iteration 41, loss = 0.60731303\n",
      "Iteration 42, loss = 0.59977837\n",
      "Iteration 43, loss = 0.59203953\n",
      "Iteration 44, loss = 0.58404326\n",
      "Iteration 45, loss = 0.57582509\n",
      "Iteration 46, loss = 0.56747328\n",
      "Iteration 47, loss = 0.55901054\n",
      "Iteration 48, loss = 0.55046574\n",
      "Iteration 49, loss = 0.54198789\n",
      "Iteration 50, loss = 0.53355461\n",
      "Iteration 51, loss = 0.52535222\n",
      "Iteration 52, loss = 0.51725713\n",
      "Iteration 53, loss = 0.50937490\n",
      "Iteration 54, loss = 0.50174797\n",
      "Iteration 55, loss = 0.49444142\n",
      "Iteration 56, loss = 0.48742118\n",
      "Iteration 57, loss = 0.48072288\n",
      "Iteration 58, loss = 0.47435264\n",
      "Iteration 59, loss = 0.46834430\n",
      "Iteration 60, loss = 0.46269931\n",
      "Iteration 61, loss = 0.45731719\n",
      "Iteration 62, loss = 0.45224157\n",
      "Iteration 63, loss = 0.44755471\n",
      "Iteration 64, loss = 0.44311828\n",
      "Iteration 65, loss = 0.43898780\n",
      "Iteration 66, loss = 0.43508998\n",
      "Iteration 67, loss = 0.43153463\n",
      "Iteration 68, loss = 0.42815096\n",
      "Iteration 69, loss = 0.42501035\n",
      "Iteration 70, loss = 0.42214033\n",
      "Iteration 71, loss = 0.41941852\n",
      "Iteration 72, loss = 0.41686786\n",
      "Iteration 73, loss = 0.41448344\n",
      "Iteration 74, loss = 0.41223093\n",
      "Iteration 75, loss = 0.41019560\n",
      "Iteration 76, loss = 0.40825472\n",
      "Iteration 77, loss = 0.40658519\n",
      "Iteration 78, loss = 0.40489627\n",
      "Iteration 79, loss = 0.40336448\n",
      "Iteration 80, loss = 0.40197188\n",
      "Iteration 81, loss = 0.40068195\n",
      "Iteration 82, loss = 0.39941482\n",
      "Iteration 83, loss = 0.39828813\n",
      "Iteration 84, loss = 0.39706643\n",
      "Iteration 85, loss = 0.39629396\n",
      "Iteration 86, loss = 0.39520789\n",
      "Iteration 87, loss = 0.39437138\n",
      "Iteration 88, loss = 0.39353372\n",
      "Iteration 89, loss = 0.39276412\n",
      "Iteration 90, loss = 0.39210402\n",
      "Iteration 91, loss = 0.39140693\n",
      "Iteration 92, loss = 0.39079178\n",
      "Iteration 93, loss = 0.39023116\n",
      "Iteration 94, loss = 0.38970623\n",
      "Iteration 95, loss = 0.38943385\n",
      "Iteration 96, loss = 0.38875664\n",
      "Iteration 97, loss = 0.38826749\n",
      "Iteration 98, loss = 0.38798034\n",
      "Iteration 99, loss = 0.38745226\n",
      "Iteration 100, loss = 0.38710501\n",
      "Iteration 101, loss = 0.38683324\n",
      "Iteration 102, loss = 0.38648956\n",
      "Iteration 103, loss = 0.38628385\n",
      "Iteration 104, loss = 0.38581799\n",
      "Iteration 105, loss = 0.38558841\n",
      "Iteration 106, loss = 0.38524807\n",
      "Iteration 107, loss = 0.38530618\n",
      "Iteration 108, loss = 0.38479993\n",
      "Iteration 109, loss = 0.38480998\n",
      "Iteration 110, loss = 0.38442812\n",
      "Iteration 111, loss = 0.38432367\n",
      "Iteration 112, loss = 0.38402514\n",
      "Iteration 113, loss = 0.38392758\n",
      "Iteration 114, loss = 0.38374635\n",
      "Iteration 115, loss = 0.38366859\n",
      "Iteration 116, loss = 0.38349186\n",
      "Iteration 117, loss = 0.38332665\n",
      "Iteration 118, loss = 0.38321186\n",
      "Iteration 119, loss = 0.38318932\n",
      "Iteration 120, loss = 0.38297323\n",
      "Iteration 121, loss = 0.38286481\n",
      "Iteration 122, loss = 0.38275059\n",
      "Iteration 123, loss = 0.38268945\n",
      "Iteration 124, loss = 0.38255751\n",
      "Iteration 125, loss = 0.38250800\n",
      "Iteration 126, loss = 0.38231783\n",
      "Iteration 127, loss = 0.38231740\n",
      "Iteration 128, loss = 0.38229341\n",
      "Iteration 129, loss = 0.38212995\n",
      "Iteration 130, loss = 0.38207133\n",
      "Iteration 131, loss = 0.38198796\n",
      "Iteration 132, loss = 0.38200218\n",
      "Iteration 133, loss = 0.38189275\n",
      "Iteration 134, loss = 0.38174486\n",
      "Iteration 135, loss = 0.38173286\n",
      "Iteration 136, loss = 0.38167922\n",
      "Iteration 137, loss = 0.38162981\n",
      "Iteration 138, loss = 0.38156708\n",
      "Iteration 139, loss = 0.38155842\n",
      "Iteration 140, loss = 0.38150709\n",
      "Iteration 141, loss = 0.38138101\n",
      "Iteration 142, loss = 0.38136061\n",
      "Iteration 143, loss = 0.38127069\n",
      "Iteration 144, loss = 0.38128766\n",
      "Iteration 145, loss = 0.38118173\n",
      "Iteration 146, loss = 0.38119633\n",
      "Iteration 147, loss = 0.38112393\n",
      "Iteration 148, loss = 0.38109234\n",
      "Iteration 149, loss = 0.38108655\n",
      "Iteration 150, loss = 0.38107240\n",
      "Iteration 151, loss = 0.38097380\n",
      "Iteration 152, loss = 0.38089359\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69299712\n",
      "Iteration 2, loss = 0.69232412\n",
      "Iteration 3, loss = 0.69208932\n",
      "Iteration 4, loss = 0.69191288\n",
      "Iteration 5, loss = 0.69169086\n",
      "Iteration 6, loss = 0.69146389\n",
      "Iteration 7, loss = 0.69124142\n",
      "Iteration 8, loss = 0.69096860\n",
      "Iteration 9, loss = 0.69056068\n",
      "Iteration 10, loss = 0.69018409\n",
      "Iteration 11, loss = 0.68976991\n",
      "Iteration 12, loss = 0.68930064\n",
      "Iteration 13, loss = 0.68876544\n",
      "Iteration 14, loss = 0.68814347\n",
      "Iteration 15, loss = 0.68754072\n",
      "Iteration 16, loss = 0.68677417\n",
      "Iteration 17, loss = 0.68594998\n",
      "Iteration 18, loss = 0.68509779\n",
      "Iteration 19, loss = 0.68408361\n",
      "Iteration 20, loss = 0.68304799\n",
      "Iteration 21, loss = 0.68184394\n",
      "Iteration 22, loss = 0.68053487\n",
      "Iteration 23, loss = 0.67905726\n",
      "Iteration 24, loss = 0.67746861\n",
      "Iteration 25, loss = 0.67571349\n",
      "Iteration 26, loss = 0.67383955\n",
      "Iteration 27, loss = 0.67166025\n",
      "Iteration 28, loss = 0.66934749\n",
      "Iteration 29, loss = 0.66677720\n",
      "Iteration 30, loss = 0.66394146\n",
      "Iteration 31, loss = 0.66080589\n",
      "Iteration 32, loss = 0.65740902\n",
      "Iteration 33, loss = 0.65377399\n",
      "Iteration 34, loss = 0.64956651\n",
      "Iteration 35, loss = 0.64513115\n",
      "Iteration 36, loss = 0.64021744\n",
      "Iteration 37, loss = 0.63495438\n",
      "Iteration 38, loss = 0.62927057\n",
      "Iteration 39, loss = 0.62322601\n",
      "Iteration 40, loss = 0.61677061\n",
      "Iteration 41, loss = 0.61002685\n",
      "Iteration 42, loss = 0.60277995\n",
      "Iteration 43, loss = 0.59527688\n",
      "Iteration 44, loss = 0.58753878\n",
      "Iteration 45, loss = 0.57959343\n",
      "Iteration 46, loss = 0.57149254\n",
      "Iteration 47, loss = 0.56331778\n",
      "Iteration 48, loss = 0.55504804\n",
      "Iteration 49, loss = 0.54684175\n",
      "Iteration 50, loss = 0.53863874\n",
      "Iteration 51, loss = 0.53059749\n",
      "Iteration 52, loss = 0.52273918\n",
      "Iteration 53, loss = 0.51511248\n",
      "Iteration 54, loss = 0.50769655\n",
      "Iteration 55, loss = 0.50058624\n",
      "Iteration 56, loss = 0.49371943\n",
      "Iteration 57, loss = 0.48712118\n",
      "Iteration 58, loss = 0.48095254\n",
      "Iteration 59, loss = 0.47513755\n",
      "Iteration 60, loss = 0.46952974\n",
      "Iteration 61, loss = 0.46422604\n",
      "Iteration 62, loss = 0.45933321\n",
      "Iteration 63, loss = 0.45465915\n",
      "Iteration 64, loss = 0.45030548\n",
      "Iteration 65, loss = 0.44626653\n",
      "Iteration 66, loss = 0.44250184\n",
      "Iteration 67, loss = 0.43899739\n",
      "Iteration 68, loss = 0.43560020\n",
      "Iteration 69, loss = 0.43251207\n",
      "Iteration 70, loss = 0.42961625\n",
      "Iteration 71, loss = 0.42700170\n",
      "Iteration 72, loss = 0.42439003\n",
      "Iteration 73, loss = 0.42213671\n",
      "Iteration 74, loss = 0.41997425\n",
      "Iteration 75, loss = 0.41792184\n",
      "Iteration 76, loss = 0.41606700\n",
      "Iteration 77, loss = 0.41440913\n",
      "Iteration 78, loss = 0.41264379\n",
      "Iteration 79, loss = 0.41111184\n",
      "Iteration 80, loss = 0.40972103\n",
      "Iteration 81, loss = 0.40839661\n",
      "Iteration 82, loss = 0.40736217\n",
      "Iteration 83, loss = 0.40629253\n",
      "Iteration 84, loss = 0.40504857\n",
      "Iteration 85, loss = 0.40408355\n",
      "Iteration 86, loss = 0.40307060\n",
      "Iteration 87, loss = 0.40219971\n",
      "Iteration 88, loss = 0.40135085\n",
      "Iteration 89, loss = 0.40060241\n",
      "Iteration 90, loss = 0.40002868\n",
      "Iteration 91, loss = 0.39928094\n",
      "Iteration 92, loss = 0.39865939\n",
      "Iteration 93, loss = 0.39815566\n",
      "Iteration 94, loss = 0.39759262\n",
      "Iteration 95, loss = 0.39719499\n",
      "Iteration 96, loss = 0.39654179\n",
      "Iteration 97, loss = 0.39618036\n",
      "Iteration 98, loss = 0.39573464\n",
      "Iteration 99, loss = 0.39542485\n",
      "Iteration 100, loss = 0.39503089\n",
      "Iteration 101, loss = 0.39463064\n",
      "Iteration 102, loss = 0.39431595\n",
      "Iteration 103, loss = 0.39408406\n",
      "Iteration 104, loss = 0.39377788\n",
      "Iteration 105, loss = 0.39347645\n",
      "Iteration 106, loss = 0.39315236\n",
      "Iteration 107, loss = 0.39323577\n",
      "Iteration 108, loss = 0.39280154\n",
      "Iteration 109, loss = 0.39262633\n",
      "Iteration 110, loss = 0.39243710\n",
      "Iteration 111, loss = 0.39226348\n",
      "Iteration 112, loss = 0.39196540\n",
      "Iteration 113, loss = 0.39187698\n",
      "Iteration 114, loss = 0.39163288\n",
      "Iteration 115, loss = 0.39157092\n",
      "Iteration 116, loss = 0.39158643\n",
      "Iteration 117, loss = 0.39128316\n",
      "Iteration 118, loss = 0.39113263\n",
      "Iteration 119, loss = 0.39103950\n",
      "Iteration 120, loss = 0.39091675\n",
      "Iteration 121, loss = 0.39073106\n",
      "Iteration 122, loss = 0.39060481\n",
      "Iteration 123, loss = 0.39062056\n",
      "Iteration 124, loss = 0.39044091\n",
      "Iteration 125, loss = 0.39038133\n",
      "Iteration 126, loss = 0.39030863\n",
      "Iteration 127, loss = 0.39017459\n",
      "Iteration 128, loss = 0.39012080\n",
      "Iteration 129, loss = 0.39001661\n",
      "Iteration 130, loss = 0.39000851\n",
      "Iteration 131, loss = 0.38992028\n",
      "Iteration 132, loss = 0.38980504\n",
      "Iteration 133, loss = 0.38989503\n",
      "Iteration 134, loss = 0.38974980\n",
      "Iteration 135, loss = 0.38968153\n",
      "Iteration 136, loss = 0.38953787\n",
      "Iteration 137, loss = 0.38950618\n",
      "Iteration 138, loss = 0.38947783\n",
      "Iteration 139, loss = 0.38942720\n",
      "Iteration 140, loss = 0.38926285\n",
      "Iteration 141, loss = 0.38926522\n",
      "Iteration 142, loss = 0.38932236\n",
      "Iteration 143, loss = 0.38913341\n",
      "Iteration 144, loss = 0.38917788\n",
      "Iteration 145, loss = 0.38907637\n",
      "Iteration 146, loss = 0.38904228\n",
      "Iteration 147, loss = 0.38895905\n",
      "Iteration 148, loss = 0.38901192\n",
      "Iteration 149, loss = 0.38891937\n",
      "Iteration 150, loss = 0.38886665\n",
      "Iteration 151, loss = 0.38886388\n",
      "Iteration 152, loss = 0.38878400\n",
      "Iteration 153, loss = 0.38879676\n",
      "Iteration 154, loss = 0.38870681\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69290217\n",
      "Iteration 2, loss = 0.69228937\n",
      "Iteration 3, loss = 0.69212296\n",
      "Iteration 4, loss = 0.69194071\n",
      "Iteration 5, loss = 0.69174784\n",
      "Iteration 6, loss = 0.69158023\n",
      "Iteration 7, loss = 0.69122146\n",
      "Iteration 8, loss = 0.69095664\n",
      "Iteration 9, loss = 0.69069945\n",
      "Iteration 10, loss = 0.69031453\n",
      "Iteration 11, loss = 0.68987529\n",
      "Iteration 12, loss = 0.68942700\n",
      "Iteration 13, loss = 0.68888461\n",
      "Iteration 14, loss = 0.68830069\n",
      "Iteration 15, loss = 0.68767211\n",
      "Iteration 16, loss = 0.68702361\n",
      "Iteration 17, loss = 0.68620365\n",
      "Iteration 18, loss = 0.68538710\n",
      "Iteration 19, loss = 0.68440304\n",
      "Iteration 20, loss = 0.68339852\n",
      "Iteration 21, loss = 0.68219269\n",
      "Iteration 22, loss = 0.68093181\n",
      "Iteration 23, loss = 0.67957576\n",
      "Iteration 24, loss = 0.67804901\n",
      "Iteration 25, loss = 0.67637500\n",
      "Iteration 26, loss = 0.67454594\n",
      "Iteration 27, loss = 0.67250605\n",
      "Iteration 28, loss = 0.67034469\n",
      "Iteration 29, loss = 0.66784161\n",
      "Iteration 30, loss = 0.66517029\n",
      "Iteration 31, loss = 0.66224812\n",
      "Iteration 32, loss = 0.65901371\n",
      "Iteration 33, loss = 0.65543075\n",
      "Iteration 34, loss = 0.65156479\n",
      "Iteration 35, loss = 0.64735813\n",
      "Iteration 36, loss = 0.64275339\n",
      "Iteration 37, loss = 0.63774749\n",
      "Iteration 38, loss = 0.63247612\n",
      "Iteration 39, loss = 0.62669928\n",
      "Iteration 40, loss = 0.62056708\n",
      "Iteration 41, loss = 0.61404375\n",
      "Iteration 42, loss = 0.60725580\n",
      "Iteration 43, loss = 0.60011914\n",
      "Iteration 44, loss = 0.59273932\n",
      "Iteration 45, loss = 0.58513015\n",
      "Iteration 46, loss = 0.57736542\n",
      "Iteration 47, loss = 0.56945736\n",
      "Iteration 48, loss = 0.56149437\n",
      "Iteration 49, loss = 0.55361367\n",
      "Iteration 50, loss = 0.54575040\n",
      "Iteration 51, loss = 0.53782322\n",
      "Iteration 52, loss = 0.53016599\n",
      "Iteration 53, loss = 0.52270913\n",
      "Iteration 54, loss = 0.51549288\n",
      "Iteration 55, loss = 0.50843950\n",
      "Iteration 56, loss = 0.50175018\n",
      "Iteration 57, loss = 0.49531288\n",
      "Iteration 58, loss = 0.48927589\n",
      "Iteration 59, loss = 0.48349533\n",
      "Iteration 60, loss = 0.47799830\n",
      "Iteration 61, loss = 0.47278571\n",
      "Iteration 62, loss = 0.46786251\n",
      "Iteration 63, loss = 0.46329710\n",
      "Iteration 64, loss = 0.45895947\n",
      "Iteration 65, loss = 0.45499372\n",
      "Iteration 66, loss = 0.45117086\n",
      "Iteration 67, loss = 0.44762567\n",
      "Iteration 68, loss = 0.44432928\n",
      "Iteration 69, loss = 0.44132310\n",
      "Iteration 70, loss = 0.43844079\n",
      "Iteration 71, loss = 0.43579081\n",
      "Iteration 72, loss = 0.43332718\n",
      "Iteration 73, loss = 0.43104325\n",
      "Iteration 74, loss = 0.42888395\n",
      "Iteration 75, loss = 0.42689893\n",
      "Iteration 76, loss = 0.42504872\n",
      "Iteration 77, loss = 0.42326952\n",
      "Iteration 78, loss = 0.42168669\n",
      "Iteration 79, loss = 0.42015908\n",
      "Iteration 80, loss = 0.41875026\n",
      "Iteration 81, loss = 0.41749997\n",
      "Iteration 82, loss = 0.41627140\n",
      "Iteration 83, loss = 0.41515330\n",
      "Iteration 84, loss = 0.41415221\n",
      "Iteration 85, loss = 0.41317328\n",
      "Iteration 86, loss = 0.41221190\n",
      "Iteration 87, loss = 0.41145870\n",
      "Iteration 88, loss = 0.41059643\n",
      "Iteration 89, loss = 0.40989450\n",
      "Iteration 90, loss = 0.40915346\n",
      "Iteration 91, loss = 0.40851743\n",
      "Iteration 92, loss = 0.40789970\n",
      "Iteration 93, loss = 0.40729891\n",
      "Iteration 94, loss = 0.40678883\n",
      "Iteration 95, loss = 0.40628748\n",
      "Iteration 96, loss = 0.40586920\n",
      "Iteration 97, loss = 0.40553488\n",
      "Iteration 98, loss = 0.40501699\n",
      "Iteration 99, loss = 0.40462615\n",
      "Iteration 100, loss = 0.40426416\n",
      "Iteration 101, loss = 0.40393294\n",
      "Iteration 102, loss = 0.40361498\n",
      "Iteration 103, loss = 0.40330074\n",
      "Iteration 104, loss = 0.40298479\n",
      "Iteration 105, loss = 0.40271530\n",
      "Iteration 106, loss = 0.40255388\n",
      "Iteration 107, loss = 0.40237980\n",
      "Iteration 108, loss = 0.40206474\n",
      "Iteration 109, loss = 0.40185659\n",
      "Iteration 110, loss = 0.40156396\n",
      "Iteration 111, loss = 0.40144099\n",
      "Iteration 112, loss = 0.40125760\n",
      "Iteration 113, loss = 0.40111644\n",
      "Iteration 114, loss = 0.40088993\n",
      "Iteration 115, loss = 0.40086045\n",
      "Iteration 116, loss = 0.40059509\n",
      "Iteration 117, loss = 0.40043491\n",
      "Iteration 118, loss = 0.40029759\n",
      "Iteration 119, loss = 0.40024088\n",
      "Iteration 120, loss = 0.40006580\n",
      "Iteration 121, loss = 0.40002845\n",
      "Iteration 122, loss = 0.39988344\n",
      "Iteration 123, loss = 0.39980980\n",
      "Iteration 124, loss = 0.39964247\n",
      "Iteration 125, loss = 0.39949671\n",
      "Iteration 126, loss = 0.39940388\n",
      "Iteration 127, loss = 0.39939427\n",
      "Iteration 128, loss = 0.39926297\n",
      "Iteration 129, loss = 0.39912520\n",
      "Iteration 130, loss = 0.39905273\n",
      "Iteration 131, loss = 0.39901838\n",
      "Iteration 132, loss = 0.39891405\n",
      "Iteration 133, loss = 0.39891217\n",
      "Iteration 134, loss = 0.39876276\n",
      "Iteration 135, loss = 0.39873766\n",
      "Iteration 136, loss = 0.39864063\n",
      "Iteration 137, loss = 0.39863443\n",
      "Iteration 138, loss = 0.39847246\n",
      "Iteration 139, loss = 0.39844533\n",
      "Iteration 140, loss = 0.39841245\n",
      "Iteration 141, loss = 0.39835247\n",
      "Iteration 142, loss = 0.39827666\n",
      "Iteration 143, loss = 0.39821692\n",
      "Iteration 144, loss = 0.39818229\n",
      "Iteration 145, loss = 0.39822975\n",
      "Iteration 146, loss = 0.39799561\n",
      "Iteration 147, loss = 0.39807929\n",
      "Iteration 148, loss = 0.39783834\n",
      "Iteration 149, loss = 0.39790135\n",
      "Iteration 150, loss = 0.39789318\n",
      "Iteration 151, loss = 0.39802215\n",
      "Iteration 152, loss = 0.39776375\n",
      "Iteration 153, loss = 0.39780189\n",
      "Iteration 154, loss = 0.39764676\n",
      "Iteration 155, loss = 0.39757902\n",
      "Iteration 156, loss = 0.39752240\n",
      "Iteration 157, loss = 0.39760038\n",
      "Iteration 158, loss = 0.39755992\n",
      "Iteration 159, loss = 0.39752563\n",
      "Iteration 160, loss = 0.39740843\n",
      "Iteration 161, loss = 0.39732707\n",
      "Iteration 162, loss = 0.39734872\n",
      "Iteration 163, loss = 0.39720620\n",
      "Iteration 164, loss = 0.39743699\n",
      "Iteration 165, loss = 0.39728475\n",
      "Iteration 166, loss = 0.39720732\n",
      "Iteration 167, loss = 0.39709884\n",
      "Iteration 168, loss = 0.39717971\n",
      "Iteration 169, loss = 0.39704512\n",
      "Iteration 170, loss = 0.39697566\n",
      "Iteration 171, loss = 0.39702821\n",
      "Iteration 172, loss = 0.39704048\n",
      "Iteration 173, loss = 0.39691534\n",
      "Iteration 174, loss = 0.39683096\n",
      "Iteration 175, loss = 0.39691511\n",
      "Iteration 176, loss = 0.39702991\n",
      "Iteration 177, loss = 0.39683030\n",
      "Iteration 178, loss = 0.39674924\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.73936050\n",
      "Iteration 2, loss = 0.69346295\n",
      "Iteration 3, loss = 0.69286193\n",
      "Iteration 4, loss = 0.69241185\n",
      "Iteration 5, loss = 0.69197547\n",
      "Iteration 6, loss = 0.69149947\n",
      "Iteration 7, loss = 0.69099091\n",
      "Iteration 8, loss = 0.69038717\n",
      "Iteration 9, loss = 0.68974772\n",
      "Iteration 10, loss = 0.68901217\n",
      "Iteration 11, loss = 0.68814371\n",
      "Iteration 12, loss = 0.68719552\n",
      "Iteration 13, loss = 0.68605252\n",
      "Iteration 14, loss = 0.68485091\n",
      "Iteration 15, loss = 0.68340624\n",
      "Iteration 16, loss = 0.68182852\n",
      "Iteration 17, loss = 0.68008602\n",
      "Iteration 18, loss = 0.67801043\n",
      "Iteration 19, loss = 0.67568362\n",
      "Iteration 20, loss = 0.67317388\n",
      "Iteration 21, loss = 0.67036826\n",
      "Iteration 22, loss = 0.66720674\n",
      "Iteration 23, loss = 0.66375403\n",
      "Iteration 24, loss = 0.65991371\n",
      "Iteration 25, loss = 0.65571410\n",
      "Iteration 26, loss = 0.65115569\n",
      "Iteration 27, loss = 0.64611604\n",
      "Iteration 28, loss = 0.64065141\n",
      "Iteration 29, loss = 0.63473415\n",
      "Iteration 30, loss = 0.62836326\n",
      "Iteration 31, loss = 0.62147239\n",
      "Iteration 32, loss = 0.61411056\n",
      "Iteration 33, loss = 0.60629343\n",
      "Iteration 34, loss = 0.59802285\n",
      "Iteration 35, loss = 0.58939603\n",
      "Iteration 36, loss = 0.58029991\n",
      "Iteration 37, loss = 0.57096600\n",
      "Iteration 38, loss = 0.56139748\n",
      "Iteration 39, loss = 0.55167543\n",
      "Iteration 40, loss = 0.54194598\n",
      "Iteration 41, loss = 0.53213748\n",
      "Iteration 42, loss = 0.52259622\n",
      "Iteration 43, loss = 0.51314086\n",
      "Iteration 44, loss = 0.50405121\n",
      "Iteration 45, loss = 0.49518691\n",
      "Iteration 46, loss = 0.48686505\n",
      "Iteration 47, loss = 0.47890905\n",
      "Iteration 48, loss = 0.47132574\n",
      "Iteration 49, loss = 0.46425421\n",
      "Iteration 50, loss = 0.45755571\n",
      "Iteration 51, loss = 0.45141263\n",
      "Iteration 52, loss = 0.44577172\n",
      "Iteration 53, loss = 0.44040530\n",
      "Iteration 54, loss = 0.43555615\n",
      "Iteration 55, loss = 0.43097794\n",
      "Iteration 56, loss = 0.42686909\n",
      "Iteration 57, loss = 0.42304843\n",
      "Iteration 58, loss = 0.41953556\n",
      "Iteration 59, loss = 0.41629455\n",
      "Iteration 60, loss = 0.41337391\n",
      "Iteration 61, loss = 0.41064991\n",
      "Iteration 62, loss = 0.40818162\n",
      "Iteration 63, loss = 0.40595549\n",
      "Iteration 64, loss = 0.40392185\n",
      "Iteration 65, loss = 0.40196382\n",
      "Iteration 66, loss = 0.40029665\n",
      "Iteration 67, loss = 0.39871398\n",
      "Iteration 68, loss = 0.39726461\n",
      "Iteration 69, loss = 0.39590139\n",
      "Iteration 70, loss = 0.39464077\n",
      "Iteration 71, loss = 0.39353056\n",
      "Iteration 72, loss = 0.39247658\n",
      "Iteration 73, loss = 0.39161037\n",
      "Iteration 74, loss = 0.39076227\n",
      "Iteration 75, loss = 0.38991502\n",
      "Iteration 76, loss = 0.38913325\n",
      "Iteration 77, loss = 0.38851832\n",
      "Iteration 78, loss = 0.38791154\n",
      "Iteration 79, loss = 0.38733072\n",
      "Iteration 80, loss = 0.38678768\n",
      "Iteration 81, loss = 0.38641862\n",
      "Iteration 82, loss = 0.38584635\n",
      "Iteration 83, loss = 0.38565751\n",
      "Iteration 84, loss = 0.38506914\n",
      "Iteration 85, loss = 0.38472770\n",
      "Iteration 86, loss = 0.38432480\n",
      "Iteration 87, loss = 0.38413777\n",
      "Iteration 88, loss = 0.38375423\n",
      "Iteration 89, loss = 0.38354091\n",
      "Iteration 90, loss = 0.38343240\n",
      "Iteration 91, loss = 0.38306564\n",
      "Iteration 92, loss = 0.38290098\n",
      "Iteration 93, loss = 0.38270761\n",
      "Iteration 94, loss = 0.38245073\n",
      "Iteration 95, loss = 0.38219457\n",
      "Iteration 96, loss = 0.38212230\n",
      "Iteration 97, loss = 0.38192026\n",
      "Iteration 98, loss = 0.38181485\n",
      "Iteration 99, loss = 0.38159042\n",
      "Iteration 100, loss = 0.38143781\n",
      "Iteration 101, loss = 0.38137862\n",
      "Iteration 102, loss = 0.38122557\n",
      "Iteration 103, loss = 0.38116400\n",
      "Iteration 104, loss = 0.38100501\n",
      "Iteration 105, loss = 0.38104738\n",
      "Iteration 106, loss = 0.38076656\n",
      "Iteration 107, loss = 0.38099001\n",
      "Iteration 108, loss = 0.38072946\n",
      "Iteration 109, loss = 0.38068810\n",
      "Iteration 110, loss = 0.38055305\n",
      "Iteration 111, loss = 0.38041637\n",
      "Iteration 112, loss = 0.38039856\n",
      "Iteration 113, loss = 0.38030037\n",
      "Iteration 114, loss = 0.38018901\n",
      "Iteration 115, loss = 0.38033820\n",
      "Iteration 116, loss = 0.38015699\n",
      "Iteration 117, loss = 0.38015454\n",
      "Iteration 118, loss = 0.37994915\n",
      "Iteration 119, loss = 0.37988193\n",
      "Iteration 120, loss = 0.37991214\n",
      "Iteration 121, loss = 0.37984367\n",
      "Iteration 122, loss = 0.37963132\n",
      "Iteration 123, loss = 0.37976171\n",
      "Iteration 124, loss = 0.37971992\n",
      "Iteration 125, loss = 0.37970544\n",
      "Iteration 126, loss = 0.37950434\n",
      "Iteration 127, loss = 0.37959737\n",
      "Iteration 128, loss = 0.37951135\n",
      "Iteration 129, loss = 0.37944678\n",
      "Iteration 130, loss = 0.37938853\n",
      "Iteration 131, loss = 0.37944605\n",
      "Iteration 132, loss = 0.37931354\n",
      "Iteration 133, loss = 0.37936016\n",
      "Iteration 134, loss = 0.37923716\n",
      "Iteration 135, loss = 0.37924654\n",
      "Iteration 136, loss = 0.37918778\n",
      "Iteration 137, loss = 0.37922712\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.74074269\n",
      "Iteration 2, loss = 0.69341212\n",
      "Iteration 3, loss = 0.69284810\n",
      "Iteration 4, loss = 0.69244000\n",
      "Iteration 5, loss = 0.69196532\n",
      "Iteration 6, loss = 0.69147221\n",
      "Iteration 7, loss = 0.69093689\n",
      "Iteration 8, loss = 0.69032367\n",
      "Iteration 9, loss = 0.68966363\n",
      "Iteration 10, loss = 0.68893712\n",
      "Iteration 11, loss = 0.68806572\n",
      "Iteration 12, loss = 0.68712984\n",
      "Iteration 13, loss = 0.68600167\n",
      "Iteration 14, loss = 0.68476913\n",
      "Iteration 15, loss = 0.68335193\n",
      "Iteration 16, loss = 0.68178993\n",
      "Iteration 17, loss = 0.67994833\n",
      "Iteration 18, loss = 0.67794757\n",
      "Iteration 19, loss = 0.67566709\n",
      "Iteration 20, loss = 0.67305333\n",
      "Iteration 21, loss = 0.67023970\n",
      "Iteration 22, loss = 0.66712578\n",
      "Iteration 23, loss = 0.66369326\n",
      "Iteration 24, loss = 0.65988781\n",
      "Iteration 25, loss = 0.65576576\n",
      "Iteration 26, loss = 0.65131187\n",
      "Iteration 27, loss = 0.64620189\n",
      "Iteration 28, loss = 0.64086817\n",
      "Iteration 29, loss = 0.63500851\n",
      "Iteration 30, loss = 0.62876736\n",
      "Iteration 31, loss = 0.62196867\n",
      "Iteration 32, loss = 0.61482925\n",
      "Iteration 33, loss = 0.60713632\n",
      "Iteration 34, loss = 0.59902189\n",
      "Iteration 35, loss = 0.59057909\n",
      "Iteration 36, loss = 0.58175036\n",
      "Iteration 37, loss = 0.57262025\n",
      "Iteration 38, loss = 0.56334435\n",
      "Iteration 39, loss = 0.55395786\n",
      "Iteration 40, loss = 0.54453475\n",
      "Iteration 41, loss = 0.53506956\n",
      "Iteration 42, loss = 0.52581593\n",
      "Iteration 43, loss = 0.51668691\n",
      "Iteration 44, loss = 0.50788516\n",
      "Iteration 45, loss = 0.49948818\n",
      "Iteration 46, loss = 0.49132540\n",
      "Iteration 47, loss = 0.48373679\n",
      "Iteration 48, loss = 0.47651627\n",
      "Iteration 49, loss = 0.46967881\n",
      "Iteration 50, loss = 0.46324667\n",
      "Iteration 51, loss = 0.45725924\n",
      "Iteration 52, loss = 0.45183526\n",
      "Iteration 53, loss = 0.44667286\n",
      "Iteration 54, loss = 0.44195242\n",
      "Iteration 55, loss = 0.43757905\n",
      "Iteration 56, loss = 0.43352702\n",
      "Iteration 57, loss = 0.42989869\n",
      "Iteration 58, loss = 0.42655771\n",
      "Iteration 59, loss = 0.42340255\n",
      "Iteration 60, loss = 0.42055802\n",
      "Iteration 61, loss = 0.41799152\n",
      "Iteration 62, loss = 0.41574693\n",
      "Iteration 63, loss = 0.41349578\n",
      "Iteration 64, loss = 0.41150178\n",
      "Iteration 65, loss = 0.40964510\n",
      "Iteration 66, loss = 0.40803797\n",
      "Iteration 67, loss = 0.40645901\n",
      "Iteration 68, loss = 0.40506885\n",
      "Iteration 69, loss = 0.40375082\n",
      "Iteration 70, loss = 0.40257914\n",
      "Iteration 71, loss = 0.40166432\n",
      "Iteration 72, loss = 0.40047693\n",
      "Iteration 73, loss = 0.39961258\n",
      "Iteration 74, loss = 0.39869516\n",
      "Iteration 75, loss = 0.39815655\n",
      "Iteration 76, loss = 0.39724418\n",
      "Iteration 77, loss = 0.39667479\n",
      "Iteration 78, loss = 0.39599298\n",
      "Iteration 79, loss = 0.39551854\n",
      "Iteration 80, loss = 0.39490029\n",
      "Iteration 81, loss = 0.39457472\n",
      "Iteration 82, loss = 0.39396363\n",
      "Iteration 83, loss = 0.39366551\n",
      "Iteration 84, loss = 0.39323794\n",
      "Iteration 85, loss = 0.39291596\n",
      "Iteration 86, loss = 0.39265378\n",
      "Iteration 87, loss = 0.39232125\n",
      "Iteration 88, loss = 0.39199358\n",
      "Iteration 89, loss = 0.39178475\n",
      "Iteration 90, loss = 0.39149499\n",
      "Iteration 91, loss = 0.39128912\n",
      "Iteration 92, loss = 0.39113575\n",
      "Iteration 93, loss = 0.39091458\n",
      "Iteration 94, loss = 0.39079102\n",
      "Iteration 95, loss = 0.39052078\n",
      "Iteration 96, loss = 0.39032432\n",
      "Iteration 97, loss = 0.39016692\n",
      "Iteration 98, loss = 0.39002293\n",
      "Iteration 99, loss = 0.38989727\n",
      "Iteration 100, loss = 0.38972844\n",
      "Iteration 101, loss = 0.38961287\n",
      "Iteration 102, loss = 0.38947878\n",
      "Iteration 103, loss = 0.38945995\n",
      "Iteration 104, loss = 0.38933274\n",
      "Iteration 105, loss = 0.38939002\n",
      "Iteration 106, loss = 0.38899819\n",
      "Iteration 107, loss = 0.38920169\n",
      "Iteration 108, loss = 0.38900932\n",
      "Iteration 109, loss = 0.38884741\n",
      "Iteration 110, loss = 0.38872745\n",
      "Iteration 111, loss = 0.38870791\n",
      "Iteration 112, loss = 0.38867087\n",
      "Iteration 113, loss = 0.38854336\n",
      "Iteration 114, loss = 0.38844698\n",
      "Iteration 115, loss = 0.38838069\n",
      "Iteration 116, loss = 0.38837904\n",
      "Iteration 117, loss = 0.38835944\n",
      "Iteration 118, loss = 0.38831949\n",
      "Iteration 119, loss = 0.38819759\n",
      "Iteration 120, loss = 0.38804056\n",
      "Iteration 121, loss = 0.38824456\n",
      "Iteration 122, loss = 0.38800353\n",
      "Iteration 123, loss = 0.38793776\n",
      "Iteration 124, loss = 0.38797175\n",
      "Iteration 125, loss = 0.38788929\n",
      "Iteration 126, loss = 0.38782334\n",
      "Iteration 127, loss = 0.38773105\n",
      "Iteration 128, loss = 0.38771325\n",
      "Iteration 129, loss = 0.38764456\n",
      "Iteration 130, loss = 0.38775353\n",
      "Iteration 131, loss = 0.38764092\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.74037364\n",
      "Iteration 2, loss = 0.69314620\n",
      "Iteration 3, loss = 0.69293661\n",
      "Iteration 4, loss = 0.69245696\n",
      "Iteration 5, loss = 0.69198824\n",
      "Iteration 6, loss = 0.69147232\n",
      "Iteration 7, loss = 0.69095758\n",
      "Iteration 8, loss = 0.69034918\n",
      "Iteration 9, loss = 0.68975372\n",
      "Iteration 10, loss = 0.68897171\n",
      "Iteration 11, loss = 0.68818670\n",
      "Iteration 12, loss = 0.68722029\n",
      "Iteration 13, loss = 0.68614072\n",
      "Iteration 14, loss = 0.68506394\n",
      "Iteration 15, loss = 0.68359091\n",
      "Iteration 16, loss = 0.68201078\n",
      "Iteration 17, loss = 0.68030164\n",
      "Iteration 18, loss = 0.67834182\n",
      "Iteration 19, loss = 0.67612752\n",
      "Iteration 20, loss = 0.67369118\n",
      "Iteration 21, loss = 0.67093592\n",
      "Iteration 22, loss = 0.66791856\n",
      "Iteration 23, loss = 0.66460956\n",
      "Iteration 24, loss = 0.66093107\n",
      "Iteration 25, loss = 0.65693881\n",
      "Iteration 26, loss = 0.65260852\n",
      "Iteration 27, loss = 0.64784016\n",
      "Iteration 28, loss = 0.64270858\n",
      "Iteration 29, loss = 0.63708745\n",
      "Iteration 30, loss = 0.63107846\n",
      "Iteration 31, loss = 0.62467271\n",
      "Iteration 32, loss = 0.61775879\n",
      "Iteration 33, loss = 0.61039610\n",
      "Iteration 34, loss = 0.60267594\n",
      "Iteration 35, loss = 0.59456907\n",
      "Iteration 36, loss = 0.58606094\n",
      "Iteration 37, loss = 0.57732901\n",
      "Iteration 38, loss = 0.56844187\n",
      "Iteration 39, loss = 0.55932444\n",
      "Iteration 40, loss = 0.55020157\n",
      "Iteration 41, loss = 0.54109906\n",
      "Iteration 42, loss = 0.53213636\n",
      "Iteration 43, loss = 0.52336359\n",
      "Iteration 44, loss = 0.51475311\n",
      "Iteration 45, loss = 0.50652373\n",
      "Iteration 46, loss = 0.49861032\n",
      "Iteration 47, loss = 0.49114975\n",
      "Iteration 48, loss = 0.48409358\n",
      "Iteration 49, loss = 0.47733100\n",
      "Iteration 50, loss = 0.47116285\n",
      "Iteration 51, loss = 0.46535437\n",
      "Iteration 52, loss = 0.45990904\n",
      "Iteration 53, loss = 0.45493008\n",
      "Iteration 54, loss = 0.45027009\n",
      "Iteration 55, loss = 0.44603437\n",
      "Iteration 56, loss = 0.44202206\n",
      "Iteration 57, loss = 0.43842532\n",
      "Iteration 58, loss = 0.43517859\n",
      "Iteration 59, loss = 0.43209583\n",
      "Iteration 60, loss = 0.42939384\n",
      "Iteration 61, loss = 0.42675066\n",
      "Iteration 62, loss = 0.42449652\n",
      "Iteration 63, loss = 0.42231996\n",
      "Iteration 64, loss = 0.42033834\n",
      "Iteration 65, loss = 0.41849063\n",
      "Iteration 66, loss = 0.41691952\n",
      "Iteration 67, loss = 0.41539476\n",
      "Iteration 68, loss = 0.41425825\n",
      "Iteration 69, loss = 0.41282333\n",
      "Iteration 70, loss = 0.41161764\n",
      "Iteration 71, loss = 0.41048499\n",
      "Iteration 72, loss = 0.40957658\n",
      "Iteration 73, loss = 0.40873032\n",
      "Iteration 74, loss = 0.40790955\n",
      "Iteration 75, loss = 0.40721724\n",
      "Iteration 76, loss = 0.40640217\n",
      "Iteration 77, loss = 0.40586522\n",
      "Iteration 78, loss = 0.40522749\n",
      "Iteration 79, loss = 0.40464493\n",
      "Iteration 80, loss = 0.40414580\n",
      "Iteration 81, loss = 0.40368651\n",
      "Iteration 82, loss = 0.40318470\n",
      "Iteration 83, loss = 0.40279437\n",
      "Iteration 84, loss = 0.40248787\n",
      "Iteration 85, loss = 0.40209203\n",
      "Iteration 86, loss = 0.40171913\n",
      "Iteration 87, loss = 0.40163549\n",
      "Iteration 88, loss = 0.40117591\n",
      "Iteration 89, loss = 0.40089159\n",
      "Iteration 90, loss = 0.40075160\n",
      "Iteration 91, loss = 0.40050933\n",
      "Iteration 92, loss = 0.40026399\n",
      "Iteration 93, loss = 0.40006983\n",
      "Iteration 94, loss = 0.39990686\n",
      "Iteration 95, loss = 0.39975811\n",
      "Iteration 96, loss = 0.39946912\n",
      "Iteration 97, loss = 0.39932585\n",
      "Iteration 98, loss = 0.39920082\n",
      "Iteration 99, loss = 0.39916711\n",
      "Iteration 100, loss = 0.39888943\n",
      "Iteration 101, loss = 0.39898189\n",
      "Iteration 102, loss = 0.39875449\n",
      "Iteration 103, loss = 0.39857911\n",
      "Iteration 104, loss = 0.39847832\n",
      "Iteration 105, loss = 0.39847658\n",
      "Iteration 106, loss = 0.39826945\n",
      "Iteration 107, loss = 0.39832292\n",
      "Iteration 108, loss = 0.39802905\n",
      "Iteration 109, loss = 0.39810083\n",
      "Iteration 110, loss = 0.39784284\n",
      "Iteration 111, loss = 0.39779760\n",
      "Iteration 112, loss = 0.39772569\n",
      "Iteration 113, loss = 0.39771045\n",
      "Iteration 114, loss = 0.39751815\n",
      "Iteration 115, loss = 0.39753549\n",
      "Iteration 116, loss = 0.39750574\n",
      "Iteration 117, loss = 0.39756902\n",
      "Iteration 118, loss = 0.39724295\n",
      "Iteration 119, loss = 0.39720329\n",
      "Iteration 120, loss = 0.39716825\n",
      "Iteration 121, loss = 0.39725776\n",
      "Iteration 122, loss = 0.39701219\n",
      "Iteration 123, loss = 0.39692013\n",
      "Iteration 124, loss = 0.39706603\n",
      "Iteration 125, loss = 0.39695655\n",
      "Iteration 126, loss = 0.39677174\n",
      "Iteration 127, loss = 0.39684133\n",
      "Iteration 128, loss = 0.39672614\n",
      "Iteration 129, loss = 0.39655887\n",
      "Iteration 130, loss = 0.39664336\n",
      "Iteration 131, loss = 0.39666656\n",
      "Iteration 132, loss = 0.39660017\n",
      "Iteration 133, loss = 0.39651677\n",
      "Iteration 134, loss = 0.39655741\n",
      "Iteration 135, loss = 0.39632835\n",
      "Iteration 136, loss = 0.39645396\n",
      "Iteration 137, loss = 0.39626043\n",
      "Iteration 138, loss = 0.39623326\n",
      "Iteration 139, loss = 0.39621889\n",
      "Iteration 140, loss = 0.39618143\n",
      "Iteration 141, loss = 0.39611154\n",
      "Iteration 142, loss = 0.39609184\n",
      "Iteration 143, loss = 0.39598384\n",
      "Iteration 144, loss = 0.39595282\n",
      "Iteration 145, loss = 0.39593301\n",
      "Iteration 146, loss = 0.39592128\n",
      "Iteration 147, loss = 0.39600565\n",
      "Iteration 148, loss = 0.39585154\n",
      "Iteration 149, loss = 0.39568555\n",
      "Iteration 150, loss = 0.39577630\n",
      "Iteration 151, loss = 0.39558319\n",
      "Iteration 152, loss = 0.39569351\n",
      "Iteration 153, loss = 0.39555857\n",
      "Iteration 154, loss = 0.39572855\n",
      "Iteration 155, loss = 0.39543430\n",
      "Iteration 156, loss = 0.39555157\n",
      "Iteration 157, loss = 0.39538903\n",
      "Iteration 158, loss = 0.39552249\n",
      "Iteration 159, loss = 0.39540903\n",
      "Iteration 160, loss = 0.39533796\n",
      "Iteration 161, loss = 0.39537071\n",
      "Iteration 162, loss = 0.39515980\n",
      "Iteration 163, loss = 0.39526414\n",
      "Iteration 164, loss = 0.39521217\n",
      "Iteration 165, loss = 0.39524095\n",
      "Iteration 166, loss = 0.39517556\n",
      "Iteration 167, loss = 0.39511864\n",
      "Iteration 168, loss = 0.39514010\n",
      "Iteration 169, loss = 0.39504237\n",
      "Iteration 170, loss = 0.39509003\n",
      "Iteration 171, loss = 0.39510417\n",
      "Iteration 172, loss = 0.39494767\n",
      "Iteration 173, loss = 0.39509816\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69549968\n",
      "Iteration 2, loss = 0.69311813\n",
      "Iteration 3, loss = 0.69278785\n",
      "Iteration 4, loss = 0.69258932\n",
      "Iteration 5, loss = 0.69226796\n",
      "Iteration 6, loss = 0.69198359\n",
      "Iteration 7, loss = 0.69177200\n",
      "Iteration 8, loss = 0.69134207\n",
      "Iteration 9, loss = 0.69102533\n",
      "Iteration 10, loss = 0.69059880\n",
      "Iteration 11, loss = 0.69011621\n",
      "Iteration 12, loss = 0.68965096\n",
      "Iteration 13, loss = 0.68907050\n",
      "Iteration 14, loss = 0.68849441\n",
      "Iteration 15, loss = 0.68764733\n",
      "Iteration 16, loss = 0.68684304\n",
      "Iteration 17, loss = 0.68586726\n",
      "Iteration 18, loss = 0.68486124\n",
      "Iteration 19, loss = 0.68350887\n",
      "Iteration 20, loss = 0.68212446\n",
      "Iteration 21, loss = 0.68062860\n",
      "Iteration 22, loss = 0.67886588\n",
      "Iteration 23, loss = 0.67690982\n",
      "Iteration 24, loss = 0.67474293\n",
      "Iteration 25, loss = 0.67224408\n",
      "Iteration 26, loss = 0.66955897\n",
      "Iteration 27, loss = 0.66650335\n",
      "Iteration 28, loss = 0.66340682\n",
      "Iteration 29, loss = 0.65963144\n",
      "Iteration 30, loss = 0.65562859\n",
      "Iteration 31, loss = 0.65121612\n",
      "Iteration 32, loss = 0.64650070\n",
      "Iteration 33, loss = 0.64133165\n",
      "Iteration 34, loss = 0.63561893\n",
      "Iteration 35, loss = 0.62951439\n",
      "Iteration 36, loss = 0.62292960\n",
      "Iteration 37, loss = 0.61583366\n",
      "Iteration 38, loss = 0.60822432\n",
      "Iteration 39, loss = 0.60027028\n",
      "Iteration 40, loss = 0.59178489\n",
      "Iteration 41, loss = 0.58298641\n",
      "Iteration 42, loss = 0.57376347\n",
      "Iteration 43, loss = 0.56423707\n",
      "Iteration 44, loss = 0.55461599\n",
      "Iteration 45, loss = 0.54492563\n",
      "Iteration 46, loss = 0.53522287\n",
      "Iteration 47, loss = 0.52562661\n",
      "Iteration 48, loss = 0.51614244\n",
      "Iteration 49, loss = 0.50693986\n",
      "Iteration 50, loss = 0.49807451\n",
      "Iteration 51, loss = 0.48953653\n",
      "Iteration 52, loss = 0.48137470\n",
      "Iteration 53, loss = 0.47369906\n",
      "Iteration 54, loss = 0.46647656\n",
      "Iteration 55, loss = 0.45967714\n",
      "Iteration 56, loss = 0.45343996\n",
      "Iteration 57, loss = 0.44760607\n",
      "Iteration 58, loss = 0.44216965\n",
      "Iteration 59, loss = 0.43723844\n",
      "Iteration 60, loss = 0.43244973\n",
      "Iteration 61, loss = 0.42821467\n",
      "Iteration 62, loss = 0.42435650\n",
      "Iteration 63, loss = 0.42068351\n",
      "Iteration 64, loss = 0.41736805\n",
      "Iteration 65, loss = 0.41436713\n",
      "Iteration 66, loss = 0.41161013\n",
      "Iteration 67, loss = 0.40894748\n",
      "Iteration 68, loss = 0.40674352\n",
      "Iteration 69, loss = 0.40456984\n",
      "Iteration 70, loss = 0.40258338\n",
      "Iteration 71, loss = 0.40089743\n",
      "Iteration 72, loss = 0.39913697\n",
      "Iteration 73, loss = 0.39787938\n",
      "Iteration 74, loss = 0.39624673\n",
      "Iteration 75, loss = 0.39507927\n",
      "Iteration 76, loss = 0.39382016\n",
      "Iteration 77, loss = 0.39278018\n",
      "Iteration 78, loss = 0.39190762\n",
      "Iteration 79, loss = 0.39098090\n",
      "Iteration 80, loss = 0.39015828\n",
      "Iteration 81, loss = 0.38950718\n",
      "Iteration 82, loss = 0.38870701\n",
      "Iteration 83, loss = 0.38799729\n",
      "Iteration 84, loss = 0.38759847\n",
      "Iteration 85, loss = 0.38690652\n",
      "Iteration 86, loss = 0.38650643\n",
      "Iteration 87, loss = 0.38590973\n",
      "Iteration 88, loss = 0.38572826\n",
      "Iteration 89, loss = 0.38516326\n",
      "Iteration 90, loss = 0.38486444\n",
      "Iteration 91, loss = 0.38446494\n",
      "Iteration 92, loss = 0.38411447\n",
      "Iteration 93, loss = 0.38384567\n",
      "Iteration 94, loss = 0.38361714\n",
      "Iteration 95, loss = 0.38337310\n",
      "Iteration 96, loss = 0.38294893\n",
      "Iteration 97, loss = 0.38281446\n",
      "Iteration 98, loss = 0.38252568\n",
      "Iteration 99, loss = 0.38250031\n",
      "Iteration 100, loss = 0.38218536\n",
      "Iteration 101, loss = 0.38209304\n",
      "Iteration 102, loss = 0.38200058\n",
      "Iteration 103, loss = 0.38181015\n",
      "Iteration 104, loss = 0.38164865\n",
      "Iteration 105, loss = 0.38143172\n",
      "Iteration 106, loss = 0.38143565\n",
      "Iteration 107, loss = 0.38115596\n",
      "Iteration 108, loss = 0.38106819\n",
      "Iteration 109, loss = 0.38106810\n",
      "Iteration 110, loss = 0.38093105\n",
      "Iteration 111, loss = 0.38074126\n",
      "Iteration 112, loss = 0.38072571\n",
      "Iteration 113, loss = 0.38059409\n",
      "Iteration 114, loss = 0.38061782\n",
      "Iteration 115, loss = 0.38047649\n",
      "Iteration 116, loss = 0.38036593\n",
      "Iteration 117, loss = 0.38023735\n",
      "Iteration 118, loss = 0.38041902\n",
      "Iteration 119, loss = 0.38023961\n",
      "Iteration 120, loss = 0.38015228\n",
      "Iteration 121, loss = 0.38012212\n",
      "Iteration 122, loss = 0.38004091\n",
      "Iteration 123, loss = 0.38001667\n",
      "Iteration 124, loss = 0.37990360\n",
      "Iteration 125, loss = 0.37984089\n",
      "Iteration 126, loss = 0.37978246\n",
      "Iteration 127, loss = 0.37972396\n",
      "Iteration 128, loss = 0.37966607\n",
      "Iteration 129, loss = 0.37963665\n",
      "Iteration 130, loss = 0.37974646\n",
      "Iteration 131, loss = 0.37967258\n",
      "Iteration 132, loss = 0.37955265\n",
      "Iteration 133, loss = 0.37965900\n",
      "Iteration 134, loss = 0.37946096\n",
      "Iteration 135, loss = 0.37945797\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69603343\n",
      "Iteration 2, loss = 0.69301775\n",
      "Iteration 3, loss = 0.69276445\n",
      "Iteration 4, loss = 0.69252754\n",
      "Iteration 5, loss = 0.69228974\n",
      "Iteration 6, loss = 0.69199972\n",
      "Iteration 7, loss = 0.69172122\n",
      "Iteration 8, loss = 0.69126730\n",
      "Iteration 9, loss = 0.69099014\n",
      "Iteration 10, loss = 0.69053951\n",
      "Iteration 11, loss = 0.69017169\n",
      "Iteration 12, loss = 0.68959509\n",
      "Iteration 13, loss = 0.68898107\n",
      "Iteration 14, loss = 0.68845252\n",
      "Iteration 15, loss = 0.68766460\n",
      "Iteration 16, loss = 0.68671751\n",
      "Iteration 17, loss = 0.68581028\n",
      "Iteration 18, loss = 0.68474033\n",
      "Iteration 19, loss = 0.68344947\n",
      "Iteration 20, loss = 0.68202406\n",
      "Iteration 21, loss = 0.68051894\n",
      "Iteration 22, loss = 0.67878337\n",
      "Iteration 23, loss = 0.67682986\n",
      "Iteration 24, loss = 0.67466654\n",
      "Iteration 25, loss = 0.67216626\n",
      "Iteration 26, loss = 0.66951943\n",
      "Iteration 27, loss = 0.66647434\n",
      "Iteration 28, loss = 0.66332182\n",
      "Iteration 29, loss = 0.65965871\n",
      "Iteration 30, loss = 0.65571140\n",
      "Iteration 31, loss = 0.65137751\n",
      "Iteration 32, loss = 0.64668357\n",
      "Iteration 33, loss = 0.64147904\n",
      "Iteration 34, loss = 0.63594371\n",
      "Iteration 35, loss = 0.62990105\n",
      "Iteration 36, loss = 0.62345348\n",
      "Iteration 37, loss = 0.61645032\n",
      "Iteration 38, loss = 0.60908250\n",
      "Iteration 39, loss = 0.60121946\n",
      "Iteration 40, loss = 0.59296101\n",
      "Iteration 41, loss = 0.58437351\n",
      "Iteration 42, loss = 0.57537194\n",
      "Iteration 43, loss = 0.56616793\n",
      "Iteration 44, loss = 0.55685544\n",
      "Iteration 45, loss = 0.54740777\n",
      "Iteration 46, loss = 0.53801805\n",
      "Iteration 47, loss = 0.52873348\n",
      "Iteration 48, loss = 0.51955724\n",
      "Iteration 49, loss = 0.51066766\n",
      "Iteration 50, loss = 0.50210477\n",
      "Iteration 51, loss = 0.49392608\n",
      "Iteration 52, loss = 0.48616788\n",
      "Iteration 53, loss = 0.47873547\n",
      "Iteration 54, loss = 0.47176518\n",
      "Iteration 55, loss = 0.46521660\n",
      "Iteration 56, loss = 0.45921728\n",
      "Iteration 57, loss = 0.45357466\n",
      "Iteration 58, loss = 0.44826595\n",
      "Iteration 59, loss = 0.44344695\n",
      "Iteration 60, loss = 0.43902092\n",
      "Iteration 61, loss = 0.43498984\n",
      "Iteration 62, loss = 0.43115762\n",
      "Iteration 63, loss = 0.42773376\n",
      "Iteration 64, loss = 0.42456148\n",
      "Iteration 65, loss = 0.42171985\n",
      "Iteration 66, loss = 0.41909534\n",
      "Iteration 67, loss = 0.41642005\n",
      "Iteration 68, loss = 0.41415926\n",
      "Iteration 69, loss = 0.41209896\n",
      "Iteration 70, loss = 0.41022527\n",
      "Iteration 71, loss = 0.40855296\n",
      "Iteration 72, loss = 0.40694981\n",
      "Iteration 73, loss = 0.40555381\n",
      "Iteration 74, loss = 0.40416693\n",
      "Iteration 75, loss = 0.40296176\n",
      "Iteration 76, loss = 0.40191997\n",
      "Iteration 77, loss = 0.40090547\n",
      "Iteration 78, loss = 0.39996726\n",
      "Iteration 79, loss = 0.39904753\n",
      "Iteration 80, loss = 0.39826632\n",
      "Iteration 81, loss = 0.39758109\n",
      "Iteration 82, loss = 0.39687402\n",
      "Iteration 83, loss = 0.39615300\n",
      "Iteration 84, loss = 0.39558264\n",
      "Iteration 85, loss = 0.39510174\n",
      "Iteration 86, loss = 0.39456926\n",
      "Iteration 87, loss = 0.39410198\n",
      "Iteration 88, loss = 0.39382559\n",
      "Iteration 89, loss = 0.39327341\n",
      "Iteration 90, loss = 0.39315783\n",
      "Iteration 91, loss = 0.39261127\n",
      "Iteration 92, loss = 0.39229320\n",
      "Iteration 93, loss = 0.39196721\n",
      "Iteration 94, loss = 0.39186518\n",
      "Iteration 95, loss = 0.39147482\n",
      "Iteration 96, loss = 0.39134127\n",
      "Iteration 97, loss = 0.39104662\n",
      "Iteration 98, loss = 0.39102638\n",
      "Iteration 99, loss = 0.39070619\n",
      "Iteration 100, loss = 0.39049641\n",
      "Iteration 101, loss = 0.39037557\n",
      "Iteration 102, loss = 0.39012612\n",
      "Iteration 103, loss = 0.39007769\n",
      "Iteration 104, loss = 0.38992339\n",
      "Iteration 105, loss = 0.38977093\n",
      "Iteration 106, loss = 0.38975034\n",
      "Iteration 107, loss = 0.38951771\n",
      "Iteration 108, loss = 0.38939598\n",
      "Iteration 109, loss = 0.38928589\n",
      "Iteration 110, loss = 0.38922381\n",
      "Iteration 111, loss = 0.38908420\n",
      "Iteration 112, loss = 0.38915109\n",
      "Iteration 113, loss = 0.38890357\n",
      "Iteration 114, loss = 0.38879305\n",
      "Iteration 115, loss = 0.38872591\n",
      "Iteration 116, loss = 0.38862790\n",
      "Iteration 117, loss = 0.38855380\n",
      "Iteration 118, loss = 0.38860331\n",
      "Iteration 119, loss = 0.38852182\n",
      "Iteration 120, loss = 0.38837397\n",
      "Iteration 121, loss = 0.38824562\n",
      "Iteration 122, loss = 0.38825857\n",
      "Iteration 123, loss = 0.38822145\n",
      "Iteration 124, loss = 0.38826326\n",
      "Iteration 125, loss = 0.38818403\n",
      "Iteration 126, loss = 0.38794932\n",
      "Iteration 127, loss = 0.38814727\n",
      "Iteration 128, loss = 0.38800304\n",
      "Iteration 129, loss = 0.38799401\n",
      "Iteration 130, loss = 0.38799487\n",
      "Iteration 131, loss = 0.38781761\n",
      "Iteration 132, loss = 0.38799357\n",
      "Iteration 133, loss = 0.38775834\n",
      "Iteration 134, loss = 0.38764907\n",
      "Iteration 135, loss = 0.38765020\n",
      "Iteration 136, loss = 0.38754209\n",
      "Iteration 137, loss = 0.38768810\n",
      "Iteration 138, loss = 0.38752404\n",
      "Iteration 139, loss = 0.38745297\n",
      "Iteration 140, loss = 0.38739687\n",
      "Iteration 141, loss = 0.38749338\n",
      "Iteration 142, loss = 0.38734804\n",
      "Iteration 143, loss = 0.38738702\n",
      "Iteration 144, loss = 0.38735835\n",
      "Iteration 145, loss = 0.38729785\n",
      "Iteration 146, loss = 0.38724577\n",
      "Iteration 147, loss = 0.38729211\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69645848\n",
      "Iteration 2, loss = 0.69306284\n",
      "Iteration 3, loss = 0.69279799\n",
      "Iteration 4, loss = 0.69257446\n",
      "Iteration 5, loss = 0.69231476\n",
      "Iteration 6, loss = 0.69199745\n",
      "Iteration 7, loss = 0.69167389\n",
      "Iteration 8, loss = 0.69131966\n",
      "Iteration 9, loss = 0.69096949\n",
      "Iteration 10, loss = 0.69059426\n",
      "Iteration 11, loss = 0.69012400\n",
      "Iteration 12, loss = 0.68960915\n",
      "Iteration 13, loss = 0.68911738\n",
      "Iteration 14, loss = 0.68841409\n",
      "Iteration 15, loss = 0.68766111\n",
      "Iteration 16, loss = 0.68685770\n",
      "Iteration 17, loss = 0.68592072\n",
      "Iteration 18, loss = 0.68485493\n",
      "Iteration 19, loss = 0.68374110\n",
      "Iteration 20, loss = 0.68234201\n",
      "Iteration 21, loss = 0.68087593\n",
      "Iteration 22, loss = 0.67917785\n",
      "Iteration 23, loss = 0.67731056\n",
      "Iteration 24, loss = 0.67519625\n",
      "Iteration 25, loss = 0.67285205\n",
      "Iteration 26, loss = 0.67017966\n",
      "Iteration 27, loss = 0.66742095\n",
      "Iteration 28, loss = 0.66423804\n",
      "Iteration 29, loss = 0.66077493\n",
      "Iteration 30, loss = 0.65699954\n",
      "Iteration 31, loss = 0.65284989\n",
      "Iteration 32, loss = 0.64837650\n",
      "Iteration 33, loss = 0.64341089\n",
      "Iteration 34, loss = 0.63813526\n",
      "Iteration 35, loss = 0.63232272\n",
      "Iteration 36, loss = 0.62624987\n",
      "Iteration 37, loss = 0.61961585\n",
      "Iteration 38, loss = 0.61244895\n",
      "Iteration 39, loss = 0.60498354\n",
      "Iteration 40, loss = 0.59705130\n",
      "Iteration 41, loss = 0.58872539\n",
      "Iteration 42, loss = 0.58024873\n",
      "Iteration 43, loss = 0.57139774\n",
      "Iteration 44, loss = 0.56247960\n",
      "Iteration 45, loss = 0.55340922\n",
      "Iteration 46, loss = 0.54433975\n",
      "Iteration 47, loss = 0.53540848\n",
      "Iteration 48, loss = 0.52647057\n",
      "Iteration 49, loss = 0.51782921\n",
      "Iteration 50, loss = 0.50960262\n",
      "Iteration 51, loss = 0.50154352\n",
      "Iteration 52, loss = 0.49380765\n",
      "Iteration 53, loss = 0.48663112\n",
      "Iteration 54, loss = 0.47985437\n",
      "Iteration 55, loss = 0.47349339\n",
      "Iteration 56, loss = 0.46745336\n",
      "Iteration 57, loss = 0.46184677\n",
      "Iteration 58, loss = 0.45676899\n",
      "Iteration 59, loss = 0.45202698\n",
      "Iteration 60, loss = 0.44753852\n",
      "Iteration 61, loss = 0.44355337\n",
      "Iteration 62, loss = 0.43974848\n",
      "Iteration 63, loss = 0.43632937\n",
      "Iteration 64, loss = 0.43315892\n",
      "Iteration 65, loss = 0.43026656\n",
      "Iteration 66, loss = 0.42764789\n",
      "Iteration 67, loss = 0.42527033\n",
      "Iteration 68, loss = 0.42301302\n",
      "Iteration 69, loss = 0.42100194\n",
      "Iteration 70, loss = 0.41914324\n",
      "Iteration 71, loss = 0.41750863\n",
      "Iteration 72, loss = 0.41594008\n",
      "Iteration 73, loss = 0.41468604\n",
      "Iteration 74, loss = 0.41317781\n",
      "Iteration 75, loss = 0.41204708\n",
      "Iteration 76, loss = 0.41087906\n",
      "Iteration 77, loss = 0.40990385\n",
      "Iteration 78, loss = 0.40896364\n",
      "Iteration 79, loss = 0.40808319\n",
      "Iteration 80, loss = 0.40738672\n",
      "Iteration 81, loss = 0.40674172\n",
      "Iteration 82, loss = 0.40597688\n",
      "Iteration 83, loss = 0.40536113\n",
      "Iteration 84, loss = 0.40483841\n",
      "Iteration 85, loss = 0.40422623\n",
      "Iteration 86, loss = 0.40379802\n",
      "Iteration 87, loss = 0.40331187\n",
      "Iteration 88, loss = 0.40292198\n",
      "Iteration 89, loss = 0.40260397\n",
      "Iteration 90, loss = 0.40228229\n",
      "Iteration 91, loss = 0.40199631\n",
      "Iteration 92, loss = 0.40154149\n",
      "Iteration 93, loss = 0.40121364\n",
      "Iteration 94, loss = 0.40094915\n",
      "Iteration 95, loss = 0.40085463\n",
      "Iteration 96, loss = 0.40049453\n",
      "Iteration 97, loss = 0.40034958\n",
      "Iteration 98, loss = 0.40006551\n",
      "Iteration 99, loss = 0.39998800\n",
      "Iteration 100, loss = 0.39968850\n",
      "Iteration 101, loss = 0.39952171\n",
      "Iteration 102, loss = 0.39944392\n",
      "Iteration 103, loss = 0.39908055\n",
      "Iteration 104, loss = 0.39920908\n",
      "Iteration 105, loss = 0.39894609\n",
      "Iteration 106, loss = 0.39879068\n",
      "Iteration 107, loss = 0.39866208\n",
      "Iteration 108, loss = 0.39852762\n",
      "Iteration 109, loss = 0.39852022\n",
      "Iteration 110, loss = 0.39841163\n",
      "Iteration 111, loss = 0.39820622\n",
      "Iteration 112, loss = 0.39808129\n",
      "Iteration 113, loss = 0.39806418\n",
      "Iteration 114, loss = 0.39795906\n",
      "Iteration 115, loss = 0.39787841\n",
      "Iteration 116, loss = 0.39785059\n",
      "Iteration 117, loss = 0.39771496\n",
      "Iteration 118, loss = 0.39764025\n",
      "Iteration 119, loss = 0.39763300\n",
      "Iteration 120, loss = 0.39747753\n",
      "Iteration 121, loss = 0.39750558\n",
      "Iteration 122, loss = 0.39765155\n",
      "Iteration 123, loss = 0.39730824\n",
      "Iteration 124, loss = 0.39719147\n",
      "Iteration 125, loss = 0.39716433\n",
      "Iteration 126, loss = 0.39702954\n",
      "Iteration 127, loss = 0.39707175\n",
      "Iteration 128, loss = 0.39687717\n",
      "Iteration 129, loss = 0.39697347\n",
      "Iteration 130, loss = 0.39677511\n",
      "Iteration 131, loss = 0.39693280\n",
      "Iteration 132, loss = 0.39677417\n",
      "Iteration 133, loss = 0.39668980\n",
      "Iteration 134, loss = 0.39690915\n",
      "Iteration 135, loss = 0.39658757\n",
      "Iteration 136, loss = 0.39653994\n",
      "Iteration 137, loss = 0.39646525\n",
      "Iteration 138, loss = 0.39643117\n",
      "Iteration 139, loss = 0.39633648\n",
      "Iteration 140, loss = 0.39643607\n",
      "Iteration 141, loss = 0.39626250\n",
      "Iteration 142, loss = 0.39620745\n",
      "Iteration 143, loss = 0.39617829\n",
      "Iteration 144, loss = 0.39612068\n",
      "Iteration 145, loss = 0.39611894\n",
      "Iteration 146, loss = 0.39611136\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.67975992\n",
      "Iteration 2, loss = 0.65394503\n",
      "Iteration 3, loss = 0.64025174\n",
      "Iteration 4, loss = 0.62744581\n",
      "Iteration 5, loss = 0.61550372\n",
      "Iteration 6, loss = 0.60434211\n",
      "Iteration 7, loss = 0.59394855\n",
      "Iteration 8, loss = 0.58423548\n",
      "Iteration 9, loss = 0.57510189\n",
      "Iteration 10, loss = 0.56654467\n",
      "Iteration 11, loss = 0.55856361\n",
      "Iteration 12, loss = 0.55101141\n",
      "Iteration 13, loss = 0.54389842\n",
      "Iteration 14, loss = 0.53727232\n",
      "Iteration 15, loss = 0.53097686\n",
      "Iteration 16, loss = 0.52510123\n",
      "Iteration 17, loss = 0.51951399\n",
      "Iteration 18, loss = 0.51425465\n",
      "Iteration 19, loss = 0.50923013\n",
      "Iteration 20, loss = 0.50451989\n",
      "Iteration 21, loss = 0.50003258\n",
      "Iteration 22, loss = 0.49578168\n",
      "Iteration 23, loss = 0.49176602\n",
      "Iteration 24, loss = 0.48798216\n",
      "Iteration 25, loss = 0.48426453\n",
      "Iteration 26, loss = 0.48075867\n",
      "Iteration 27, loss = 0.47746204\n",
      "Iteration 28, loss = 0.47428984\n",
      "Iteration 29, loss = 0.47125927\n",
      "Iteration 30, loss = 0.46838446\n",
      "Iteration 31, loss = 0.46567031\n",
      "Iteration 32, loss = 0.46302711\n",
      "Iteration 33, loss = 0.46051512\n",
      "Iteration 34, loss = 0.45810512\n",
      "Iteration 35, loss = 0.45577369\n",
      "Iteration 36, loss = 0.45359453\n",
      "Iteration 37, loss = 0.45144778\n",
      "Iteration 38, loss = 0.44942404\n",
      "Iteration 39, loss = 0.44744858\n",
      "Iteration 40, loss = 0.44555896\n",
      "Iteration 41, loss = 0.44374348\n",
      "Iteration 42, loss = 0.44199862\n",
      "Iteration 43, loss = 0.44031514\n",
      "Iteration 44, loss = 0.43868818\n",
      "Iteration 45, loss = 0.43713297\n",
      "Iteration 46, loss = 0.43564446\n",
      "Iteration 47, loss = 0.43418430\n",
      "Iteration 48, loss = 0.43277229\n",
      "Iteration 49, loss = 0.43142580\n",
      "Iteration 50, loss = 0.43011476\n",
      "Iteration 51, loss = 0.42887781\n",
      "Iteration 52, loss = 0.42767442\n",
      "Iteration 53, loss = 0.42649751\n",
      "Iteration 54, loss = 0.42534053\n",
      "Iteration 55, loss = 0.42424997\n",
      "Iteration 56, loss = 0.42318692\n",
      "Iteration 57, loss = 0.42219489\n",
      "Iteration 58, loss = 0.42115542\n",
      "Iteration 59, loss = 0.42019172\n",
      "Iteration 60, loss = 0.41927148\n",
      "Iteration 61, loss = 0.41837301\n",
      "Iteration 62, loss = 0.41746161\n",
      "Iteration 63, loss = 0.41660835\n",
      "Iteration 64, loss = 0.41577336\n",
      "Iteration 65, loss = 0.41495857\n",
      "Iteration 66, loss = 0.41417224\n",
      "Iteration 67, loss = 0.41342853\n",
      "Iteration 68, loss = 0.41267280\n",
      "Iteration 69, loss = 0.41197754\n",
      "Iteration 70, loss = 0.41126790\n",
      "Iteration 71, loss = 0.41057622\n",
      "Iteration 72, loss = 0.40991620\n",
      "Iteration 73, loss = 0.40926147\n",
      "Iteration 74, loss = 0.40864785\n",
      "Iteration 75, loss = 0.40803176\n",
      "Iteration 76, loss = 0.40743332\n",
      "Iteration 77, loss = 0.40686185\n",
      "Iteration 78, loss = 0.40628510\n",
      "Iteration 79, loss = 0.40574867\n",
      "Iteration 80, loss = 0.40524613\n",
      "Iteration 81, loss = 0.40471775\n",
      "Iteration 82, loss = 0.40418212\n",
      "Iteration 83, loss = 0.40369929\n",
      "Iteration 84, loss = 0.40321324\n",
      "Iteration 85, loss = 0.40273819\n",
      "Iteration 86, loss = 0.40229465\n",
      "Iteration 87, loss = 0.40183837\n",
      "Iteration 88, loss = 0.40143566\n",
      "Iteration 89, loss = 0.40096474\n",
      "Iteration 90, loss = 0.40054311\n",
      "Iteration 91, loss = 0.40014653\n",
      "Iteration 92, loss = 0.39973768\n",
      "Iteration 93, loss = 0.39935729\n",
      "Iteration 94, loss = 0.39897832\n",
      "Iteration 95, loss = 0.39860988\n",
      "Iteration 96, loss = 0.39823472\n",
      "Iteration 97, loss = 0.39789983\n",
      "Iteration 98, loss = 0.39755490\n",
      "Iteration 99, loss = 0.39723846\n",
      "Iteration 100, loss = 0.39688395\n",
      "Iteration 101, loss = 0.39654767\n",
      "Iteration 102, loss = 0.39625585\n",
      "Iteration 103, loss = 0.39601071\n",
      "Iteration 104, loss = 0.39563180\n",
      "Iteration 105, loss = 0.39532177\n",
      "Iteration 106, loss = 0.39507569\n",
      "Iteration 107, loss = 0.39475189\n",
      "Iteration 108, loss = 0.39450141\n",
      "Iteration 109, loss = 0.39421241\n",
      "Iteration 110, loss = 0.39394840\n",
      "Iteration 111, loss = 0.39366603\n",
      "Iteration 112, loss = 0.39342268\n",
      "Iteration 113, loss = 0.39315615\n",
      "Iteration 114, loss = 0.39292325\n",
      "Iteration 115, loss = 0.39267577\n",
      "Iteration 116, loss = 0.39246617\n",
      "Iteration 117, loss = 0.39222105\n",
      "Iteration 118, loss = 0.39200429\n",
      "Iteration 119, loss = 0.39178613\n",
      "Iteration 120, loss = 0.39158150\n",
      "Iteration 121, loss = 0.39135617\n",
      "Iteration 122, loss = 0.39113210\n",
      "Iteration 123, loss = 0.39091701\n",
      "Iteration 124, loss = 0.39071917\n",
      "Iteration 125, loss = 0.39051080\n",
      "Iteration 126, loss = 0.39035415\n",
      "Iteration 127, loss = 0.39017766\n",
      "Iteration 128, loss = 0.38995232\n",
      "Iteration 129, loss = 0.38977825\n",
      "Iteration 130, loss = 0.38962438\n",
      "Iteration 131, loss = 0.38941100\n",
      "Iteration 132, loss = 0.38924658\n",
      "Iteration 133, loss = 0.38906564\n",
      "Iteration 134, loss = 0.38890162\n",
      "Iteration 135, loss = 0.38874713\n",
      "Iteration 136, loss = 0.38857102\n",
      "Iteration 137, loss = 0.38844559\n",
      "Iteration 138, loss = 0.38827135\n",
      "Iteration 139, loss = 0.38815425\n",
      "Iteration 140, loss = 0.38797416\n",
      "Iteration 141, loss = 0.38782343\n",
      "Iteration 142, loss = 0.38767555\n",
      "Iteration 143, loss = 0.38753662\n",
      "Iteration 144, loss = 0.38741485\n",
      "Iteration 145, loss = 0.38728399\n",
      "Iteration 146, loss = 0.38713573\n",
      "Iteration 147, loss = 0.38701585\n",
      "Iteration 148, loss = 0.38688215\n",
      "Iteration 149, loss = 0.38679029\n",
      "Iteration 150, loss = 0.38661751\n",
      "Iteration 151, loss = 0.38648722\n",
      "Iteration 152, loss = 0.38638827\n",
      "Iteration 153, loss = 0.38625768\n",
      "Iteration 154, loss = 0.38613265\n",
      "Iteration 155, loss = 0.38602040\n",
      "Iteration 156, loss = 0.38591937\n",
      "Iteration 157, loss = 0.38579558\n",
      "Iteration 158, loss = 0.38569763\n",
      "Iteration 159, loss = 0.38559092\n",
      "Iteration 160, loss = 0.38549775\n",
      "Iteration 161, loss = 0.38536754\n",
      "Iteration 162, loss = 0.38527754\n",
      "Iteration 163, loss = 0.38518563\n",
      "Iteration 164, loss = 0.38507237\n",
      "Iteration 165, loss = 0.38500443\n",
      "Iteration 166, loss = 0.38486681\n",
      "Iteration 167, loss = 0.38479484\n",
      "Iteration 168, loss = 0.38469886\n",
      "Iteration 169, loss = 0.38462863\n",
      "Iteration 170, loss = 0.38450799\n",
      "Iteration 171, loss = 0.38442680\n",
      "Iteration 172, loss = 0.38432120\n",
      "Iteration 173, loss = 0.38423057\n",
      "Iteration 174, loss = 0.38415343\n",
      "Iteration 175, loss = 0.38406531\n",
      "Iteration 176, loss = 0.38397813\n",
      "Iteration 177, loss = 0.38389846\n",
      "Iteration 178, loss = 0.38382336\n",
      "Iteration 179, loss = 0.38374538\n",
      "Iteration 180, loss = 0.38366613\n",
      "Iteration 181, loss = 0.38359614\n",
      "Iteration 182, loss = 0.38352467\n",
      "Iteration 183, loss = 0.38344031\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.68096745\n",
      "Iteration 2, loss = 0.65469753\n",
      "Iteration 3, loss = 0.64100819\n",
      "Iteration 4, loss = 0.62834740\n",
      "Iteration 5, loss = 0.61659484\n",
      "Iteration 6, loss = 0.60551340\n",
      "Iteration 7, loss = 0.59529300\n",
      "Iteration 8, loss = 0.58568962\n",
      "Iteration 9, loss = 0.57673775\n",
      "Iteration 10, loss = 0.56831809\n",
      "Iteration 11, loss = 0.56045394\n",
      "Iteration 12, loss = 0.55307844\n",
      "Iteration 13, loss = 0.54614795\n",
      "Iteration 14, loss = 0.53965212\n",
      "Iteration 15, loss = 0.53350272\n",
      "Iteration 16, loss = 0.52775118\n",
      "Iteration 17, loss = 0.52230853\n",
      "Iteration 18, loss = 0.51716000\n",
      "Iteration 19, loss = 0.51232664\n",
      "Iteration 20, loss = 0.50770218\n",
      "Iteration 21, loss = 0.50340349\n",
      "Iteration 22, loss = 0.49928523\n",
      "Iteration 23, loss = 0.49531844\n",
      "Iteration 24, loss = 0.49159389\n",
      "Iteration 25, loss = 0.48807535\n",
      "Iteration 26, loss = 0.48469710\n",
      "Iteration 27, loss = 0.48152682\n",
      "Iteration 28, loss = 0.47846663\n",
      "Iteration 29, loss = 0.47552134\n",
      "Iteration 30, loss = 0.47278048\n",
      "Iteration 31, loss = 0.47011168\n",
      "Iteration 32, loss = 0.46756721\n",
      "Iteration 33, loss = 0.46513418\n",
      "Iteration 34, loss = 0.46277505\n",
      "Iteration 35, loss = 0.46054747\n",
      "Iteration 36, loss = 0.45841713\n",
      "Iteration 37, loss = 0.45636615\n",
      "Iteration 38, loss = 0.45441651\n",
      "Iteration 39, loss = 0.45254788\n",
      "Iteration 40, loss = 0.45074251\n",
      "Iteration 41, loss = 0.44902242\n",
      "Iteration 42, loss = 0.44733150\n",
      "Iteration 43, loss = 0.44569676\n",
      "Iteration 44, loss = 0.44422517\n",
      "Iteration 45, loss = 0.44263920\n",
      "Iteration 46, loss = 0.44121583\n",
      "Iteration 47, loss = 0.43983401\n",
      "Iteration 48, loss = 0.43848258\n",
      "Iteration 49, loss = 0.43719954\n",
      "Iteration 50, loss = 0.43594846\n",
      "Iteration 51, loss = 0.43478730\n",
      "Iteration 52, loss = 0.43357975\n",
      "Iteration 53, loss = 0.43247088\n",
      "Iteration 54, loss = 0.43137450\n",
      "Iteration 55, loss = 0.43035887\n",
      "Iteration 56, loss = 0.42929738\n",
      "Iteration 57, loss = 0.42832837\n",
      "Iteration 58, loss = 0.42737085\n",
      "Iteration 59, loss = 0.42643860\n",
      "Iteration 60, loss = 0.42556361\n",
      "Iteration 61, loss = 0.42472270\n",
      "Iteration 62, loss = 0.42382953\n",
      "Iteration 63, loss = 0.42302247\n",
      "Iteration 64, loss = 0.42223520\n",
      "Iteration 65, loss = 0.42146022\n",
      "Iteration 66, loss = 0.42071577\n",
      "Iteration 67, loss = 0.42002390\n",
      "Iteration 68, loss = 0.41931070\n",
      "Iteration 69, loss = 0.41862675\n",
      "Iteration 70, loss = 0.41795749\n",
      "Iteration 71, loss = 0.41732392\n",
      "Iteration 72, loss = 0.41666434\n",
      "Iteration 73, loss = 0.41604113\n",
      "Iteration 74, loss = 0.41546091\n",
      "Iteration 75, loss = 0.41486290\n",
      "Iteration 76, loss = 0.41433573\n",
      "Iteration 77, loss = 0.41380042\n",
      "Iteration 78, loss = 0.41322637\n",
      "Iteration 79, loss = 0.41269842\n",
      "Iteration 80, loss = 0.41221821\n",
      "Iteration 81, loss = 0.41174334\n",
      "Iteration 82, loss = 0.41122828\n",
      "Iteration 83, loss = 0.41075244\n",
      "Iteration 84, loss = 0.41030182\n",
      "Iteration 85, loss = 0.40983739\n",
      "Iteration 86, loss = 0.40941621\n",
      "Iteration 87, loss = 0.40898637\n",
      "Iteration 88, loss = 0.40860814\n",
      "Iteration 89, loss = 0.40816173\n",
      "Iteration 90, loss = 0.40780168\n",
      "Iteration 91, loss = 0.40739669\n",
      "Iteration 92, loss = 0.40700270\n",
      "Iteration 93, loss = 0.40663664\n",
      "Iteration 94, loss = 0.40631053\n",
      "Iteration 95, loss = 0.40592954\n",
      "Iteration 96, loss = 0.40559791\n",
      "Iteration 97, loss = 0.40530185\n",
      "Iteration 98, loss = 0.40495105\n",
      "Iteration 99, loss = 0.40464713\n",
      "Iteration 100, loss = 0.40430916\n",
      "Iteration 101, loss = 0.40399217\n",
      "Iteration 102, loss = 0.40369241\n",
      "Iteration 103, loss = 0.40339014\n",
      "Iteration 104, loss = 0.40316298\n",
      "Iteration 105, loss = 0.40282448\n",
      "Iteration 106, loss = 0.40256740\n",
      "Iteration 107, loss = 0.40230118\n",
      "Iteration 108, loss = 0.40204782\n",
      "Iteration 109, loss = 0.40180161\n",
      "Iteration 110, loss = 0.40157117\n",
      "Iteration 111, loss = 0.40128480\n",
      "Iteration 112, loss = 0.40102933\n",
      "Iteration 113, loss = 0.40079935\n",
      "Iteration 114, loss = 0.40055583\n",
      "Iteration 115, loss = 0.40036770\n",
      "Iteration 116, loss = 0.40018141\n",
      "Iteration 117, loss = 0.39989102\n",
      "Iteration 118, loss = 0.39969692\n",
      "Iteration 119, loss = 0.39951670\n",
      "Iteration 120, loss = 0.39928459\n",
      "Iteration 121, loss = 0.39908037\n",
      "Iteration 122, loss = 0.39889245\n",
      "Iteration 123, loss = 0.39868796\n",
      "Iteration 124, loss = 0.39849273\n",
      "Iteration 125, loss = 0.39830957\n",
      "Iteration 126, loss = 0.39813553\n",
      "Iteration 127, loss = 0.39795802\n",
      "Iteration 128, loss = 0.39776601\n",
      "Iteration 129, loss = 0.39761456\n",
      "Iteration 130, loss = 0.39743617\n",
      "Iteration 131, loss = 0.39726661\n",
      "Iteration 132, loss = 0.39709438\n",
      "Iteration 133, loss = 0.39694628\n",
      "Iteration 134, loss = 0.39679043\n",
      "Iteration 135, loss = 0.39663167\n",
      "Iteration 136, loss = 0.39650601\n",
      "Iteration 137, loss = 0.39632842\n",
      "Iteration 138, loss = 0.39618630\n",
      "Iteration 139, loss = 0.39607467\n",
      "Iteration 140, loss = 0.39590802\n",
      "Iteration 141, loss = 0.39576972\n",
      "Iteration 142, loss = 0.39563962\n",
      "Iteration 143, loss = 0.39552820\n",
      "Iteration 144, loss = 0.39538837\n",
      "Iteration 145, loss = 0.39527847\n",
      "Iteration 146, loss = 0.39513736\n",
      "Iteration 147, loss = 0.39500512\n",
      "Iteration 148, loss = 0.39494679\n",
      "Iteration 149, loss = 0.39479699\n",
      "Iteration 150, loss = 0.39464379\n",
      "Iteration 151, loss = 0.39454978\n",
      "Iteration 152, loss = 0.39442062\n",
      "Iteration 153, loss = 0.39430720\n",
      "Iteration 154, loss = 0.39419306\n",
      "Iteration 155, loss = 0.39410386\n",
      "Iteration 156, loss = 0.39401995\n",
      "Iteration 157, loss = 0.39386948\n",
      "Iteration 158, loss = 0.39377297\n",
      "Iteration 159, loss = 0.39367799\n",
      "Iteration 160, loss = 0.39357428\n",
      "Iteration 161, loss = 0.39346787\n",
      "Iteration 162, loss = 0.39338984\n",
      "Iteration 163, loss = 0.39329983\n",
      "Iteration 164, loss = 0.39319816\n",
      "Iteration 165, loss = 0.39310170\n",
      "Iteration 166, loss = 0.39300786\n",
      "Iteration 167, loss = 0.39291308\n",
      "Iteration 168, loss = 0.39282606\n",
      "Iteration 169, loss = 0.39274477\n",
      "Iteration 170, loss = 0.39266182\n",
      "Iteration 171, loss = 0.39256056\n",
      "Iteration 172, loss = 0.39248871\n",
      "Iteration 173, loss = 0.39242259\n",
      "Iteration 174, loss = 0.39233004\n",
      "Iteration 175, loss = 0.39226319\n",
      "Iteration 176, loss = 0.39220196\n",
      "Iteration 177, loss = 0.39210147\n",
      "Iteration 178, loss = 0.39203286\n",
      "Iteration 179, loss = 0.39196195\n",
      "Iteration 180, loss = 0.39190970\n",
      "Iteration 181, loss = 0.39187196\n",
      "Iteration 182, loss = 0.39172833\n",
      "Iteration 183, loss = 0.39165790\n",
      "Iteration 184, loss = 0.39158682\n",
      "Iteration 185, loss = 0.39152429\n",
      "Iteration 186, loss = 0.39146731\n",
      "Iteration 187, loss = 0.39140157\n",
      "Iteration 188, loss = 0.39133372\n",
      "Iteration 189, loss = 0.39126538\n",
      "Iteration 190, loss = 0.39121821\n",
      "Iteration 191, loss = 0.39115364\n",
      "Iteration 192, loss = 0.39108148\n",
      "Iteration 193, loss = 0.39102564\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.68141899\n",
      "Iteration 2, loss = 0.65537792\n",
      "Iteration 3, loss = 0.64202212\n",
      "Iteration 4, loss = 0.62960945\n",
      "Iteration 5, loss = 0.61804165\n",
      "Iteration 6, loss = 0.60727575\n",
      "Iteration 7, loss = 0.59722430\n",
      "Iteration 8, loss = 0.58790051\n",
      "Iteration 9, loss = 0.57913998\n",
      "Iteration 10, loss = 0.57095438\n",
      "Iteration 11, loss = 0.56324110\n",
      "Iteration 12, loss = 0.55603865\n",
      "Iteration 13, loss = 0.54929016\n",
      "Iteration 14, loss = 0.54293651\n",
      "Iteration 15, loss = 0.53695108\n",
      "Iteration 16, loss = 0.53131137\n",
      "Iteration 17, loss = 0.52602478\n",
      "Iteration 18, loss = 0.52098861\n",
      "Iteration 19, loss = 0.51627790\n",
      "Iteration 20, loss = 0.51181850\n",
      "Iteration 21, loss = 0.50756766\n",
      "Iteration 22, loss = 0.50361994\n",
      "Iteration 23, loss = 0.49976005\n",
      "Iteration 24, loss = 0.49618870\n",
      "Iteration 25, loss = 0.49268475\n",
      "Iteration 26, loss = 0.48943315\n",
      "Iteration 27, loss = 0.48629896\n",
      "Iteration 28, loss = 0.48335458\n",
      "Iteration 29, loss = 0.48050529\n",
      "Iteration 30, loss = 0.47781597\n",
      "Iteration 31, loss = 0.47527078\n",
      "Iteration 32, loss = 0.47279937\n",
      "Iteration 33, loss = 0.47044763\n",
      "Iteration 34, loss = 0.46820538\n",
      "Iteration 35, loss = 0.46605292\n",
      "Iteration 36, loss = 0.46398667\n",
      "Iteration 37, loss = 0.46207913\n",
      "Iteration 38, loss = 0.46013646\n",
      "Iteration 39, loss = 0.45833888\n",
      "Iteration 40, loss = 0.45658145\n",
      "Iteration 41, loss = 0.45488699\n",
      "Iteration 42, loss = 0.45327073\n",
      "Iteration 43, loss = 0.45170949\n",
      "Iteration 44, loss = 0.45021077\n",
      "Iteration 45, loss = 0.44882629\n",
      "Iteration 46, loss = 0.44738121\n",
      "Iteration 47, loss = 0.44607631\n",
      "Iteration 48, loss = 0.44479248\n",
      "Iteration 49, loss = 0.44355038\n",
      "Iteration 50, loss = 0.44234209\n",
      "Iteration 51, loss = 0.44122230\n",
      "Iteration 52, loss = 0.44013568\n",
      "Iteration 53, loss = 0.43899096\n",
      "Iteration 54, loss = 0.43794244\n",
      "Iteration 55, loss = 0.43694516\n",
      "Iteration 56, loss = 0.43595125\n",
      "Iteration 57, loss = 0.43501159\n",
      "Iteration 58, loss = 0.43413256\n",
      "Iteration 59, loss = 0.43321330\n",
      "Iteration 60, loss = 0.43236328\n",
      "Iteration 61, loss = 0.43157280\n",
      "Iteration 62, loss = 0.43072204\n",
      "Iteration 63, loss = 0.42996186\n",
      "Iteration 64, loss = 0.42919477\n",
      "Iteration 65, loss = 0.42845302\n",
      "Iteration 66, loss = 0.42776148\n",
      "Iteration 67, loss = 0.42706374\n",
      "Iteration 68, loss = 0.42637863\n",
      "Iteration 69, loss = 0.42573738\n",
      "Iteration 70, loss = 0.42513581\n",
      "Iteration 71, loss = 0.42447435\n",
      "Iteration 72, loss = 0.42389743\n",
      "Iteration 73, loss = 0.42330507\n",
      "Iteration 74, loss = 0.42273083\n",
      "Iteration 75, loss = 0.42219151\n",
      "Iteration 76, loss = 0.42165031\n",
      "Iteration 77, loss = 0.42113017\n",
      "Iteration 78, loss = 0.42063949\n",
      "Iteration 79, loss = 0.42014178\n",
      "Iteration 80, loss = 0.41964797\n",
      "Iteration 81, loss = 0.41917497\n",
      "Iteration 82, loss = 0.41877700\n",
      "Iteration 83, loss = 0.41827640\n",
      "Iteration 84, loss = 0.41785854\n",
      "Iteration 85, loss = 0.41745097\n",
      "Iteration 86, loss = 0.41701234\n",
      "Iteration 87, loss = 0.41660787\n",
      "Iteration 88, loss = 0.41621742\n",
      "Iteration 89, loss = 0.41587930\n",
      "Iteration 90, loss = 0.41547331\n",
      "Iteration 91, loss = 0.41509757\n",
      "Iteration 92, loss = 0.41477417\n",
      "Iteration 93, loss = 0.41438582\n",
      "Iteration 94, loss = 0.41405983\n",
      "Iteration 95, loss = 0.41374631\n",
      "Iteration 96, loss = 0.41343404\n",
      "Iteration 97, loss = 0.41308488\n",
      "Iteration 98, loss = 0.41279663\n",
      "Iteration 99, loss = 0.41249017\n",
      "Iteration 100, loss = 0.41217436\n",
      "Iteration 101, loss = 0.41188377\n",
      "Iteration 102, loss = 0.41162659\n",
      "Iteration 103, loss = 0.41132345\n",
      "Iteration 104, loss = 0.41107293\n",
      "Iteration 105, loss = 0.41079431\n",
      "Iteration 106, loss = 0.41055862\n",
      "Iteration 107, loss = 0.41028782\n",
      "Iteration 108, loss = 0.41006800\n",
      "Iteration 109, loss = 0.40979002\n",
      "Iteration 110, loss = 0.40957157\n",
      "Iteration 111, loss = 0.40933085\n",
      "Iteration 112, loss = 0.40910751\n",
      "Iteration 113, loss = 0.40888348\n",
      "Iteration 114, loss = 0.40865985\n",
      "Iteration 115, loss = 0.40843719\n",
      "Iteration 116, loss = 0.40824072\n",
      "Iteration 117, loss = 0.40805422\n",
      "Iteration 118, loss = 0.40782545\n",
      "Iteration 119, loss = 0.40766045\n",
      "Iteration 120, loss = 0.40745634\n",
      "Iteration 121, loss = 0.40727225\n",
      "Iteration 122, loss = 0.40707183\n",
      "Iteration 123, loss = 0.40692628\n",
      "Iteration 124, loss = 0.40675584\n",
      "Iteration 125, loss = 0.40652470\n",
      "Iteration 126, loss = 0.40636559\n",
      "Iteration 127, loss = 0.40620485\n",
      "Iteration 128, loss = 0.40603637\n",
      "Iteration 129, loss = 0.40587451\n",
      "Iteration 130, loss = 0.40573880\n",
      "Iteration 131, loss = 0.40558485\n",
      "Iteration 132, loss = 0.40540655\n",
      "Iteration 133, loss = 0.40530906\n",
      "Iteration 134, loss = 0.40512687\n",
      "Iteration 135, loss = 0.40497159\n",
      "Iteration 136, loss = 0.40486785\n",
      "Iteration 137, loss = 0.40469542\n",
      "Iteration 138, loss = 0.40458837\n",
      "Iteration 139, loss = 0.40442025\n",
      "Iteration 140, loss = 0.40428210\n",
      "Iteration 141, loss = 0.40415669\n",
      "Iteration 142, loss = 0.40404056\n",
      "Iteration 143, loss = 0.40391945\n",
      "Iteration 144, loss = 0.40379621\n",
      "Iteration 145, loss = 0.40369198\n",
      "Iteration 146, loss = 0.40360261\n",
      "Iteration 147, loss = 0.40348551\n",
      "Iteration 148, loss = 0.40330754\n",
      "Iteration 149, loss = 0.40321345\n",
      "Iteration 150, loss = 0.40309030\n",
      "Iteration 151, loss = 0.40304496\n",
      "Iteration 152, loss = 0.40291144\n",
      "Iteration 153, loss = 0.40279232\n",
      "Iteration 154, loss = 0.40271245\n",
      "Iteration 155, loss = 0.40258354\n",
      "Iteration 156, loss = 0.40245046\n",
      "Iteration 157, loss = 0.40241660\n",
      "Iteration 158, loss = 0.40228584\n",
      "Iteration 159, loss = 0.40217786\n",
      "Iteration 160, loss = 0.40209015\n",
      "Iteration 161, loss = 0.40198921\n",
      "Iteration 162, loss = 0.40193369\n",
      "Iteration 163, loss = 0.40184108\n",
      "Iteration 164, loss = 0.40174933\n",
      "Iteration 165, loss = 0.40164994\n",
      "Iteration 166, loss = 0.40155141\n",
      "Iteration 167, loss = 0.40147067\n",
      "Iteration 168, loss = 0.40138875\n",
      "Iteration 169, loss = 0.40130921\n",
      "Iteration 170, loss = 0.40122008\n",
      "Iteration 171, loss = 0.40114122\n",
      "Iteration 172, loss = 0.40107690\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.68841564\n",
      "Iteration 2, loss = 0.68216130\n",
      "Iteration 3, loss = 0.67642741\n",
      "Iteration 4, loss = 0.66764171\n",
      "Iteration 5, loss = 0.65387053\n",
      "Iteration 6, loss = 0.63337154\n",
      "Iteration 7, loss = 0.60517715\n",
      "Iteration 8, loss = 0.57009812\n",
      "Iteration 9, loss = 0.53211884\n",
      "Iteration 10, loss = 0.49643321\n",
      "Iteration 11, loss = 0.46645458\n",
      "Iteration 12, loss = 0.44309323\n",
      "Iteration 13, loss = 0.42582648\n",
      "Iteration 14, loss = 0.41314236\n",
      "Iteration 15, loss = 0.40427883\n",
      "Iteration 16, loss = 0.39788267\n",
      "Iteration 17, loss = 0.39320954\n",
      "Iteration 18, loss = 0.39002061\n",
      "Iteration 19, loss = 0.38758639\n",
      "Iteration 20, loss = 0.38572679\n",
      "Iteration 21, loss = 0.38432201\n",
      "Iteration 22, loss = 0.38318995\n",
      "Iteration 23, loss = 0.38260901\n",
      "Iteration 24, loss = 0.38214105\n",
      "Iteration 25, loss = 0.38140704\n",
      "Iteration 26, loss = 0.38137826\n",
      "Iteration 27, loss = 0.38116249\n",
      "Iteration 28, loss = 0.38074983\n",
      "Iteration 29, loss = 0.38060506\n",
      "Iteration 30, loss = 0.38021371\n",
      "Iteration 31, loss = 0.38017849\n",
      "Iteration 32, loss = 0.38028634\n",
      "Iteration 33, loss = 0.38012322\n",
      "Iteration 34, loss = 0.37987506\n",
      "Iteration 35, loss = 0.37991734\n",
      "Iteration 36, loss = 0.37980816\n",
      "Iteration 37, loss = 0.37966612\n",
      "Iteration 38, loss = 0.37961817\n",
      "Iteration 39, loss = 0.37942700\n",
      "Iteration 40, loss = 0.37955147\n",
      "Iteration 41, loss = 0.37940958\n",
      "Iteration 42, loss = 0.37936356\n",
      "Iteration 43, loss = 0.37919790\n",
      "Iteration 44, loss = 0.37920192\n",
      "Iteration 45, loss = 0.37915217\n",
      "Iteration 46, loss = 0.37906865\n",
      "Iteration 47, loss = 0.37922858\n",
      "Iteration 48, loss = 0.37895734\n",
      "Iteration 49, loss = 0.37901374\n",
      "Iteration 50, loss = 0.37914627\n",
      "Iteration 51, loss = 0.37882621\n",
      "Iteration 52, loss = 0.37920932\n",
      "Iteration 53, loss = 0.37910679\n",
      "Iteration 54, loss = 0.37879544\n",
      "Iteration 55, loss = 0.37856595\n",
      "Iteration 56, loss = 0.37882615\n",
      "Iteration 57, loss = 0.37875886\n",
      "Iteration 58, loss = 0.37869765\n",
      "Iteration 59, loss = 0.37866815\n",
      "Iteration 60, loss = 0.37857029\n",
      "Iteration 61, loss = 0.37847577\n",
      "Iteration 62, loss = 0.37865863\n",
      "Iteration 63, loss = 0.37846125\n",
      "Iteration 64, loss = 0.37856356\n",
      "Iteration 65, loss = 0.37841575\n",
      "Iteration 66, loss = 0.37845668\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.68822958\n",
      "Iteration 2, loss = 0.68241533\n",
      "Iteration 3, loss = 0.67681684\n",
      "Iteration 4, loss = 0.66839671\n",
      "Iteration 5, loss = 0.65518348\n",
      "Iteration 6, loss = 0.63546536\n",
      "Iteration 7, loss = 0.60828081\n",
      "Iteration 8, loss = 0.57439183\n",
      "Iteration 9, loss = 0.53759866\n",
      "Iteration 10, loss = 0.50295747\n",
      "Iteration 11, loss = 0.47359824\n",
      "Iteration 12, loss = 0.45052327\n",
      "Iteration 13, loss = 0.43344702\n",
      "Iteration 14, loss = 0.42096707\n",
      "Iteration 15, loss = 0.41207872\n",
      "Iteration 16, loss = 0.40564408\n",
      "Iteration 17, loss = 0.40100147\n",
      "Iteration 18, loss = 0.39791801\n",
      "Iteration 19, loss = 0.39533509\n",
      "Iteration 20, loss = 0.39377329\n",
      "Iteration 21, loss = 0.39244970\n",
      "Iteration 22, loss = 0.39110773\n",
      "Iteration 23, loss = 0.39059384\n",
      "Iteration 24, loss = 0.39013459\n",
      "Iteration 25, loss = 0.38948080\n",
      "Iteration 26, loss = 0.38949312\n",
      "Iteration 27, loss = 0.38884630\n",
      "Iteration 28, loss = 0.38843872\n",
      "Iteration 29, loss = 0.38849782\n",
      "Iteration 30, loss = 0.38819534\n",
      "Iteration 31, loss = 0.38814165\n",
      "Iteration 32, loss = 0.38803061\n",
      "Iteration 33, loss = 0.38795644\n",
      "Iteration 34, loss = 0.38778975\n",
      "Iteration 35, loss = 0.38782776\n",
      "Iteration 36, loss = 0.38762762\n",
      "Iteration 37, loss = 0.38766239\n",
      "Iteration 38, loss = 0.38747162\n",
      "Iteration 39, loss = 0.38734822\n",
      "Iteration 40, loss = 0.38731361\n",
      "Iteration 41, loss = 0.38737696\n",
      "Iteration 42, loss = 0.38754042\n",
      "Iteration 43, loss = 0.38758268\n",
      "Iteration 44, loss = 0.38698611\n",
      "Iteration 45, loss = 0.38698920\n",
      "Iteration 46, loss = 0.38688694\n",
      "Iteration 47, loss = 0.38716671\n",
      "Iteration 48, loss = 0.38670458\n",
      "Iteration 49, loss = 0.38677806\n",
      "Iteration 50, loss = 0.38686869\n",
      "Iteration 51, loss = 0.38677408\n",
      "Iteration 52, loss = 0.38666142\n",
      "Iteration 53, loss = 0.38648925\n",
      "Iteration 54, loss = 0.38649243\n",
      "Iteration 55, loss = 0.38668190\n",
      "Iteration 56, loss = 0.38637701\n",
      "Iteration 57, loss = 0.38626065\n",
      "Iteration 58, loss = 0.38666596\n",
      "Iteration 59, loss = 0.38630333\n",
      "Iteration 60, loss = 0.38616824\n",
      "Iteration 61, loss = 0.38608873\n",
      "Iteration 62, loss = 0.38625144\n",
      "Iteration 63, loss = 0.38610635\n",
      "Iteration 64, loss = 0.38608457\n",
      "Iteration 65, loss = 0.38613361\n",
      "Iteration 66, loss = 0.38634273\n",
      "Iteration 67, loss = 0.38615062\n",
      "Iteration 68, loss = 0.38635992\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.68810620\n",
      "Iteration 2, loss = 0.68258327\n",
      "Iteration 3, loss = 0.67722208\n",
      "Iteration 4, loss = 0.66908643\n",
      "Iteration 5, loss = 0.65651651\n",
      "Iteration 6, loss = 0.63784860\n",
      "Iteration 7, loss = 0.61169489\n",
      "Iteration 8, loss = 0.57937503\n",
      "Iteration 9, loss = 0.54423145\n",
      "Iteration 10, loss = 0.51035367\n",
      "Iteration 11, loss = 0.48171439\n",
      "Iteration 12, loss = 0.45874383\n",
      "Iteration 13, loss = 0.44184229\n",
      "Iteration 14, loss = 0.42943227\n",
      "Iteration 15, loss = 0.42070313\n",
      "Iteration 16, loss = 0.41436497\n",
      "Iteration 17, loss = 0.40987720\n",
      "Iteration 18, loss = 0.40680035\n",
      "Iteration 19, loss = 0.40412053\n",
      "Iteration 20, loss = 0.40254173\n",
      "Iteration 21, loss = 0.40121536\n",
      "Iteration 22, loss = 0.40050828\n",
      "Iteration 23, loss = 0.39945779\n",
      "Iteration 24, loss = 0.39927996\n",
      "Iteration 25, loss = 0.39878746\n",
      "Iteration 26, loss = 0.39813176\n",
      "Iteration 27, loss = 0.39784088\n",
      "Iteration 28, loss = 0.39729856\n",
      "Iteration 29, loss = 0.39719117\n",
      "Iteration 30, loss = 0.39718050\n",
      "Iteration 31, loss = 0.39685827\n",
      "Iteration 32, loss = 0.39674880\n",
      "Iteration 33, loss = 0.39655400\n",
      "Iteration 34, loss = 0.39632546\n",
      "Iteration 35, loss = 0.39606382\n",
      "Iteration 36, loss = 0.39644994\n",
      "Iteration 37, loss = 0.39599617\n",
      "Iteration 38, loss = 0.39591218\n",
      "Iteration 39, loss = 0.39583251\n",
      "Iteration 40, loss = 0.39581098\n",
      "Iteration 41, loss = 0.39550899\n",
      "Iteration 42, loss = 0.39571545\n",
      "Iteration 43, loss = 0.39553918\n",
      "Iteration 44, loss = 0.39514761\n",
      "Iteration 45, loss = 0.39561974\n",
      "Iteration 46, loss = 0.39535259\n",
      "Iteration 47, loss = 0.39503604\n",
      "Iteration 48, loss = 0.39516770\n",
      "Iteration 49, loss = 0.39501111\n",
      "Iteration 50, loss = 0.39513352\n",
      "Iteration 51, loss = 0.39551336\n",
      "Iteration 52, loss = 0.39469920\n",
      "Iteration 53, loss = 0.39470720\n",
      "Iteration 54, loss = 0.39487901\n",
      "Iteration 55, loss = 0.39460878\n",
      "Iteration 56, loss = 0.39460283\n",
      "Iteration 57, loss = 0.39464723\n",
      "Iteration 58, loss = 0.39431230\n",
      "Iteration 59, loss = 0.39460884\n",
      "Iteration 60, loss = 0.39475830\n",
      "Iteration 61, loss = 0.39467492\n",
      "Iteration 62, loss = 0.39447209\n",
      "Iteration 63, loss = 0.39441660\n",
      "Iteration 64, loss = 0.39423076\n",
      "Iteration 65, loss = 0.39420997\n",
      "Iteration 66, loss = 0.39427916\n",
      "Iteration 67, loss = 0.39439110\n",
      "Iteration 68, loss = 0.39427338\n",
      "Iteration 69, loss = 0.39425746\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.81393224\n",
      "Iteration 2, loss = 0.68291616\n",
      "Iteration 3, loss = 0.66065678\n",
      "Iteration 4, loss = 0.63163222\n",
      "Iteration 5, loss = 0.59652308\n",
      "Iteration 6, loss = 0.55782735\n",
      "Iteration 7, loss = 0.51959095\n",
      "Iteration 8, loss = 0.48542473\n",
      "Iteration 9, loss = 0.45862110\n",
      "Iteration 10, loss = 0.43789738\n",
      "Iteration 11, loss = 0.42260035\n",
      "Iteration 12, loss = 0.41170920\n",
      "Iteration 13, loss = 0.40389746\n",
      "Iteration 14, loss = 0.39804304\n",
      "Iteration 15, loss = 0.39444430\n",
      "Iteration 16, loss = 0.39114615\n",
      "Iteration 17, loss = 0.38941596\n",
      "Iteration 18, loss = 0.38742726\n",
      "Iteration 19, loss = 0.38618193\n",
      "Iteration 20, loss = 0.38555146\n",
      "Iteration 21, loss = 0.38448406\n",
      "Iteration 22, loss = 0.38387496\n",
      "Iteration 23, loss = 0.38367312\n",
      "Iteration 24, loss = 0.38324198\n",
      "Iteration 25, loss = 0.38250135\n",
      "Iteration 26, loss = 0.38236469\n",
      "Iteration 27, loss = 0.38238166\n",
      "Iteration 28, loss = 0.38188777\n",
      "Iteration 29, loss = 0.38169749\n",
      "Iteration 30, loss = 0.38135669\n",
      "Iteration 31, loss = 0.38144738\n",
      "Iteration 32, loss = 0.38093322\n",
      "Iteration 33, loss = 0.38106571\n",
      "Iteration 34, loss = 0.38087641\n",
      "Iteration 35, loss = 0.38072899\n",
      "Iteration 36, loss = 0.38062313\n",
      "Iteration 37, loss = 0.38037984\n",
      "Iteration 38, loss = 0.38101477\n",
      "Iteration 39, loss = 0.38022240\n",
      "Iteration 40, loss = 0.38054442\n",
      "Iteration 41, loss = 0.37978789\n",
      "Iteration 42, loss = 0.37988494\n",
      "Iteration 43, loss = 0.37981821\n",
      "Iteration 44, loss = 0.37984602\n",
      "Iteration 45, loss = 0.37977889\n",
      "Iteration 46, loss = 0.37922309\n",
      "Iteration 47, loss = 0.38017797\n",
      "Iteration 48, loss = 0.37927093\n",
      "Iteration 49, loss = 0.37952452\n",
      "Iteration 50, loss = 0.37916060\n",
      "Iteration 51, loss = 0.37906021\n",
      "Iteration 52, loss = 0.37940113\n",
      "Iteration 53, loss = 0.37901223\n",
      "Iteration 54, loss = 0.37902823\n",
      "Iteration 55, loss = 0.37905996\n",
      "Iteration 56, loss = 0.37914240\n",
      "Iteration 57, loss = 0.37900703\n",
      "Iteration 58, loss = 0.37892781\n",
      "Iteration 59, loss = 0.37837416\n",
      "Iteration 60, loss = 0.37882098\n",
      "Iteration 61, loss = 0.37855172\n",
      "Iteration 62, loss = 0.37848620\n",
      "Iteration 63, loss = 0.37867579\n",
      "Iteration 64, loss = 0.37856904\n",
      "Iteration 65, loss = 0.37866913\n",
      "Iteration 66, loss = 0.37857339\n",
      "Iteration 67, loss = 0.37872711\n",
      "Iteration 68, loss = 0.37849162\n",
      "Iteration 69, loss = 0.37829345\n",
      "Iteration 70, loss = 0.37822949\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.81649602\n",
      "Iteration 2, loss = 0.68264469\n",
      "Iteration 3, loss = 0.66027556\n",
      "Iteration 4, loss = 0.63161007\n",
      "Iteration 5, loss = 0.59711275\n",
      "Iteration 6, loss = 0.55950099\n",
      "Iteration 7, loss = 0.52241174\n",
      "Iteration 8, loss = 0.48992847\n",
      "Iteration 9, loss = 0.46385944\n",
      "Iteration 10, loss = 0.44407866\n",
      "Iteration 11, loss = 0.42937659\n",
      "Iteration 12, loss = 0.41885073\n",
      "Iteration 13, loss = 0.41157617\n",
      "Iteration 14, loss = 0.40601988\n",
      "Iteration 15, loss = 0.40208761\n",
      "Iteration 16, loss = 0.39918586\n",
      "Iteration 17, loss = 0.39704635\n",
      "Iteration 18, loss = 0.39543978\n",
      "Iteration 19, loss = 0.39421725\n",
      "Iteration 20, loss = 0.39338506\n",
      "Iteration 21, loss = 0.39256806\n",
      "Iteration 22, loss = 0.39193179\n",
      "Iteration 23, loss = 0.39169728\n",
      "Iteration 24, loss = 0.39129080\n",
      "Iteration 25, loss = 0.39082295\n",
      "Iteration 26, loss = 0.39038677\n",
      "Iteration 27, loss = 0.39040497\n",
      "Iteration 28, loss = 0.38992258\n",
      "Iteration 29, loss = 0.38952902\n",
      "Iteration 30, loss = 0.38942674\n",
      "Iteration 31, loss = 0.38922540\n",
      "Iteration 32, loss = 0.38935962\n",
      "Iteration 33, loss = 0.38924875\n",
      "Iteration 34, loss = 0.38912216\n",
      "Iteration 35, loss = 0.38877816\n",
      "Iteration 36, loss = 0.38890279\n",
      "Iteration 37, loss = 0.38835675\n",
      "Iteration 38, loss = 0.38827923\n",
      "Iteration 39, loss = 0.38817788\n",
      "Iteration 40, loss = 0.38817379\n",
      "Iteration 41, loss = 0.38806835\n",
      "Iteration 42, loss = 0.38793303\n",
      "Iteration 43, loss = 0.38736577\n",
      "Iteration 44, loss = 0.38770994\n",
      "Iteration 45, loss = 0.38751001\n",
      "Iteration 46, loss = 0.38736192\n",
      "Iteration 47, loss = 0.38729310\n",
      "Iteration 48, loss = 0.38698958\n",
      "Iteration 49, loss = 0.38711714\n",
      "Iteration 50, loss = 0.38696388\n",
      "Iteration 51, loss = 0.38691304\n",
      "Iteration 52, loss = 0.38705076\n",
      "Iteration 53, loss = 0.38659156\n",
      "Iteration 54, loss = 0.38670731\n",
      "Iteration 55, loss = 0.38694954\n",
      "Iteration 56, loss = 0.38662691\n",
      "Iteration 57, loss = 0.38681941\n",
      "Iteration 58, loss = 0.38728734\n",
      "Iteration 59, loss = 0.38660483\n",
      "Iteration 60, loss = 0.38630216\n",
      "Iteration 61, loss = 0.38624379\n",
      "Iteration 62, loss = 0.38676438\n",
      "Iteration 63, loss = 0.38633782\n",
      "Iteration 64, loss = 0.38643028\n",
      "Iteration 65, loss = 0.38625139\n",
      "Iteration 66, loss = 0.38632528\n",
      "Iteration 67, loss = 0.38631218\n",
      "Iteration 68, loss = 0.38593879\n",
      "Iteration 69, loss = 0.38606361\n",
      "Iteration 70, loss = 0.38601607\n",
      "Iteration 71, loss = 0.38605211\n",
      "Iteration 72, loss = 0.38621419\n",
      "Iteration 73, loss = 0.38633775\n",
      "Iteration 74, loss = 0.38590478\n",
      "Iteration 75, loss = 0.38582754\n",
      "Iteration 76, loss = 0.38603619\n",
      "Iteration 77, loss = 0.38591884\n",
      "Iteration 78, loss = 0.38588573\n",
      "Iteration 79, loss = 0.38600060\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.81582413\n",
      "Iteration 2, loss = 0.68267140\n",
      "Iteration 3, loss = 0.66095953\n",
      "Iteration 4, loss = 0.63304303\n",
      "Iteration 5, loss = 0.59951115\n",
      "Iteration 6, loss = 0.56311014\n",
      "Iteration 7, loss = 0.52739456\n",
      "Iteration 8, loss = 0.49604161\n",
      "Iteration 9, loss = 0.47073095\n",
      "Iteration 10, loss = 0.45158513\n",
      "Iteration 11, loss = 0.43759342\n",
      "Iteration 12, loss = 0.42732912\n",
      "Iteration 13, loss = 0.42010757\n",
      "Iteration 14, loss = 0.41481109\n",
      "Iteration 15, loss = 0.41131395\n",
      "Iteration 16, loss = 0.40867990\n",
      "Iteration 17, loss = 0.40643013\n",
      "Iteration 18, loss = 0.40477530\n",
      "Iteration 19, loss = 0.40346064\n",
      "Iteration 20, loss = 0.40283244\n",
      "Iteration 21, loss = 0.40196069\n",
      "Iteration 22, loss = 0.40124950\n",
      "Iteration 23, loss = 0.40077821\n",
      "Iteration 24, loss = 0.40033917\n",
      "Iteration 25, loss = 0.40005706\n",
      "Iteration 26, loss = 0.39955362\n",
      "Iteration 27, loss = 0.39950482\n",
      "Iteration 28, loss = 0.39931297\n",
      "Iteration 29, loss = 0.39883359\n",
      "Iteration 30, loss = 0.39845710\n",
      "Iteration 31, loss = 0.39859817\n",
      "Iteration 32, loss = 0.39850638\n",
      "Iteration 33, loss = 0.39773696\n",
      "Iteration 34, loss = 0.39762968\n",
      "Iteration 35, loss = 0.39740240\n",
      "Iteration 36, loss = 0.39733231\n",
      "Iteration 37, loss = 0.39716912\n",
      "Iteration 38, loss = 0.39728635\n",
      "Iteration 39, loss = 0.39685035\n",
      "Iteration 40, loss = 0.39677782\n",
      "Iteration 41, loss = 0.39649982\n",
      "Iteration 42, loss = 0.39636209\n",
      "Iteration 43, loss = 0.39616274\n",
      "Iteration 44, loss = 0.39612278\n",
      "Iteration 45, loss = 0.39585932\n",
      "Iteration 46, loss = 0.39575288\n",
      "Iteration 47, loss = 0.39572506\n",
      "Iteration 48, loss = 0.39551609\n",
      "Iteration 49, loss = 0.39577244\n",
      "Iteration 50, loss = 0.39593242\n",
      "Iteration 51, loss = 0.39551798\n",
      "Iteration 52, loss = 0.39527664\n",
      "Iteration 53, loss = 0.39502816\n",
      "Iteration 54, loss = 0.39514946\n",
      "Iteration 55, loss = 0.39534862\n",
      "Iteration 56, loss = 0.39495873\n",
      "Iteration 57, loss = 0.39507041\n",
      "Iteration 58, loss = 0.39505044\n",
      "Iteration 59, loss = 0.39486393\n",
      "Iteration 60, loss = 0.39462154\n",
      "Iteration 61, loss = 0.39482801\n",
      "Iteration 62, loss = 0.39482413\n",
      "Iteration 63, loss = 0.39472323\n",
      "Iteration 64, loss = 0.39475067\n",
      "Iteration 65, loss = 0.39440696\n",
      "Iteration 66, loss = 0.39461028\n",
      "Iteration 67, loss = 0.39463540\n",
      "Iteration 68, loss = 0.39425744\n",
      "Iteration 69, loss = 0.39447634\n",
      "Iteration 70, loss = 0.39428495\n",
      "Iteration 71, loss = 0.39406607\n",
      "Iteration 72, loss = 0.39421536\n",
      "Iteration 73, loss = 0.39410261\n",
      "Iteration 74, loss = 0.39433852\n",
      "Iteration 75, loss = 0.39430590\n",
      "Iteration 76, loss = 0.39398814\n",
      "Iteration 77, loss = 0.39420573\n",
      "Iteration 78, loss = 0.39441893\n",
      "Iteration 79, loss = 0.39407867\n",
      "Iteration 80, loss = 0.39380459\n",
      "Iteration 81, loss = 0.39465005\n",
      "Iteration 82, loss = 0.39378818\n",
      "Iteration 83, loss = 0.39375444\n",
      "Iteration 84, loss = 0.39423803\n",
      "Iteration 85, loss = 0.39385448\n",
      "Iteration 86, loss = 0.39389482\n",
      "Iteration 87, loss = 0.39363653\n",
      "Iteration 88, loss = 0.39371411\n",
      "Iteration 89, loss = 0.39386307\n",
      "Iteration 90, loss = 0.39375266\n",
      "Iteration 91, loss = 0.39383726\n",
      "Iteration 92, loss = 0.39382651\n",
      "Iteration 93, loss = 0.39370821\n",
      "Iteration 94, loss = 0.39366128\n",
      "Iteration 95, loss = 0.39370421\n",
      "Iteration 96, loss = 0.39383419\n",
      "Iteration 97, loss = 0.39358374\n",
      "Iteration 98, loss = 0.39350392\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.71059927\n",
      "Iteration 2, loss = 0.69312922\n",
      "Iteration 3, loss = 0.69065335\n",
      "Iteration 4, loss = 0.68855951\n",
      "Iteration 5, loss = 0.68591995\n",
      "Iteration 6, loss = 0.68233369\n",
      "Iteration 7, loss = 0.67720069\n",
      "Iteration 8, loss = 0.66914719\n",
      "Iteration 9, loss = 0.65729459\n",
      "Iteration 10, loss = 0.63959615\n",
      "Iteration 11, loss = 0.61439404\n",
      "Iteration 12, loss = 0.58142893\n",
      "Iteration 13, loss = 0.54353887\n",
      "Iteration 14, loss = 0.50567663\n",
      "Iteration 15, loss = 0.47245497\n",
      "Iteration 16, loss = 0.44668385\n",
      "Iteration 17, loss = 0.42752590\n",
      "Iteration 18, loss = 0.41417387\n",
      "Iteration 19, loss = 0.40429632\n",
      "Iteration 20, loss = 0.39834051\n",
      "Iteration 21, loss = 0.39322229\n",
      "Iteration 22, loss = 0.38964398\n",
      "Iteration 23, loss = 0.38760981\n",
      "Iteration 24, loss = 0.38587809\n",
      "Iteration 25, loss = 0.38465555\n",
      "Iteration 26, loss = 0.38336966\n",
      "Iteration 27, loss = 0.38284781\n",
      "Iteration 28, loss = 0.38206853\n",
      "Iteration 29, loss = 0.38193262\n",
      "Iteration 30, loss = 0.38168021\n",
      "Iteration 31, loss = 0.38101994\n",
      "Iteration 32, loss = 0.38091637\n",
      "Iteration 33, loss = 0.38055920\n",
      "Iteration 34, loss = 0.38040478\n",
      "Iteration 35, loss = 0.38044035\n",
      "Iteration 36, loss = 0.38004675\n",
      "Iteration 37, loss = 0.38007214\n",
      "Iteration 38, loss = 0.37968736\n",
      "Iteration 39, loss = 0.37947521\n",
      "Iteration 40, loss = 0.37937554\n",
      "Iteration 41, loss = 0.37965538\n",
      "Iteration 42, loss = 0.37914631\n",
      "Iteration 43, loss = 0.37904329\n",
      "Iteration 44, loss = 0.37869189\n",
      "Iteration 45, loss = 0.37885215\n",
      "Iteration 46, loss = 0.37876264\n",
      "Iteration 47, loss = 0.37893154\n",
      "Iteration 48, loss = 0.37865803\n",
      "Iteration 49, loss = 0.37880214\n",
      "Iteration 50, loss = 0.37856112\n",
      "Iteration 51, loss = 0.37834515\n",
      "Iteration 52, loss = 0.37800620\n",
      "Iteration 53, loss = 0.37832742\n",
      "Iteration 54, loss = 0.37815315\n",
      "Iteration 55, loss = 0.37799507\n",
      "Iteration 56, loss = 0.37799096\n",
      "Iteration 57, loss = 0.37783194\n",
      "Iteration 58, loss = 0.37778377\n",
      "Iteration 59, loss = 0.37812439\n",
      "Iteration 60, loss = 0.37783901\n",
      "Iteration 61, loss = 0.37769778\n",
      "Iteration 62, loss = 0.37763487\n",
      "Iteration 63, loss = 0.37741501\n",
      "Iteration 64, loss = 0.37756021\n",
      "Iteration 65, loss = 0.37763373\n",
      "Iteration 66, loss = 0.37755565\n",
      "Iteration 67, loss = 0.37727596\n",
      "Iteration 68, loss = 0.37766064\n",
      "Iteration 69, loss = 0.37750760\n",
      "Iteration 70, loss = 0.37742191\n",
      "Iteration 71, loss = 0.37779267\n",
      "Iteration 72, loss = 0.37704750\n",
      "Iteration 73, loss = 0.37727863\n",
      "Iteration 74, loss = 0.37734444\n",
      "Iteration 75, loss = 0.37730427\n",
      "Iteration 76, loss = 0.37704872\n",
      "Iteration 77, loss = 0.37705676\n",
      "Iteration 78, loss = 0.37716962\n",
      "Iteration 79, loss = 0.37709253\n",
      "Iteration 80, loss = 0.37731025\n",
      "Iteration 81, loss = 0.37713370\n",
      "Iteration 82, loss = 0.37705699\n",
      "Iteration 83, loss = 0.37702296\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.71200736\n",
      "Iteration 2, loss = 0.69286609\n",
      "Iteration 3, loss = 0.69048138\n",
      "Iteration 4, loss = 0.68831843\n",
      "Iteration 5, loss = 0.68577437\n",
      "Iteration 6, loss = 0.68216567\n",
      "Iteration 7, loss = 0.67692443\n",
      "Iteration 8, loss = 0.66882531\n",
      "Iteration 9, loss = 0.65698027\n",
      "Iteration 10, loss = 0.63924108\n",
      "Iteration 11, loss = 0.61428842\n",
      "Iteration 12, loss = 0.58168435\n",
      "Iteration 13, loss = 0.54472046\n",
      "Iteration 14, loss = 0.50859229\n",
      "Iteration 15, loss = 0.47679199\n",
      "Iteration 16, loss = 0.45197500\n",
      "Iteration 17, loss = 0.43376095\n",
      "Iteration 18, loss = 0.42099276\n",
      "Iteration 19, loss = 0.41210066\n",
      "Iteration 20, loss = 0.40578736\n",
      "Iteration 21, loss = 0.40139111\n",
      "Iteration 22, loss = 0.39797150\n",
      "Iteration 23, loss = 0.39571652\n",
      "Iteration 24, loss = 0.39429424\n",
      "Iteration 25, loss = 0.39293631\n",
      "Iteration 26, loss = 0.39231274\n",
      "Iteration 27, loss = 0.39085878\n",
      "Iteration 28, loss = 0.39052684\n",
      "Iteration 29, loss = 0.39040174\n",
      "Iteration 30, loss = 0.39029322\n",
      "Iteration 31, loss = 0.38953056\n",
      "Iteration 32, loss = 0.38918903\n",
      "Iteration 33, loss = 0.38915062\n",
      "Iteration 34, loss = 0.38868770\n",
      "Iteration 35, loss = 0.38833145\n",
      "Iteration 36, loss = 0.38835941\n",
      "Iteration 37, loss = 0.38825860\n",
      "Iteration 38, loss = 0.38805755\n",
      "Iteration 39, loss = 0.38824875\n",
      "Iteration 40, loss = 0.38761017\n",
      "Iteration 41, loss = 0.38804912\n",
      "Iteration 42, loss = 0.38750554\n",
      "Iteration 43, loss = 0.38724212\n",
      "Iteration 44, loss = 0.38713266\n",
      "Iteration 45, loss = 0.38709400\n",
      "Iteration 46, loss = 0.38682293\n",
      "Iteration 47, loss = 0.38707927\n",
      "Iteration 48, loss = 0.38679388\n",
      "Iteration 49, loss = 0.38664057\n",
      "Iteration 50, loss = 0.38656350\n",
      "Iteration 51, loss = 0.38629273\n",
      "Iteration 52, loss = 0.38653107\n",
      "Iteration 53, loss = 0.38617088\n",
      "Iteration 54, loss = 0.38639918\n",
      "Iteration 55, loss = 0.38595072\n",
      "Iteration 56, loss = 0.38598027\n",
      "Iteration 57, loss = 0.38611021\n",
      "Iteration 58, loss = 0.38583159\n",
      "Iteration 59, loss = 0.38584354\n",
      "Iteration 60, loss = 0.38613606\n",
      "Iteration 61, loss = 0.38566098\n",
      "Iteration 62, loss = 0.38569066\n",
      "Iteration 63, loss = 0.38544247\n",
      "Iteration 64, loss = 0.38563526\n",
      "Iteration 65, loss = 0.38560278\n",
      "Iteration 66, loss = 0.38586551\n",
      "Iteration 67, loss = 0.38547217\n",
      "Iteration 68, loss = 0.38540453\n",
      "Iteration 69, loss = 0.38553644\n",
      "Iteration 70, loss = 0.38537603\n",
      "Iteration 71, loss = 0.38559429\n",
      "Iteration 72, loss = 0.38505839\n",
      "Iteration 73, loss = 0.38518483\n",
      "Iteration 74, loss = 0.38526331\n",
      "Iteration 75, loss = 0.38497383\n",
      "Iteration 76, loss = 0.38584805\n",
      "Iteration 77, loss = 0.38579479\n",
      "Iteration 78, loss = 0.38540738\n",
      "Iteration 79, loss = 0.38513448\n",
      "Iteration 80, loss = 0.38536752\n",
      "Iteration 81, loss = 0.38498075\n",
      "Iteration 82, loss = 0.38497893\n",
      "Iteration 83, loss = 0.38502927\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.71305555\n",
      "Iteration 2, loss = 0.69282810\n",
      "Iteration 3, loss = 0.69052575\n",
      "Iteration 4, loss = 0.68840609\n",
      "Iteration 5, loss = 0.68586973\n",
      "Iteration 6, loss = 0.68233648\n",
      "Iteration 7, loss = 0.67719193\n",
      "Iteration 8, loss = 0.66949309\n",
      "Iteration 9, loss = 0.65797618\n",
      "Iteration 10, loss = 0.64102059\n",
      "Iteration 11, loss = 0.61678688\n",
      "Iteration 12, loss = 0.58582724\n",
      "Iteration 13, loss = 0.55048828\n",
      "Iteration 14, loss = 0.51513775\n",
      "Iteration 15, loss = 0.48437130\n",
      "Iteration 16, loss = 0.46003529\n",
      "Iteration 17, loss = 0.44237796\n",
      "Iteration 18, loss = 0.42953529\n",
      "Iteration 19, loss = 0.42113099\n",
      "Iteration 20, loss = 0.41458807\n",
      "Iteration 21, loss = 0.41023371\n",
      "Iteration 22, loss = 0.40719833\n",
      "Iteration 23, loss = 0.40518465\n",
      "Iteration 24, loss = 0.40348554\n",
      "Iteration 25, loss = 0.40243715\n",
      "Iteration 26, loss = 0.40106143\n",
      "Iteration 27, loss = 0.40071957\n",
      "Iteration 28, loss = 0.40002453\n",
      "Iteration 29, loss = 0.39942549\n",
      "Iteration 30, loss = 0.39895564\n",
      "Iteration 31, loss = 0.39866561\n",
      "Iteration 32, loss = 0.39848507\n",
      "Iteration 33, loss = 0.39798868\n",
      "Iteration 34, loss = 0.39778404\n",
      "Iteration 35, loss = 0.39792257\n",
      "Iteration 36, loss = 0.39684025\n",
      "Iteration 37, loss = 0.39784533\n",
      "Iteration 38, loss = 0.39668975\n",
      "Iteration 39, loss = 0.39667020\n",
      "Iteration 40, loss = 0.39652793\n",
      "Iteration 41, loss = 0.39643909\n",
      "Iteration 42, loss = 0.39608436\n",
      "Iteration 43, loss = 0.39612645\n",
      "Iteration 44, loss = 0.39604393\n",
      "Iteration 45, loss = 0.39612932\n",
      "Iteration 46, loss = 0.39534712\n",
      "Iteration 47, loss = 0.39587842\n",
      "Iteration 48, loss = 0.39522619\n",
      "Iteration 49, loss = 0.39511846\n",
      "Iteration 50, loss = 0.39486302\n",
      "Iteration 51, loss = 0.39504011\n",
      "Iteration 52, loss = 0.39512432\n",
      "Iteration 53, loss = 0.39473463\n",
      "Iteration 54, loss = 0.39504831\n",
      "Iteration 55, loss = 0.39488961\n",
      "Iteration 56, loss = 0.39434049\n",
      "Iteration 57, loss = 0.39432018\n",
      "Iteration 58, loss = 0.39422635\n",
      "Iteration 59, loss = 0.39425291\n",
      "Iteration 60, loss = 0.39424822\n",
      "Iteration 61, loss = 0.39409739\n",
      "Iteration 62, loss = 0.39384881\n",
      "Iteration 63, loss = 0.39402588\n",
      "Iteration 64, loss = 0.39375292\n",
      "Iteration 65, loss = 0.39362212\n",
      "Iteration 66, loss = 0.39387868\n",
      "Iteration 67, loss = 0.39394351\n",
      "Iteration 68, loss = 0.39379590\n",
      "Iteration 69, loss = 0.39370185\n",
      "Iteration 70, loss = 0.39350129\n",
      "Iteration 71, loss = 0.39376456\n",
      "Iteration 72, loss = 0.39366508\n",
      "Iteration 73, loss = 0.39375308\n",
      "Iteration 74, loss = 0.39374491\n",
      "Iteration 75, loss = 0.39328332\n",
      "Iteration 76, loss = 0.39331239\n",
      "Iteration 77, loss = 0.39343506\n",
      "Iteration 78, loss = 0.39330104\n",
      "Iteration 79, loss = 0.39319956\n",
      "Iteration 80, loss = 0.39325884\n",
      "Iteration 81, loss = 0.39346433\n",
      "Iteration 82, loss = 0.39326144\n",
      "Iteration 83, loss = 0.39311608\n",
      "Iteration 84, loss = 0.39341843\n",
      "Iteration 85, loss = 0.39304233\n",
      "Iteration 86, loss = 0.39305374\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.77441521\n",
      "Iteration 2, loss = 0.66670084\n",
      "Iteration 3, loss = 0.62454795\n",
      "Iteration 4, loss = 0.57113698\n",
      "Iteration 5, loss = 0.51651598\n",
      "Iteration 6, loss = 0.47215152\n",
      "Iteration 7, loss = 0.44170458\n",
      "Iteration 8, loss = 0.42280509\n",
      "Iteration 9, loss = 0.41104931\n",
      "Iteration 10, loss = 0.40389131\n",
      "Iteration 11, loss = 0.39950219\n",
      "Iteration 12, loss = 0.39662781\n",
      "Iteration 13, loss = 0.39458994\n",
      "Iteration 14, loss = 0.39341859\n",
      "Iteration 15, loss = 0.39242633\n",
      "Iteration 16, loss = 0.39198351\n",
      "Iteration 17, loss = 0.39139433\n",
      "Iteration 18, loss = 0.39056923\n",
      "Iteration 19, loss = 0.39047608\n",
      "Iteration 20, loss = 0.39030153\n",
      "Iteration 21, loss = 0.38979570\n",
      "Iteration 22, loss = 0.38932167\n",
      "Iteration 23, loss = 0.38908437\n",
      "Iteration 24, loss = 0.38897692\n",
      "Iteration 25, loss = 0.38865575\n",
      "Iteration 26, loss = 0.38843370\n",
      "Iteration 27, loss = 0.38867706\n",
      "Iteration 28, loss = 0.38846058\n",
      "Iteration 29, loss = 0.38811941\n",
      "Iteration 30, loss = 0.38786343\n",
      "Iteration 31, loss = 0.38774650\n",
      "Iteration 32, loss = 0.38761154\n",
      "Iteration 33, loss = 0.38737708\n",
      "Iteration 34, loss = 0.38733313\n",
      "Iteration 35, loss = 0.38710430\n",
      "Iteration 36, loss = 0.38732042\n",
      "Iteration 37, loss = 0.38682934\n",
      "Iteration 38, loss = 0.38685700\n",
      "Iteration 39, loss = 0.38663364\n",
      "Iteration 40, loss = 0.38676230\n",
      "Iteration 41, loss = 0.38691735\n",
      "Iteration 42, loss = 0.38667825\n",
      "Iteration 43, loss = 0.38650354\n",
      "Iteration 44, loss = 0.38632862\n",
      "Iteration 45, loss = 0.38662285\n",
      "Iteration 46, loss = 0.38629037\n",
      "Iteration 47, loss = 0.38622683\n",
      "Iteration 48, loss = 0.38610439\n",
      "Iteration 49, loss = 0.38619829\n",
      "Iteration 50, loss = 0.38625402\n",
      "Iteration 51, loss = 0.38605427\n",
      "Iteration 52, loss = 0.38593832\n",
      "Iteration 53, loss = 0.38607939\n",
      "Iteration 54, loss = 0.38606428\n",
      "Iteration 55, loss = 0.38596636\n",
      "Iteration 56, loss = 0.38600886\n",
      "Iteration 57, loss = 0.38591173\n",
      "Iteration 58, loss = 0.38575509\n",
      "Iteration 59, loss = 0.38598944\n",
      "Iteration 60, loss = 0.38612368\n",
      "Iteration 61, loss = 0.38612336\n",
      "Iteration 62, loss = 0.38588730\n",
      "Iteration 63, loss = 0.38595664\n",
      "Iteration 64, loss = 0.38586854\n",
      "Iteration 65, loss = 0.38573331\n",
      "Iteration 66, loss = 0.38585848\n",
      "Iteration 67, loss = 0.38602176\n",
      "Iteration 68, loss = 0.38586097\n",
      "Iteration 69, loss = 0.38575208\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "{'mean_fit_time': array([2.14508303, 1.75204547, 1.74183075, 1.70529103, 1.44952099,\n",
      "       0.77298029, 0.9602387 , 1.03761435]), 'std_fit_time': array([0.17205089, 0.15166141, 0.27674349, 0.12553727, 0.07495922,\n",
      "       0.05940652, 0.15237008, 0.06720138]), 'mean_score_time': array([0.00137846, 0.00119932, 0.00120687, 0.001278  , 0.0012819 ,\n",
      "       0.00117318, 0.00120234, 0.00126505]), 'std_score_time': array([2.44871576e-04, 3.45283739e-05, 1.64076333e-05, 8.38077386e-05,\n",
      "       2.48923333e-04, 4.09642314e-05, 6.12817690e-06, 9.09366092e-05]), 'param_activation': masked_array(data=['logistic', 'logistic', 'logistic', 'logistic', 'tanh',\n",
      "                   'tanh', 'tanh', 'tanh'],\n",
      "             mask=[False, False, False, False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_hidden_layer_sizes': masked_array(data=[(), (1,), (2,), (3,), (), (1,), (2,), (3,)],\n",
      "             mask=[False, False, False, False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'params': [{'activation': 'logistic', 'hidden_layer_sizes': ()}, {'activation': 'logistic', 'hidden_layer_sizes': (1,)}, {'activation': 'logistic', 'hidden_layer_sizes': (2,)}, {'activation': 'logistic', 'hidden_layer_sizes': (3,)}, {'activation': 'tanh', 'hidden_layer_sizes': ()}, {'activation': 'tanh', 'hidden_layer_sizes': (1,)}, {'activation': 'tanh', 'hidden_layer_sizes': (2,)}, {'activation': 'tanh', 'hidden_layer_sizes': (3,)}], 'split0_test_score': array([0.81193801, 0.81365984, 0.8130859 , 0.8130859 , 0.81136407,\n",
      "       0.81729482, 0.81729482, 0.81557299]), 'split1_test_score': array([0.82399082, 0.82552133, 0.82456476, 0.82552133, 0.82399082,\n",
      "       0.82590396, 0.82628659, 0.82494739]), 'split2_test_score': array([0.83716035, 0.83562954, 0.83524684, 0.83524684, 0.836969  ,\n",
      "       0.8358209 , 0.83677765, 0.83601225]), 'mean_test_score': array([0.82436306, 0.82493691, 0.82429917, 0.82461802, 0.82410796,\n",
      "       0.82633989, 0.82678635, 0.82551087]), 'std_test_score': array([0.01030034, 0.00897861, 0.00904912, 0.00906969, 0.0104535 ,\n",
      "       0.00756952, 0.00796168, 0.0083538 ]), 'rank_test_score': array([6, 4, 7, 5, 8, 2, 1, 3], dtype=int32)}\n",
      "{'activation': 'tanh', 'hidden_layer_sizes': (2,)}\n",
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n",
      "{'mean_fit_time': array([0.19079932, 0.38548708, 0.5692265 , 0.77628255, 0.25755032,\n",
      "       0.52742132, 0.78554527, 1.12231437, 0.33777102, 0.65304915,\n",
      "       1.02157537, 1.37915508, 0.35232759, 0.71423324, 1.09748602,\n",
      "       1.45731497]), 'std_fit_time': array([0.00655251, 0.021378  , 0.02618366, 0.00253817, 0.01109864,\n",
      "       0.01731369, 0.0137707 , 0.04315873, 0.00367791, 0.02768327,\n",
      "       0.01578826, 0.04317512, 0.00196943, 0.00522101, 0.02321903,\n",
      "       0.02429477]), 'mean_score_time': array([0.02208304, 0.041471  , 0.05984783, 0.08462469, 0.02703571,\n",
      "       0.05758484, 0.07970246, 0.10610835, 0.03558334, 0.07208538,\n",
      "       0.11668777, 0.14765525, 0.03730973, 0.0759004 , 0.11916534,\n",
      "       0.15105073]), 'std_score_time': array([0.00049115, 0.00261631, 0.00374313, 0.00091532, 0.00068627,\n",
      "       0.00283928, 0.00217886, 0.0024679 , 0.00241936, 0.00050736,\n",
      "       0.01363431, 0.0038007 , 0.00223325, 0.00197474, 0.0013016 ,\n",
      "       0.00748098]), 'param_max_depth': masked_array(data=[5, 5, 5, 5, 10, 10, 10, 10, 15, 15, 15, 15, 20, 20, 20,\n",
      "                   20],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_n_estimators': masked_array(data=[50, 100, 150, 200, 50, 100, 150, 200, 50, 100, 150,\n",
      "                   200, 50, 100, 150, 200],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'params': [{'max_depth': 5, 'n_estimators': 50}, {'max_depth': 5, 'n_estimators': 100}, {'max_depth': 5, 'n_estimators': 150}, {'max_depth': 5, 'n_estimators': 200}, {'max_depth': 10, 'n_estimators': 50}, {'max_depth': 10, 'n_estimators': 100}, {'max_depth': 10, 'n_estimators': 150}, {'max_depth': 10, 'n_estimators': 200}, {'max_depth': 15, 'n_estimators': 50}, {'max_depth': 15, 'n_estimators': 100}, {'max_depth': 15, 'n_estimators': 150}, {'max_depth': 15, 'n_estimators': 200}, {'max_depth': 20, 'n_estimators': 50}, {'max_depth': 20, 'n_estimators': 100}, {'max_depth': 20, 'n_estimators': 150}, {'max_depth': 20, 'n_estimators': 200}], 'split0_test_score': array([0.81499904, 0.8157643 , 0.8157643 , 0.81499904, 0.80945093,\n",
      "       0.81002487, 0.81002487, 0.8090683 , 0.77501435, 0.77597092,\n",
      "       0.77654486, 0.77616223, 0.75683949, 0.75358714, 0.75205663,\n",
      "       0.75263057]), 'split1_test_score': array([0.82399082, 0.82494739, 0.82494739, 0.82475607, 0.8171035 ,\n",
      "       0.81825139, 0.81806007, 0.81748613, 0.78955424, 0.79204132,\n",
      "       0.79185001, 0.7916587 , 0.77004018, 0.76927492, 0.76812703,\n",
      "       0.76889229]), 'split2_test_score': array([0.83524684, 0.83601225, 0.83601225, 0.83639495, 0.82893226,\n",
      "       0.82854956, 0.82893226, 0.82931496, 0.79850746, 0.79831611,\n",
      "       0.80156908, 0.80099502, 0.77669346, 0.77554535, 0.7765021 ,\n",
      "       0.7745886 ]), 'mean_test_score': array([0.82474557, 0.82557465, 0.82557465, 0.82538336, 0.81849556,\n",
      "       0.81894194, 0.81900574, 0.81862313, 0.78769202, 0.78877612,\n",
      "       0.78998798, 0.78960532, 0.76785771, 0.7661358 , 0.76556192,\n",
      "       0.76537049]), 'std_test_score': array([0.00828334, 0.00827808, 0.00827808, 0.0087461 , 0.0080139 ,\n",
      "       0.00757842, 0.00774782, 0.00830467, 0.009681  , 0.00941003,\n",
      "       0.01030059, 0.01024139, 0.00825096, 0.00923512, 0.01014331,\n",
      "       0.0093038 ]), 'rank_test_score': array([ 4,  1,  1,  3,  8,  6,  5,  7, 12, 11,  9, 10, 13, 14, 15, 16],\n",
      "      dtype=int32)}\n",
      "{'max_depth': 5, 'n_estimators': 100}\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "dm_clf = train_dummy_classifier(X_trn, y_trn)\n",
    "logistic_clf = train_logistic(X_trn, y_trn)\n",
    "boost_clf = train_adaboost(X_trn, y_trn)\n",
    "svm_clf = train_svm(X_trn, y_trn)\n",
    "nn_clf = train_neural_network(X_trn, y_trn)\n",
    "knn_clf = train_kNN_estimators(X_trn, y_trn)\n",
    "rf_clf = train_random_forests(X_trn, y_trn)\n",
    "\n",
    "# y_pred = dm_clf.predict(X_tst)\n",
    "# stats = sklearn.metrics.precision_recall_fscore_support(y_tst, y_pred, average='binary')\n",
    "# sklearn.metrics.accuracy_score(y_tst, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plot_data(X_trn[:300], y_trn[:300])\n",
    "plot_rf_prediction(nn_clf)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "PyCharm (comp432)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
