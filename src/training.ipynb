{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import sklearn.model_selection\n",
    "import sklearn.dummy\n",
    "import sklearn.metrics\n",
    "import sklearn.ensemble\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetch the data frames from the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read both the matches and the frames\n",
    "training_df = pd.read_csv('../data/processed/diff_train.csv').drop(labels=['tier','Unnamed: 0'],axis=1)\n",
    "test_df = pd.read_csv('../data/processed/diff_test.csv').drop(labels=['tier','Unnamed: 0'],axis=1)\n",
    "# Reinterpret all values as int32s\n",
    "training_df = training_df.astype({\n",
    "    'winner': 'int32',\n",
    "    'first_kill': 'int32',\n",
    "    'first_tower': 'int32',\n",
    "    'first_inhibitor': 'int32',\n",
    "    'first_baron': 'int32',\n",
    "    'first_dragon': 'int32',\n",
    "    'first_rift_herald': 'int32',\n",
    "})\n",
    "test_df = test_df.astype({\n",
    "    'winner': 'int32',\n",
    "    'first_kill': 'int32',\n",
    "    'first_tower': 'int32',\n",
    "    'first_inhibitor': 'int32',\n",
    "    'first_baron': 'int32',\n",
    "    'first_dragon': 'int32',\n",
    "    'first_rift_herald': 'int32',\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions to format the data prior to training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def scale_data(X_trn, X_tst):\n",
    "    # Scale the data with MinMax to avoid negative values\n",
    "    scaler = sklearn.preprocessing.MinMaxScaler()\n",
    "    scaler.fit(X_trn)\n",
    "    X_trn = scaler.transform(X_trn)\n",
    "    X_tst = scaler.transform(X_tst)\n",
    "\n",
    "    return X_trn, X_tst\n",
    "\n",
    "def score_estimators(X, y, estimators):\n",
    "    \"\"\"Scores each estimator on (X, y), returning a list of scores.\"\"\"\n",
    "    # Your implementation here. Aim for 1-4 lines.\n",
    "    scores = [0 for _ in range(len(estimators))]\n",
    "    for x in range(len(estimators)):\n",
    "        scores[x] = sklearn.metrics.accuracy_score(y, estimators[x].predict(X))\n",
    "        print(sklearn.metrics.precision_recall_fscore_support(y, estimators[x].predict(X), average='binary'))\n",
    "    return scores\n",
    "\n",
    "def plot_estimator_scores(estimators, param_name, param_vals):\n",
    "    \"\"\"\n",
    "    Plots the training, validation, and testing scores of a list of estimators,\n",
    "    where `param_name` and `param_vals` are the same as for `train_estimators`.\n",
    "    The estimator with best validation score will be highlighted with an 'x'.\n",
    "    \"\"\"\n",
    "    # Your implementation here. Use as many lines as you need\n",
    "    plt.figure()\n",
    "    X = np.arange(0, len(param_vals))\n",
    "    trn_scores = score_estimators(X_trn, y_trn, estimators)\n",
    "    tst_scores = score_estimators(X_tst, y_tst, estimators)\n",
    "    index = np.argmin(trn_scores - tst_scores)\n",
    "    print(tst_scores[index])\n",
    "    plt.title(estimators[0].__class__.__name__ + \" score vs \" + param_name)\n",
    "    plt.xlabel(param_name)\n",
    "    plt.ylabel(\"score\")\n",
    "    plt.ylim(0.0, 1.05)\n",
    "    plt.scatter(X[index], tst_scores[index], marker='x', color='black', s=200)\n",
    "    plt.plot(X, trn_scores, marker='o', color='green', markerfacecolor='green', label=\"train\")\n",
    "    plt.plot(X, tst_scores, marker='o', color='red', markerfacecolor='red', label=\"test\")\n",
    "    plt.text(0, 0.4,\n",
    "             \"Optimal Test Accuracy = %.2f%% with %s = %d \" % (tst_scores[index] * 100, param_name, param_vals[index]))\n",
    "    plt.legend()\n",
    "    plt.xticks(X, param_vals)\n",
    "    \n",
    "\n",
    "def train_estimators(X, y, estimator_type, param_name, param_vals, **kwargs):\n",
    "    estimators = [0 for i in range(len(param_vals))]\n",
    "    for x in range(len(param_vals)):\n",
    "        estimators[x] = estimator_type(**{param_name: param_vals[x]}, **kwargs).fit(X, y)\n",
    "        # Training DecisionTreeClassifier(max_depth=1, random_state=0, splitter='random')...\n",
    "        print(\"Training \" + estimator_type.__name__ + \"(\" + param_name + \"=%d \" % param_vals[x] + \" {0}={1} \".format(\n",
    "            kwargs.keys(), kwargs.values()))\n",
    "    return estimators\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature selection: Select an X and y we want to train on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trn = training_df[['kill_advantage','gold_advantage']].values\n",
    "y_trn = training_df[['winner']].values.T[0]\n",
    "X_tst = test_df[['kill_advantage','gold_advantage']].values\n",
    "y_tst = test_df[['winner']].values.T[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dummy Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Here we train a dummy classifier to compare performance\n",
    "def train_dummy_classifier(X, y):\n",
    "    dummy_clf = sklearn.dummy.DummyClassifier(strategy='uniform', random_state=0)\n",
    "    # Scale the data with MinMax to avoid negative values\n",
    "    dummy_clf.fit(X, y)\n",
    "    return dummy_clf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train_kNN_estimators(X_trn,X_tst,y_trn,y_tst):\n",
    "    param_vals = [1, 5, 10, 15, 20]\n",
    "    # for x in range(5):\n",
    "    #     estimators[x] = sklearn.neighbors.KNeighborsClassifier(n_neighbors=param_vals[x]).fit(X_trn, y_trn)\n",
    "    # trn_scores = score_estimators(X_trn, y_trn, estimators)\n",
    "    # tst_scores = score_estimators(X_tst, y_tst, estimators)\n",
    "    # X_axis = np.arange(0, 5)\n",
    "    # plt.title(estimators[0].__class__.__name__ + \" score vs \" + \"n_neighbors\")\n",
    "    # plt.xlabel(\"n_neighbors\")\n",
    "    # plt.ylabel(\"score\")\n",
    "    # plt.ylim(0.0, 1.05)\n",
    "    # plt.plot(X, trn_scores, marker='o', color='green', markerfacecolor='green', label=\"train\")\n",
    "    # plt.plot(X, tst_scores, marker='o', color='red', markerfacecolor='red', label=\"test\")\n",
    "    # plt.legend()\n",
    "    # plt.xticks(X_axis, param_vals)\n",
    "\n",
    "    knn_estimators = train_estimators(X_trn, y_trn, sklearn.neighbors.KNeighborsClassifier, 'n_neighbors',\n",
    "                                      param_vals=param_vals)\n",
    "\n",
    "    plot_estimator_scores(knn_estimators, param_name='n_neighbors', param_vals=param_vals)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train_random_forests(X_trn, X_tst, y_trn, y_tst):\n",
    "    estimator = np.arange(1, 6) * 50\n",
    "    depths = np.arange(1, 6) * 5\n",
    "    estimators_meshed, depths_meshed = np.meshgrid(estimator, depths)\n",
    "\n",
    "    trn_scores = np.arange(0, 25, dtype=float)\n",
    "    tst_scores = np.arange(0, 25, dtype=float)\n",
    "    X_axis = np.arange(0, 25)\n",
    "    para_vals = np.array(\n",
    "        [\"50:5\", \"50:10\", \"50:15\", \"50:20\", \"50:25\", \"100:5\", \"100:10\", \"100:15\", \"100:20\", \"100:25\", \"150:5\", \"150:10\",\n",
    "         \"150:15\", \"150:20\",\"150:25\",\"200:5\", \"200:10\", \"200:15\", \"200:20\", \"200:25\", \"250:5\", \"250:10\", \"250:15\",\n",
    "         \"250:20\", \"250:25\"])\n",
    "    print(X_axis.shape)\n",
    "    print(para_vals.shape)\n",
    "    for i in range(5):\n",
    "        for j in range(5):\n",
    "            random_forest_model = sklearn.ensemble.RandomForestClassifier(n_estimators=estimators_meshed[j][i],\n",
    "                                                                          max_depth=depths_meshed[j][i],\n",
    "                                                                          random_state=0).fit(X_trn, y_trn)\n",
    "            trn_scores[i * 5 + j] = sklearn.metrics.accuracy_score(y_trn, random_forest_model.predict(X_trn))\n",
    "            print(\"Training Accuracy: %.2f %% with estimators = %d , depth = %d \\n\" % (\n",
    "                trn_scores[i * 5 + j] * 100, estimators_meshed[j][i], depths_meshed[j][i]))\n",
    "            tst_scores[i * 5 + j] = sklearn.metrics.accuracy_score(y_tst, random_forest_model.predict(X_tst))\n",
    "            print(\"Test Accuracy: %.2f %% with estimators = %d , depth = %d \\n\" % (\n",
    "                tst_scores[i * 5 + j] * 100, estimators_meshed[j][i], depths_meshed[j][i]))\n",
    "            print(\"Pr/Rec/Fscore: \")\n",
    "            print(sklearn.metrics.precision_recall_fscore_support(y_tst, random_forest_model.predict(X_tst), average='binary'))\n",
    "\n",
    "    plt.title(\"Random Forests score vs estimators : depth\")\n",
    "    plt.xlabel(\"estimators : depth\")\n",
    "    plt.ylabel(\"score\")\n",
    "    plt.ylim(0.0, 1.05)\n",
    "    plt.plot(X_axis, trn_scores, marker='o', color='green', markerfacecolor='green', label=\"train\")\n",
    "    plt.plot(X_axis, tst_scores, marker='o', color='red', markerfacecolor='red', label=\"test\")\n",
    "    plt.legend()\n",
    "    print(para_vals)\n",
    "    plt.xticks(X_axis, para_vals, fontsize=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This is the neural network trained in Lab 8\n",
    "\n",
    "def train_neural_network(X_trn, X_tst, y_trn, y_tst):\n",
    "    # Tensors setup\n",
    "    X_trn_torch = torch.tensor(X_trn, dtype=torch.float32)\n",
    "    y_trn_torch = torch.tensor(y_trn, dtype=torch.int64)\n",
    "    X_tst_torch = torch.tensor(X_tst, dtype=torch.float32)\n",
    "    y_tst_torch = torch.tensor(y_tst, dtype=torch.int64)\n",
    "\n",
    "    torch.manual_seed(0) # Ensure model weights initialized with same random numbers\n",
    "\n",
    "    # Create an object that holds a sequence of layers and activation functions\n",
    "    model = torch.nn.Sequential(\n",
    "        torch.nn.Linear(20, 10),   # Applies Wx+b from 10 dimensions down to 2\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(10,2)\n",
    "    )\n",
    "\n",
    "    # Create an object that can compute \"negative log likelihood of a softmax\"\n",
    "    loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # Use stochastic gradient descent to train the model\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    "\n",
    "    # Use 100 training samples at a time to compute the gradient.\n",
    "    batch_size = 200\n",
    "\n",
    "    # Make 10 passes over the training data, each time using batch_size samples to compute gradient\n",
    "    num_epoch = 10\n",
    "    next_epoch = 1\n",
    "\n",
    "    for epoch in range(next_epoch, next_epoch+num_epoch):\n",
    "        # Make an entire pass (an 'epoch') over the training data in batch_size chunks\n",
    "        for i in range(0, len(X_trn), batch_size):        \n",
    "            X_cur = X_trn_torch[i:i+batch_size]     # Slice out a mini-batch of features\n",
    "            y_cur = y_trn_torch[i:i+batch_size]     # Slice out a mini-batch of targets\n",
    "            \n",
    "            y_pred = model(X_cur)                   # Make predictions (final-layer activations)\n",
    "            l = loss(y_pred, y_cur)                 # Compute loss with respect to predictions\n",
    "            \n",
    "            model.zero_grad()                   # Reset all gradient accumulators to zero (PyTorch thing)\n",
    "            l.backward()                        # Compute gradient of loss wrt all parameters (backprop!)\n",
    "            optimizer.step()                    # Use the gradients to take a step with SGD.\n",
    "            \n",
    "        print(\"Epoch %2d: loss on final training batch: %.4f\" % (epoch, l.item()))\n",
    "        \n",
    "    print(\"Epoch %2d: loss on test set: %.4f\" % (epoch, loss(model(X_tst_torch), y_tst_torch)))\n",
    "    next_epoch = epoch+1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_trn, X_tst = scale_data(X_trn, X_tst)\n",
    "# train_neural_network(X, y)\n",
    "train_kNN_estimators(X_trn, X_tst, y_trn, y_tst)\n",
    "# train_random_forests(X_trn, X_tst, y_trn, y_tst)\n",
    "clf = train_dummy_classifier(X_trn, y_trn)\n",
    "y_pred = clf.predict(X_tst)\n",
    "stats = sklearn.metrics.precision_recall_fscore_support(y_tst, y_pred, average='binary')\n",
    "sklearn.metrics.accuracy_score(y_tst, y_pred)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "PyCharm (comp432)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
