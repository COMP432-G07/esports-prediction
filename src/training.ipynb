{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import sklearn.model_selection\n",
    "import sklearn.dummy\n",
    "import sklearn.metrics\n",
    "import sklearn.neural_network\n",
    "import sklearn.ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetch the data frames from the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read both the matches and the frames\n",
    "training_df = pd.read_csv('../data/processed/diff_train.csv').drop(labels=['tier','Unnamed: 0'],axis=1)\n",
    "test_df = pd.read_csv('../data/processed/diff_test.csv').drop(labels=['tier','Unnamed: 0'],axis=1)\n",
    "# Reinterpret all values as int32s\n",
    "training_df = training_df.astype({\n",
    "    'winner': 'int32',\n",
    "    'first_kill': 'int32',\n",
    "    'first_tower': 'int32',\n",
    "    'first_inhibitor': 'int32',\n",
    "    'first_baron': 'int32',\n",
    "    'first_dragon': 'int32',\n",
    "    'first_rift_herald': 'int32',\n",
    "})\n",
    "test_df = test_df.astype({\n",
    "    'winner': 'int32',\n",
    "    'first_kill': 'int32',\n",
    "    'first_tower': 'int32',\n",
    "    'first_inhibitor': 'int32',\n",
    "    'first_baron': 'int32',\n",
    "    'first_dragon': 'int32',\n",
    "    'first_rift_herald': 'int32',\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions to format the data prior to training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_data(X, y):\n",
    "    \"\"\"Plots the data from Exercise 1\"\"\"\n",
    "    plt.scatter(*X[y==-1].T, marker=\"x\", c=\"r\")\n",
    "    plt.scatter(*X[y==1].T, marker=\"x\", c=\"b\")\n",
    "    plt.xlim(0,1)\n",
    "    plt.ylim(0,1)\n",
    "    plt.gca().set_aspect('equal')  \n",
    "\n",
    "def plot_rf_prediction(model):\n",
    "    \"\"\"\n",
    "    Plots the model's predictions over all points in range 2D [-3, 3].\n",
    "    Assumes at most 3 classes.\n",
    "    \"\"\"\n",
    "    extent = (0, 1, 0, 1)\n",
    "    x1min, x1max ,x2min, x2max = extent\n",
    "    x1, x2 = np.meshgrid(np.linspace(x1min, x1max, 100), np.linspace(x2min, x2max, 100))\n",
    "    X = np.column_stack([x1.ravel(), x2.ravel()])\n",
    "    y = model.predict(X).reshape(x1.shape)\n",
    "    cmap = matplotlib.colors.ListedColormap(['r', 'b', 'g'])\n",
    "    plt.imshow(y, extent=extent, origin='lower', alpha=0.4, vmin=0, vmax=2, cmap=cmap, interpolation='nearest')\n",
    "    plt.xlim([x1min, x1max])\n",
    "    plt.ylim([x2min, x2max])\n",
    "    plt.gca().set_aspect('equal')\n",
    "\n",
    "def scale_data(X_trn, X_tst):\n",
    "    # Scale the data with MinMax to avoid negative values\n",
    "    scaler = sklearn.preprocessing.MinMaxScaler()\n",
    "    scaler.fit(X_trn)\n",
    "    X_trn = scaler.transform(X_trn)\n",
    "    X_tst = scaler.transform(X_tst)\n",
    "\n",
    "    return X_trn, X_tst\n",
    "\n",
    "def score_estimators(X, y, estimators):\n",
    "    \"\"\"Scores each estimator on (X, y), returning a list of scores.\"\"\"\n",
    "    # Your implementation here. Aim for 1-4 lines.\n",
    "    scores = [0 for _ in range(len(estimators))]\n",
    "    for x in range(len(estimators)):\n",
    "        scores[x] = sklearn.metrics.accuracy_score(y, estimators[x].predict(X))\n",
    "        print(sklearn.metrics.precision_recall_fscore_support(y, estimators[x].predict(X), average='binary'))\n",
    "    return scores\n",
    "\n",
    "def plot_estimator_scores(estimators, param_name, param_vals):\n",
    "    \"\"\"\n",
    "    Plots the training, validation, and testing scores of a list of estimators,\n",
    "    where `param_name` and `param_vals` are the same as for `train_estimators`.\n",
    "    The estimator with best validation score will be highlighted with an 'x'.\n",
    "    \"\"\"\n",
    "    # Your implementation here. Use as many lines as you need\n",
    "    plt.figure()\n",
    "    X = np.arange(0, len(param_vals))\n",
    "    trn_scores = score_estimators(X_trn, y_trn, estimators)\n",
    "    tst_scores = score_estimators(X_tst, y_tst, estimators)\n",
    "    index = np.argmin(trn_scores - tst_scores)\n",
    "    print(tst_scores[index])\n",
    "    plt.title(estimators[0].__class__.__name__ + \" score vs \" + param_name)\n",
    "    plt.xlabel(param_name)\n",
    "    plt.ylabel(\"score\")\n",
    "    plt.ylim(0.0, 1.05)\n",
    "    plt.scatter(X[index], tst_scores[index], marker='x', color='black', s=200)\n",
    "    plt.plot(X, trn_scores, marker='o', color='green', markerfacecolor='green', label=\"train\")\n",
    "    plt.plot(X, tst_scores, marker='o', color='red', markerfacecolor='red', label=\"test\")\n",
    "    plt.text(0, 0.4,\n",
    "             \"Optimal Test Accuracy = %.2f%% with %s = %d \" % (tst_scores[index] * 100, param_name, param_vals[index]))\n",
    "    plt.legend()\n",
    "    plt.xticks(X, param_vals)\n",
    "    \n",
    "\n",
    "def train_estimators(X, y, estimator_type, param_name, param_vals, **kwargs):\n",
    "    estimators = [0 for i in range(len(param_vals))]\n",
    "    for x in range(len(param_vals)):\n",
    "        estimators[x] = estimator_type(**{param_name: param_vals[x]}, **kwargs).fit(X, y)\n",
    "        # Training DecisionTreeClassifier(max_depth=1, random_state=0, splitter='random')...\n",
    "        print(\"Training \" + estimator_type.__name__ + \"(\" + param_name + \"=%d \" % param_vals[x] + \" {0}={1} \".format(\n",
    "            kwargs.keys(), kwargs.values()))\n",
    "    return estimators\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature selection: Select an X and y we want to train on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trn = training_df[['kill_advantage','gold_advantage']].values\n",
    "y_trn = training_df[['winner']].values.T[0]\n",
    "X_tst = test_df[['kill_advantage','gold_advantage']].values\n",
    "y_tst = test_df[['winner']].values.T[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_data(X_trn[:300], y_trn[:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dummy Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Here we train a dummy classifier to compare performance\n",
    "def train_dummy_classifier(X, y):\n",
    "    dummy_clf = sklearn.dummy.DummyClassifier(strategy='uniform', random_state=0)\n",
    "    # Scale the data with MinMax to avoid negative values\n",
    "    dummy_clf.fit(X, y)\n",
    "    return dummy_clf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train_kNN_estimators(X, y):\n",
    "    param_vals = np.arange(1,6) * 5\n",
    "\n",
    "    knn_estimators = train_estimators(X, y, sklearn.neighbors.KNeighborsClassifier, 'n_neighbors', param_vals=param_vals)\n",
    "\n",
    "    plot_estimator_scores(knn_estimators, param_name='n_neighbors', param_vals=param_vals)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train_random_forests(X, y):\n",
    "    estimator = np.arange(1, 5) * 50\n",
    "    depths = np.arange(1, 5) * 5\n",
    "    param_grid = {\n",
    "        'n_estimators': estimator, \n",
    "        'max_depth': depths\n",
    "    }\n",
    "    clf = sklearn.ensemble.RandomForestClassifier(random_state=0)\n",
    "    gscv = sklearn.model_selection.GridSearchCV(clf, param_grid=param_grid, verbose=1, cv=3)\n",
    "    gscv.fit(X, y)\n",
    "    \n",
    "    print(gscv.cv_results_)\n",
    "    \n",
    "    return gscv.best_estimator_\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15680, 2)\n",
      "Fitting 3 folds for each of 48 candidates, totalling 144 fits\n",
      "Iteration 1, loss = 0.67975992\n",
      "Iteration 2, loss = 0.65394503\n",
      "Iteration 3, loss = 0.64025174\n",
      "Iteration 4, loss = 0.62744581\n",
      "Iteration 5, loss = 0.61550372\n",
      "Iteration 6, loss = 0.60434211\n",
      "Iteration 7, loss = 0.59394855\n",
      "Iteration 8, loss = 0.58423548\n",
      "Iteration 9, loss = 0.57510189\n",
      "Iteration 10, loss = 0.56654467\n",
      "Iteration 11, loss = 0.55856361\n",
      "Iteration 12, loss = 0.55101141\n",
      "Iteration 13, loss = 0.54389842\n",
      "Iteration 14, loss = 0.53727232\n",
      "Iteration 15, loss = 0.53097686\n",
      "Iteration 16, loss = 0.52510123\n",
      "Iteration 17, loss = 0.51951399\n",
      "Iteration 18, loss = 0.51425465\n",
      "Iteration 19, loss = 0.50923013\n",
      "Iteration 20, loss = 0.50451989\n",
      "Iteration 21, loss = 0.50003258\n",
      "Iteration 22, loss = 0.49578168\n",
      "Iteration 23, loss = 0.49176602\n",
      "Iteration 24, loss = 0.48798216\n",
      "Iteration 25, loss = 0.48426453\n",
      "Iteration 26, loss = 0.48075867\n",
      "Iteration 27, loss = 0.47746204\n",
      "Iteration 28, loss = 0.47428984\n",
      "Iteration 29, loss = 0.47125927\n",
      "Iteration 30, loss = 0.46838446\n",
      "Iteration 31, loss = 0.46567031\n",
      "Iteration 32, loss = 0.46302711\n",
      "Iteration 33, loss = 0.46051512\n",
      "Iteration 34, loss = 0.45810512\n",
      "Iteration 35, loss = 0.45577369\n",
      "Iteration 36, loss = 0.45359453\n",
      "Iteration 37, loss = 0.45144778\n",
      "Iteration 38, loss = 0.44942404\n",
      "Iteration 39, loss = 0.44744858\n",
      "Iteration 40, loss = 0.44555896\n",
      "Iteration 41, loss = 0.44374348\n",
      "Iteration 42, loss = 0.44199862\n",
      "Iteration 43, loss = 0.44031514\n",
      "Iteration 44, loss = 0.43868818\n",
      "Iteration 45, loss = 0.43713297\n",
      "Iteration 46, loss = 0.43564446\n",
      "Iteration 47, loss = 0.43418430\n",
      "Iteration 48, loss = 0.43277229\n",
      "Iteration 49, loss = 0.43142580\n",
      "Iteration 50, loss = 0.43011476\n",
      "Iteration 51, loss = 0.42887781\n",
      "Iteration 52, loss = 0.42767442\n",
      "Iteration 53, loss = 0.42649751\n",
      "Iteration 54, loss = 0.42534053\n",
      "Iteration 55, loss = 0.42424997\n",
      "Iteration 56, loss = 0.42318692\n",
      "Iteration 57, loss = 0.42219489\n",
      "Iteration 58, loss = 0.42115542\n",
      "Iteration 59, loss = 0.42019172\n",
      "Iteration 60, loss = 0.41927148\n",
      "Iteration 61, loss = 0.41837301\n",
      "Iteration 62, loss = 0.41746161\n",
      "Iteration 63, loss = 0.41660835\n",
      "Iteration 64, loss = 0.41577336\n",
      "Iteration 65, loss = 0.41495857\n",
      "Iteration 66, loss = 0.41417224\n",
      "Iteration 67, loss = 0.41342853\n",
      "Iteration 68, loss = 0.41267280\n",
      "Iteration 69, loss = 0.41197754\n",
      "Iteration 70, loss = 0.41126790\n",
      "Iteration 71, loss = 0.41057622\n",
      "Iteration 72, loss = 0.40991620\n",
      "Iteration 73, loss = 0.40926147\n",
      "Iteration 74, loss = 0.40864785\n",
      "Iteration 75, loss = 0.40803176\n",
      "Iteration 76, loss = 0.40743332\n",
      "Iteration 77, loss = 0.40686185\n",
      "Iteration 78, loss = 0.40628510\n",
      "Iteration 79, loss = 0.40574867\n",
      "Iteration 80, loss = 0.40524613\n",
      "Iteration 81, loss = 0.40471775\n",
      "Iteration 82, loss = 0.40418212\n",
      "Iteration 83, loss = 0.40369929\n",
      "Iteration 84, loss = 0.40321324\n",
      "Iteration 85, loss = 0.40273819\n",
      "Iteration 86, loss = 0.40229465\n",
      "Iteration 87, loss = 0.40183837\n",
      "Iteration 88, loss = 0.40143566\n",
      "Iteration 89, loss = 0.40096474\n",
      "Iteration 90, loss = 0.40054311\n",
      "Iteration 91, loss = 0.40014653\n",
      "Iteration 92, loss = 0.39973768\n",
      "Iteration 93, loss = 0.39935729\n",
      "Iteration 94, loss = 0.39897832\n",
      "Iteration 95, loss = 0.39860988\n",
      "Iteration 96, loss = 0.39823472\n",
      "Iteration 97, loss = 0.39789983\n",
      "Iteration 98, loss = 0.39755490\n",
      "Iteration 99, loss = 0.39723846\n",
      "Iteration 100, loss = 0.39688395\n",
      "Iteration 101, loss = 0.39654767\n",
      "Iteration 102, loss = 0.39625585\n",
      "Iteration 103, loss = 0.39601071\n",
      "Iteration 104, loss = 0.39563180\n",
      "Iteration 105, loss = 0.39532177\n",
      "Iteration 106, loss = 0.39507569\n",
      "Iteration 107, loss = 0.39475189\n",
      "Iteration 108, loss = 0.39450141\n",
      "Iteration 109, loss = 0.39421241\n",
      "Iteration 110, loss = 0.39394840\n",
      "Iteration 111, loss = 0.39366603\n",
      "Iteration 112, loss = 0.39342268\n",
      "Iteration 113, loss = 0.39315615\n",
      "Iteration 114, loss = 0.39292325\n",
      "Iteration 115, loss = 0.39267577\n",
      "Iteration 116, loss = 0.39246617\n",
      "Iteration 117, loss = 0.39222105\n",
      "Iteration 118, loss = 0.39200429\n",
      "Iteration 119, loss = 0.39178613\n",
      "Iteration 120, loss = 0.39158150\n",
      "Iteration 121, loss = 0.39135617\n",
      "Iteration 122, loss = 0.39113210\n",
      "Iteration 123, loss = 0.39091701\n",
      "Iteration 124, loss = 0.39071917\n",
      "Iteration 125, loss = 0.39051080\n",
      "Iteration 126, loss = 0.39035415\n",
      "Iteration 127, loss = 0.39017766\n",
      "Iteration 128, loss = 0.38995232\n",
      "Iteration 129, loss = 0.38977825\n",
      "Iteration 130, loss = 0.38962438\n",
      "Iteration 131, loss = 0.38941100\n",
      "Iteration 132, loss = 0.38924658\n",
      "Iteration 133, loss = 0.38906564\n",
      "Iteration 134, loss = 0.38890162\n",
      "Iteration 135, loss = 0.38874713\n",
      "Iteration 136, loss = 0.38857102\n",
      "Iteration 137, loss = 0.38844559\n",
      "Iteration 138, loss = 0.38827135\n",
      "Iteration 139, loss = 0.38815425\n",
      "Iteration 140, loss = 0.38797416\n",
      "Iteration 141, loss = 0.38782343\n",
      "Iteration 142, loss = 0.38767555\n",
      "Iteration 143, loss = 0.38753662\n",
      "Iteration 144, loss = 0.38741485\n",
      "Iteration 145, loss = 0.38728399\n",
      "Iteration 146, loss = 0.38713573\n",
      "Iteration 147, loss = 0.38701585\n",
      "Iteration 148, loss = 0.38688215\n",
      "Iteration 149, loss = 0.38679029\n",
      "Iteration 150, loss = 0.38661751\n",
      "Iteration 151, loss = 0.38648722\n",
      "Iteration 152, loss = 0.38638827\n",
      "Iteration 153, loss = 0.38625768\n",
      "Iteration 154, loss = 0.38613265\n",
      "Iteration 155, loss = 0.38602040\n",
      "Iteration 156, loss = 0.38591937\n",
      "Iteration 157, loss = 0.38579558\n",
      "Iteration 158, loss = 0.38569763\n",
      "Iteration 159, loss = 0.38559092\n",
      "Iteration 160, loss = 0.38549775\n",
      "Iteration 161, loss = 0.38536754\n",
      "Iteration 162, loss = 0.38527754\n",
      "Iteration 163, loss = 0.38518563\n",
      "Iteration 164, loss = 0.38507237\n",
      "Iteration 165, loss = 0.38500443\n",
      "Iteration 166, loss = 0.38486681\n",
      "Iteration 167, loss = 0.38479484\n",
      "Iteration 168, loss = 0.38469886\n",
      "Iteration 169, loss = 0.38462863\n",
      "Iteration 170, loss = 0.38450799\n",
      "Iteration 171, loss = 0.38442680\n",
      "Iteration 172, loss = 0.38432120\n",
      "Iteration 173, loss = 0.38423057\n",
      "Iteration 174, loss = 0.38415343\n",
      "Iteration 175, loss = 0.38406531\n",
      "Iteration 176, loss = 0.38397813\n",
      "Iteration 177, loss = 0.38389846\n",
      "Iteration 178, loss = 0.38382336\n",
      "Iteration 179, loss = 0.38374538\n",
      "Iteration 180, loss = 0.38366613\n",
      "Iteration 181, loss = 0.38359614\n",
      "Iteration 182, loss = 0.38352467\n",
      "Iteration 183, loss = 0.38344031\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.68096745\n",
      "Iteration 2, loss = 0.65469753\n",
      "Iteration 3, loss = 0.64100819\n",
      "Iteration 4, loss = 0.62834740\n",
      "Iteration 5, loss = 0.61659484\n",
      "Iteration 6, loss = 0.60551340\n",
      "Iteration 7, loss = 0.59529300\n",
      "Iteration 8, loss = 0.58568962\n",
      "Iteration 9, loss = 0.57673775\n",
      "Iteration 10, loss = 0.56831809\n",
      "Iteration 11, loss = 0.56045394\n",
      "Iteration 12, loss = 0.55307844\n",
      "Iteration 13, loss = 0.54614795\n",
      "Iteration 14, loss = 0.53965212\n",
      "Iteration 15, loss = 0.53350272\n",
      "Iteration 16, loss = 0.52775118\n",
      "Iteration 17, loss = 0.52230853\n",
      "Iteration 18, loss = 0.51716000\n",
      "Iteration 19, loss = 0.51232664\n",
      "Iteration 20, loss = 0.50770218\n",
      "Iteration 21, loss = 0.50340349\n",
      "Iteration 22, loss = 0.49928523\n",
      "Iteration 23, loss = 0.49531844\n",
      "Iteration 24, loss = 0.49159389\n",
      "Iteration 25, loss = 0.48807535\n",
      "Iteration 26, loss = 0.48469710\n",
      "Iteration 27, loss = 0.48152682\n",
      "Iteration 28, loss = 0.47846663\n",
      "Iteration 29, loss = 0.47552134\n",
      "Iteration 30, loss = 0.47278048\n",
      "Iteration 31, loss = 0.47011168\n",
      "Iteration 32, loss = 0.46756721\n",
      "Iteration 33, loss = 0.46513418\n",
      "Iteration 34, loss = 0.46277505\n",
      "Iteration 35, loss = 0.46054747\n",
      "Iteration 36, loss = 0.45841713\n",
      "Iteration 37, loss = 0.45636615\n",
      "Iteration 38, loss = 0.45441651\n",
      "Iteration 39, loss = 0.45254788\n",
      "Iteration 40, loss = 0.45074251\n",
      "Iteration 41, loss = 0.44902242\n",
      "Iteration 42, loss = 0.44733150\n",
      "Iteration 43, loss = 0.44569676\n",
      "Iteration 44, loss = 0.44422517\n",
      "Iteration 45, loss = 0.44263920\n",
      "Iteration 46, loss = 0.44121583\n",
      "Iteration 47, loss = 0.43983401\n",
      "Iteration 48, loss = 0.43848258\n",
      "Iteration 49, loss = 0.43719954\n",
      "Iteration 50, loss = 0.43594846\n",
      "Iteration 51, loss = 0.43478730\n",
      "Iteration 52, loss = 0.43357975\n",
      "Iteration 53, loss = 0.43247088\n",
      "Iteration 54, loss = 0.43137450\n",
      "Iteration 55, loss = 0.43035887\n",
      "Iteration 56, loss = 0.42929738\n",
      "Iteration 57, loss = 0.42832837\n",
      "Iteration 58, loss = 0.42737085\n",
      "Iteration 59, loss = 0.42643860\n",
      "Iteration 60, loss = 0.42556361\n",
      "Iteration 61, loss = 0.42472270\n",
      "Iteration 62, loss = 0.42382953\n",
      "Iteration 63, loss = 0.42302247\n",
      "Iteration 64, loss = 0.42223520\n",
      "Iteration 65, loss = 0.42146022\n",
      "Iteration 66, loss = 0.42071577\n",
      "Iteration 67, loss = 0.42002390\n",
      "Iteration 68, loss = 0.41931070\n",
      "Iteration 69, loss = 0.41862675\n",
      "Iteration 70, loss = 0.41795749\n",
      "Iteration 71, loss = 0.41732392\n",
      "Iteration 72, loss = 0.41666434\n",
      "Iteration 73, loss = 0.41604113\n",
      "Iteration 74, loss = 0.41546091\n",
      "Iteration 75, loss = 0.41486290\n",
      "Iteration 76, loss = 0.41433573\n",
      "Iteration 77, loss = 0.41380042\n",
      "Iteration 78, loss = 0.41322637\n",
      "Iteration 79, loss = 0.41269842\n",
      "Iteration 80, loss = 0.41221821\n",
      "Iteration 81, loss = 0.41174334\n",
      "Iteration 82, loss = 0.41122828\n",
      "Iteration 83, loss = 0.41075244\n",
      "Iteration 84, loss = 0.41030182\n",
      "Iteration 85, loss = 0.40983739\n",
      "Iteration 86, loss = 0.40941621\n",
      "Iteration 87, loss = 0.40898637\n",
      "Iteration 88, loss = 0.40860814\n",
      "Iteration 89, loss = 0.40816173\n",
      "Iteration 90, loss = 0.40780168\n",
      "Iteration 91, loss = 0.40739669\n",
      "Iteration 92, loss = 0.40700270\n",
      "Iteration 93, loss = 0.40663664\n",
      "Iteration 94, loss = 0.40631053\n",
      "Iteration 95, loss = 0.40592954\n",
      "Iteration 96, loss = 0.40559791\n",
      "Iteration 97, loss = 0.40530185\n",
      "Iteration 98, loss = 0.40495105\n",
      "Iteration 99, loss = 0.40464713\n",
      "Iteration 100, loss = 0.40430916\n",
      "Iteration 101, loss = 0.40399217\n",
      "Iteration 102, loss = 0.40369241\n",
      "Iteration 103, loss = 0.40339014\n",
      "Iteration 104, loss = 0.40316298\n",
      "Iteration 105, loss = 0.40282448\n",
      "Iteration 106, loss = 0.40256740\n",
      "Iteration 107, loss = 0.40230118\n",
      "Iteration 108, loss = 0.40204782\n",
      "Iteration 109, loss = 0.40180161\n",
      "Iteration 110, loss = 0.40157117\n",
      "Iteration 111, loss = 0.40128480\n",
      "Iteration 112, loss = 0.40102933\n",
      "Iteration 113, loss = 0.40079935\n",
      "Iteration 114, loss = 0.40055583\n",
      "Iteration 115, loss = 0.40036770\n",
      "Iteration 116, loss = 0.40018141\n",
      "Iteration 117, loss = 0.39989102\n",
      "Iteration 118, loss = 0.39969692\n",
      "Iteration 119, loss = 0.39951670\n",
      "Iteration 120, loss = 0.39928459\n",
      "Iteration 121, loss = 0.39908037\n",
      "Iteration 122, loss = 0.39889245\n",
      "Iteration 123, loss = 0.39868796\n",
      "Iteration 124, loss = 0.39849273\n",
      "Iteration 125, loss = 0.39830957\n",
      "Iteration 126, loss = 0.39813553\n",
      "Iteration 127, loss = 0.39795802\n",
      "Iteration 128, loss = 0.39776601\n",
      "Iteration 129, loss = 0.39761456\n",
      "Iteration 130, loss = 0.39743617\n",
      "Iteration 131, loss = 0.39726661\n",
      "Iteration 132, loss = 0.39709438\n",
      "Iteration 133, loss = 0.39694628\n",
      "Iteration 134, loss = 0.39679043\n",
      "Iteration 135, loss = 0.39663167\n",
      "Iteration 136, loss = 0.39650601\n",
      "Iteration 137, loss = 0.39632842\n",
      "Iteration 138, loss = 0.39618630\n",
      "Iteration 139, loss = 0.39607467\n",
      "Iteration 140, loss = 0.39590802\n",
      "Iteration 141, loss = 0.39576972\n",
      "Iteration 142, loss = 0.39563962\n",
      "Iteration 143, loss = 0.39552820\n",
      "Iteration 144, loss = 0.39538837\n",
      "Iteration 145, loss = 0.39527847\n",
      "Iteration 146, loss = 0.39513736\n",
      "Iteration 147, loss = 0.39500512\n",
      "Iteration 148, loss = 0.39494679\n",
      "Iteration 149, loss = 0.39479699\n",
      "Iteration 150, loss = 0.39464379\n",
      "Iteration 151, loss = 0.39454978\n",
      "Iteration 152, loss = 0.39442062\n",
      "Iteration 153, loss = 0.39430720\n",
      "Iteration 154, loss = 0.39419306\n",
      "Iteration 155, loss = 0.39410386\n",
      "Iteration 156, loss = 0.39401995\n",
      "Iteration 157, loss = 0.39386948\n",
      "Iteration 158, loss = 0.39377297\n",
      "Iteration 159, loss = 0.39367799\n",
      "Iteration 160, loss = 0.39357428\n",
      "Iteration 161, loss = 0.39346787\n",
      "Iteration 162, loss = 0.39338984\n",
      "Iteration 163, loss = 0.39329983\n",
      "Iteration 164, loss = 0.39319816\n",
      "Iteration 165, loss = 0.39310170\n",
      "Iteration 166, loss = 0.39300786\n",
      "Iteration 167, loss = 0.39291308\n",
      "Iteration 168, loss = 0.39282606\n",
      "Iteration 169, loss = 0.39274477\n",
      "Iteration 170, loss = 0.39266182\n",
      "Iteration 171, loss = 0.39256056\n",
      "Iteration 172, loss = 0.39248871\n",
      "Iteration 173, loss = 0.39242259\n",
      "Iteration 174, loss = 0.39233004\n",
      "Iteration 175, loss = 0.39226319\n",
      "Iteration 176, loss = 0.39220196\n",
      "Iteration 177, loss = 0.39210147\n",
      "Iteration 178, loss = 0.39203286\n",
      "Iteration 179, loss = 0.39196195\n",
      "Iteration 180, loss = 0.39190970\n",
      "Iteration 181, loss = 0.39187196\n",
      "Iteration 182, loss = 0.39172833\n",
      "Iteration 183, loss = 0.39165790\n",
      "Iteration 184, loss = 0.39158682\n",
      "Iteration 185, loss = 0.39152429\n",
      "Iteration 186, loss = 0.39146731\n",
      "Iteration 187, loss = 0.39140157\n",
      "Iteration 188, loss = 0.39133372\n",
      "Iteration 189, loss = 0.39126538\n",
      "Iteration 190, loss = 0.39121821\n",
      "Iteration 191, loss = 0.39115364\n",
      "Iteration 192, loss = 0.39108148\n",
      "Iteration 193, loss = 0.39102564\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.68141899\n",
      "Iteration 2, loss = 0.65537792\n",
      "Iteration 3, loss = 0.64202212\n",
      "Iteration 4, loss = 0.62960945\n",
      "Iteration 5, loss = 0.61804165\n",
      "Iteration 6, loss = 0.60727575\n",
      "Iteration 7, loss = 0.59722430\n",
      "Iteration 8, loss = 0.58790051\n",
      "Iteration 9, loss = 0.57913998\n",
      "Iteration 10, loss = 0.57095438\n",
      "Iteration 11, loss = 0.56324110\n",
      "Iteration 12, loss = 0.55603865\n",
      "Iteration 13, loss = 0.54929016\n",
      "Iteration 14, loss = 0.54293651\n",
      "Iteration 15, loss = 0.53695108\n",
      "Iteration 16, loss = 0.53131137\n",
      "Iteration 17, loss = 0.52602478\n",
      "Iteration 18, loss = 0.52098861\n",
      "Iteration 19, loss = 0.51627790\n",
      "Iteration 20, loss = 0.51181850\n",
      "Iteration 21, loss = 0.50756766\n",
      "Iteration 22, loss = 0.50361994\n",
      "Iteration 23, loss = 0.49976005\n",
      "Iteration 24, loss = 0.49618870\n",
      "Iteration 25, loss = 0.49268475\n",
      "Iteration 26, loss = 0.48943315\n",
      "Iteration 27, loss = 0.48629896\n",
      "Iteration 28, loss = 0.48335458\n",
      "Iteration 29, loss = 0.48050529\n",
      "Iteration 30, loss = 0.47781597\n",
      "Iteration 31, loss = 0.47527078\n",
      "Iteration 32, loss = 0.47279937\n",
      "Iteration 33, loss = 0.47044763\n",
      "Iteration 34, loss = 0.46820538\n",
      "Iteration 35, loss = 0.46605292\n",
      "Iteration 36, loss = 0.46398667\n",
      "Iteration 37, loss = 0.46207913\n",
      "Iteration 38, loss = 0.46013646\n",
      "Iteration 39, loss = 0.45833888\n",
      "Iteration 40, loss = 0.45658145\n",
      "Iteration 41, loss = 0.45488699\n",
      "Iteration 42, loss = 0.45327073\n",
      "Iteration 43, loss = 0.45170949\n",
      "Iteration 44, loss = 0.45021077\n",
      "Iteration 45, loss = 0.44882629\n",
      "Iteration 46, loss = 0.44738121\n",
      "Iteration 47, loss = 0.44607631\n",
      "Iteration 48, loss = 0.44479248\n",
      "Iteration 49, loss = 0.44355038\n",
      "Iteration 50, loss = 0.44234209\n",
      "Iteration 51, loss = 0.44122230\n",
      "Iteration 52, loss = 0.44013568\n",
      "Iteration 53, loss = 0.43899096\n",
      "Iteration 54, loss = 0.43794244\n",
      "Iteration 55, loss = 0.43694516\n",
      "Iteration 56, loss = 0.43595125\n",
      "Iteration 57, loss = 0.43501159\n",
      "Iteration 58, loss = 0.43413256\n",
      "Iteration 59, loss = 0.43321330\n",
      "Iteration 60, loss = 0.43236328\n",
      "Iteration 61, loss = 0.43157280\n",
      "Iteration 62, loss = 0.43072204\n",
      "Iteration 63, loss = 0.42996186\n",
      "Iteration 64, loss = 0.42919477\n",
      "Iteration 65, loss = 0.42845302\n",
      "Iteration 66, loss = 0.42776148\n",
      "Iteration 67, loss = 0.42706374\n",
      "Iteration 68, loss = 0.42637863\n",
      "Iteration 69, loss = 0.42573738\n",
      "Iteration 70, loss = 0.42513581\n",
      "Iteration 71, loss = 0.42447435\n",
      "Iteration 72, loss = 0.42389743\n",
      "Iteration 73, loss = 0.42330507\n",
      "Iteration 74, loss = 0.42273083\n",
      "Iteration 75, loss = 0.42219151\n",
      "Iteration 76, loss = 0.42165031\n",
      "Iteration 77, loss = 0.42113017\n",
      "Iteration 78, loss = 0.42063949\n",
      "Iteration 79, loss = 0.42014178\n",
      "Iteration 80, loss = 0.41964797\n",
      "Iteration 81, loss = 0.41917497\n",
      "Iteration 82, loss = 0.41877700\n",
      "Iteration 83, loss = 0.41827640\n",
      "Iteration 84, loss = 0.41785854\n",
      "Iteration 85, loss = 0.41745097\n",
      "Iteration 86, loss = 0.41701234\n",
      "Iteration 87, loss = 0.41660787\n",
      "Iteration 88, loss = 0.41621742\n",
      "Iteration 89, loss = 0.41587930\n",
      "Iteration 90, loss = 0.41547331\n",
      "Iteration 91, loss = 0.41509757\n",
      "Iteration 92, loss = 0.41477417\n",
      "Iteration 93, loss = 0.41438582\n",
      "Iteration 94, loss = 0.41405983\n",
      "Iteration 95, loss = 0.41374631\n",
      "Iteration 96, loss = 0.41343404\n",
      "Iteration 97, loss = 0.41308488\n",
      "Iteration 98, loss = 0.41279663\n",
      "Iteration 99, loss = 0.41249017\n",
      "Iteration 100, loss = 0.41217436\n",
      "Iteration 101, loss = 0.41188377\n",
      "Iteration 102, loss = 0.41162659\n",
      "Iteration 103, loss = 0.41132345\n",
      "Iteration 104, loss = 0.41107293\n",
      "Iteration 105, loss = 0.41079431\n",
      "Iteration 106, loss = 0.41055862\n",
      "Iteration 107, loss = 0.41028782\n",
      "Iteration 108, loss = 0.41006800\n",
      "Iteration 109, loss = 0.40979002\n",
      "Iteration 110, loss = 0.40957157\n",
      "Iteration 111, loss = 0.40933085\n",
      "Iteration 112, loss = 0.40910751\n",
      "Iteration 113, loss = 0.40888348\n",
      "Iteration 114, loss = 0.40865985\n",
      "Iteration 115, loss = 0.40843719\n",
      "Iteration 116, loss = 0.40824072\n",
      "Iteration 117, loss = 0.40805422\n",
      "Iteration 118, loss = 0.40782545\n",
      "Iteration 119, loss = 0.40766045\n",
      "Iteration 120, loss = 0.40745634\n",
      "Iteration 121, loss = 0.40727225\n",
      "Iteration 122, loss = 0.40707183\n",
      "Iteration 123, loss = 0.40692628\n",
      "Iteration 124, loss = 0.40675584\n",
      "Iteration 125, loss = 0.40652470\n",
      "Iteration 126, loss = 0.40636559\n",
      "Iteration 127, loss = 0.40620485\n",
      "Iteration 128, loss = 0.40603637\n",
      "Iteration 129, loss = 0.40587451\n",
      "Iteration 130, loss = 0.40573880\n",
      "Iteration 131, loss = 0.40558485\n",
      "Iteration 132, loss = 0.40540655\n",
      "Iteration 133, loss = 0.40530906\n",
      "Iteration 134, loss = 0.40512687\n",
      "Iteration 135, loss = 0.40497159\n",
      "Iteration 136, loss = 0.40486785\n",
      "Iteration 137, loss = 0.40469542\n",
      "Iteration 138, loss = 0.40458837\n",
      "Iteration 139, loss = 0.40442025\n",
      "Iteration 140, loss = 0.40428210\n",
      "Iteration 141, loss = 0.40415669\n",
      "Iteration 142, loss = 0.40404056\n",
      "Iteration 143, loss = 0.40391945\n",
      "Iteration 144, loss = 0.40379621\n",
      "Iteration 145, loss = 0.40369198\n",
      "Iteration 146, loss = 0.40360261\n",
      "Iteration 147, loss = 0.40348551\n",
      "Iteration 148, loss = 0.40330754\n",
      "Iteration 149, loss = 0.40321345\n",
      "Iteration 150, loss = 0.40309030\n",
      "Iteration 151, loss = 0.40304496\n",
      "Iteration 152, loss = 0.40291144\n",
      "Iteration 153, loss = 0.40279232\n",
      "Iteration 154, loss = 0.40271245\n",
      "Iteration 155, loss = 0.40258354\n",
      "Iteration 156, loss = 0.40245046\n",
      "Iteration 157, loss = 0.40241660\n",
      "Iteration 158, loss = 0.40228584\n",
      "Iteration 159, loss = 0.40217786\n",
      "Iteration 160, loss = 0.40209015\n",
      "Iteration 161, loss = 0.40198921\n",
      "Iteration 162, loss = 0.40193369\n",
      "Iteration 163, loss = 0.40184108\n",
      "Iteration 164, loss = 0.40174933\n",
      "Iteration 165, loss = 0.40164994\n",
      "Iteration 166, loss = 0.40155141\n",
      "Iteration 167, loss = 0.40147067\n",
      "Iteration 168, loss = 0.40138875\n",
      "Iteration 169, loss = 0.40130921\n",
      "Iteration 170, loss = 0.40122008\n",
      "Iteration 171, loss = 0.40114122\n",
      "Iteration 172, loss = 0.40107690\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.67807457\n",
      "Iteration 2, loss = 0.64400744\n",
      "Iteration 3, loss = 0.61406341\n",
      "Iteration 4, loss = 0.58774510\n",
      "Iteration 5, loss = 0.56469772\n",
      "Iteration 6, loss = 0.54437827\n",
      "Iteration 7, loss = 0.52667325\n",
      "Iteration 8, loss = 0.51113013\n",
      "Iteration 9, loss = 0.49756639\n",
      "Iteration 10, loss = 0.48556221\n",
      "Iteration 11, loss = 0.47518821\n",
      "Iteration 12, loss = 0.46587686\n",
      "Iteration 13, loss = 0.45759234\n",
      "Iteration 14, loss = 0.45031285\n",
      "Iteration 15, loss = 0.44367817\n",
      "Iteration 16, loss = 0.43799484\n",
      "Iteration 17, loss = 0.43278609\n",
      "Iteration 18, loss = 0.42811299\n",
      "Iteration 19, loss = 0.42370272\n",
      "Iteration 20, loss = 0.41986931\n",
      "Iteration 21, loss = 0.41628781\n",
      "Iteration 22, loss = 0.41332541\n",
      "Iteration 23, loss = 0.41036882\n",
      "Iteration 24, loss = 0.40794541\n",
      "Iteration 25, loss = 0.40544150\n",
      "Iteration 26, loss = 0.40306694\n",
      "Iteration 27, loss = 0.40134183\n",
      "Iteration 28, loss = 0.39936827\n",
      "Iteration 29, loss = 0.39774737\n",
      "Iteration 30, loss = 0.39620493\n",
      "Iteration 31, loss = 0.39483038\n",
      "Iteration 32, loss = 0.39353154\n",
      "Iteration 33, loss = 0.39234303\n",
      "Iteration 34, loss = 0.39125778\n",
      "Iteration 35, loss = 0.39021676\n",
      "Iteration 36, loss = 0.38924239\n",
      "Iteration 37, loss = 0.38840829\n",
      "Iteration 38, loss = 0.38768012\n",
      "Iteration 39, loss = 0.38697751\n",
      "Iteration 40, loss = 0.38633819\n",
      "Iteration 41, loss = 0.38563294\n",
      "Iteration 42, loss = 0.38497561\n",
      "Iteration 43, loss = 0.38454556\n",
      "Iteration 44, loss = 0.38400243\n",
      "Iteration 45, loss = 0.38357637\n",
      "Iteration 46, loss = 0.38303972\n",
      "Iteration 47, loss = 0.38270372\n",
      "Iteration 48, loss = 0.38229749\n",
      "Iteration 49, loss = 0.38189062\n",
      "Iteration 50, loss = 0.38165360\n",
      "Iteration 51, loss = 0.38134230\n",
      "Iteration 52, loss = 0.38112411\n",
      "Iteration 53, loss = 0.38087168\n",
      "Iteration 54, loss = 0.38051023\n",
      "Iteration 55, loss = 0.38029303\n",
      "Iteration 56, loss = 0.38010704\n",
      "Iteration 57, loss = 0.38025077\n",
      "Iteration 58, loss = 0.37969452\n",
      "Iteration 59, loss = 0.37958606\n",
      "Iteration 60, loss = 0.37937637\n",
      "Iteration 61, loss = 0.37933653\n",
      "Iteration 62, loss = 0.37904805\n",
      "Iteration 63, loss = 0.37898092\n",
      "Iteration 64, loss = 0.37879118\n",
      "Iteration 65, loss = 0.37861754\n",
      "Iteration 66, loss = 0.37852689\n",
      "Iteration 67, loss = 0.37842127\n",
      "Iteration 68, loss = 0.37833537\n",
      "Iteration 69, loss = 0.37828527\n",
      "Iteration 70, loss = 0.37826140\n",
      "Iteration 71, loss = 0.37808661\n",
      "Iteration 72, loss = 0.37799867\n",
      "Iteration 73, loss = 0.37784685\n",
      "Iteration 74, loss = 0.37783545\n",
      "Iteration 75, loss = 0.37772873\n",
      "Iteration 76, loss = 0.37759063\n",
      "Iteration 77, loss = 0.37771393\n",
      "Iteration 78, loss = 0.37750678\n",
      "Iteration 79, loss = 0.37755491\n",
      "Iteration 80, loss = 0.37750775\n",
      "Iteration 81, loss = 0.37756435\n",
      "Iteration 82, loss = 0.37726079\n",
      "Iteration 83, loss = 0.37729369\n",
      "Iteration 84, loss = 0.37728180\n",
      "Iteration 85, loss = 0.37719325\n",
      "Iteration 86, loss = 0.37716571\n",
      "Iteration 87, loss = 0.37729019\n",
      "Iteration 88, loss = 0.37710237\n",
      "Iteration 89, loss = 0.37702601\n",
      "Iteration 90, loss = 0.37695806\n",
      "Iteration 91, loss = 0.37684368\n",
      "Iteration 92, loss = 0.37695305\n",
      "Iteration 93, loss = 0.37692174\n",
      "Iteration 94, loss = 0.37674693\n",
      "Iteration 95, loss = 0.37686086\n",
      "Iteration 96, loss = 0.37677293\n",
      "Iteration 97, loss = 0.37684342\n",
      "Iteration 98, loss = 0.37679048\n",
      "Iteration 99, loss = 0.37688432\n",
      "Iteration 100, loss = 0.37680401\n",
      "Iteration 101, loss = 0.37662587\n",
      "Iteration 102, loss = 0.37664231\n",
      "Iteration 103, loss = 0.37724151\n",
      "Iteration 104, loss = 0.37658097\n",
      "Iteration 105, loss = 0.37653200\n",
      "Iteration 106, loss = 0.37641568\n",
      "Iteration 107, loss = 0.37663919\n",
      "Iteration 108, loss = 0.37659403\n",
      "Iteration 109, loss = 0.37656416\n",
      "Iteration 110, loss = 0.37636102\n",
      "Iteration 111, loss = 0.37647009\n",
      "Iteration 112, loss = 0.37644802\n",
      "Iteration 113, loss = 0.37639419\n",
      "Iteration 114, loss = 0.37639010\n",
      "Iteration 115, loss = 0.37636705\n",
      "Iteration 116, loss = 0.37638774\n",
      "Iteration 117, loss = 0.37643378\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.68079110\n",
      "Iteration 2, loss = 0.64886648\n",
      "Iteration 3, loss = 0.62025700\n",
      "Iteration 4, loss = 0.59456385\n",
      "Iteration 5, loss = 0.57153411\n",
      "Iteration 6, loss = 0.55101340\n",
      "Iteration 7, loss = 0.53323600\n",
      "Iteration 8, loss = 0.51752472\n",
      "Iteration 9, loss = 0.50408670\n",
      "Iteration 10, loss = 0.49201328\n",
      "Iteration 11, loss = 0.48148173\n",
      "Iteration 12, loss = 0.47216394\n",
      "Iteration 13, loss = 0.46401532\n",
      "Iteration 14, loss = 0.45668856\n",
      "Iteration 15, loss = 0.45016583\n",
      "Iteration 16, loss = 0.44455048\n",
      "Iteration 17, loss = 0.43945470\n",
      "Iteration 18, loss = 0.43476812\n",
      "Iteration 19, loss = 0.43062741\n",
      "Iteration 20, loss = 0.42685995\n",
      "Iteration 21, loss = 0.42360773\n",
      "Iteration 22, loss = 0.42066468\n",
      "Iteration 23, loss = 0.41776444\n",
      "Iteration 24, loss = 0.41517724\n",
      "Iteration 25, loss = 0.41293290\n",
      "Iteration 26, loss = 0.41090960\n",
      "Iteration 27, loss = 0.40907230\n",
      "Iteration 28, loss = 0.40736176\n",
      "Iteration 29, loss = 0.40565292\n",
      "Iteration 30, loss = 0.40432061\n",
      "Iteration 31, loss = 0.40286849\n",
      "Iteration 32, loss = 0.40179114\n",
      "Iteration 33, loss = 0.40050933\n",
      "Iteration 34, loss = 0.39946918\n",
      "Iteration 35, loss = 0.39856653\n",
      "Iteration 36, loss = 0.39768050\n",
      "Iteration 37, loss = 0.39682209\n",
      "Iteration 38, loss = 0.39608962\n",
      "Iteration 39, loss = 0.39541502\n",
      "Iteration 40, loss = 0.39474470\n",
      "Iteration 41, loss = 0.39432655\n",
      "Iteration 42, loss = 0.39357517\n",
      "Iteration 43, loss = 0.39315242\n",
      "Iteration 44, loss = 0.39292430\n",
      "Iteration 45, loss = 0.39214076\n",
      "Iteration 46, loss = 0.39173812\n",
      "Iteration 47, loss = 0.39139450\n",
      "Iteration 48, loss = 0.39097310\n",
      "Iteration 49, loss = 0.39065984\n",
      "Iteration 50, loss = 0.39034745\n",
      "Iteration 51, loss = 0.39021275\n",
      "Iteration 52, loss = 0.38978118\n",
      "Iteration 53, loss = 0.38954283\n",
      "Iteration 54, loss = 0.38928077\n",
      "Iteration 55, loss = 0.38923245\n",
      "Iteration 56, loss = 0.38874895\n",
      "Iteration 57, loss = 0.38864057\n",
      "Iteration 58, loss = 0.38846868\n",
      "Iteration 59, loss = 0.38829711\n",
      "Iteration 60, loss = 0.38807913\n",
      "Iteration 61, loss = 0.38803962\n",
      "Iteration 62, loss = 0.38782672\n",
      "Iteration 63, loss = 0.38759449\n",
      "Iteration 64, loss = 0.38749273\n",
      "Iteration 65, loss = 0.38735149\n",
      "Iteration 66, loss = 0.38724186\n",
      "Iteration 67, loss = 0.38723244\n",
      "Iteration 68, loss = 0.38700069\n",
      "Iteration 69, loss = 0.38684823\n",
      "Iteration 70, loss = 0.38708626\n",
      "Iteration 71, loss = 0.38674504\n",
      "Iteration 72, loss = 0.38650689\n",
      "Iteration 73, loss = 0.38658982\n",
      "Iteration 74, loss = 0.38651615\n",
      "Iteration 75, loss = 0.38631059\n",
      "Iteration 76, loss = 0.38634127\n",
      "Iteration 77, loss = 0.38661436\n",
      "Iteration 78, loss = 0.38619108\n",
      "Iteration 79, loss = 0.38613281\n",
      "Iteration 80, loss = 0.38614142\n",
      "Iteration 81, loss = 0.38617317\n",
      "Iteration 82, loss = 0.38598187\n",
      "Iteration 83, loss = 0.38585702\n",
      "Iteration 84, loss = 0.38583391\n",
      "Iteration 85, loss = 0.38571772\n",
      "Iteration 86, loss = 0.38572335\n",
      "Iteration 87, loss = 0.38567682\n",
      "Iteration 88, loss = 0.38551981\n",
      "Iteration 89, loss = 0.38567635\n",
      "Iteration 90, loss = 0.38562767\n",
      "Iteration 91, loss = 0.38567658\n",
      "Iteration 92, loss = 0.38545850\n",
      "Iteration 93, loss = 0.38537939\n",
      "Iteration 94, loss = 0.38554306\n",
      "Iteration 95, loss = 0.38542791\n",
      "Iteration 96, loss = 0.38530168\n",
      "Iteration 97, loss = 0.38538690\n",
      "Iteration 98, loss = 0.38540239\n",
      "Iteration 99, loss = 0.38549597\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.68190202\n",
      "Iteration 2, loss = 0.65073910\n",
      "Iteration 3, loss = 0.62273679\n",
      "Iteration 4, loss = 0.59721567\n",
      "Iteration 5, loss = 0.57470598\n",
      "Iteration 6, loss = 0.55489499\n",
      "Iteration 7, loss = 0.53742411\n",
      "Iteration 8, loss = 0.52220870\n",
      "Iteration 9, loss = 0.50896895\n",
      "Iteration 10, loss = 0.49721758\n",
      "Iteration 11, loss = 0.48696661\n",
      "Iteration 12, loss = 0.47775340\n",
      "Iteration 13, loss = 0.46979228\n",
      "Iteration 14, loss = 0.46281380\n",
      "Iteration 15, loss = 0.45650198\n",
      "Iteration 16, loss = 0.45083504\n",
      "Iteration 17, loss = 0.44587572\n",
      "Iteration 18, loss = 0.44136086\n",
      "Iteration 19, loss = 0.43728018\n",
      "Iteration 20, loss = 0.43373995\n",
      "Iteration 21, loss = 0.43039373\n",
      "Iteration 22, loss = 0.42755351\n",
      "Iteration 23, loss = 0.42486642\n",
      "Iteration 24, loss = 0.42252297\n",
      "Iteration 25, loss = 0.42019045\n",
      "Iteration 26, loss = 0.41827598\n",
      "Iteration 27, loss = 0.41645715\n",
      "Iteration 28, loss = 0.41478036\n",
      "Iteration 29, loss = 0.41325958\n",
      "Iteration 30, loss = 0.41180866\n",
      "Iteration 31, loss = 0.41073431\n",
      "Iteration 32, loss = 0.40939104\n",
      "Iteration 33, loss = 0.40831178\n",
      "Iteration 34, loss = 0.40732349\n",
      "Iteration 35, loss = 0.40645662\n",
      "Iteration 36, loss = 0.40562145\n",
      "Iteration 37, loss = 0.40510223\n",
      "Iteration 38, loss = 0.40409920\n",
      "Iteration 39, loss = 0.40366960\n",
      "Iteration 40, loss = 0.40303133\n",
      "Iteration 41, loss = 0.40235032\n",
      "Iteration 42, loss = 0.40178923\n",
      "Iteration 43, loss = 0.40121581\n",
      "Iteration 44, loss = 0.40084987\n",
      "Iteration 45, loss = 0.40060551\n",
      "Iteration 46, loss = 0.40000569\n",
      "Iteration 47, loss = 0.39964032\n",
      "Iteration 48, loss = 0.39927547\n",
      "Iteration 49, loss = 0.39899084\n",
      "Iteration 50, loss = 0.39864126\n",
      "Iteration 51, loss = 0.39839355\n",
      "Iteration 52, loss = 0.39829484\n",
      "Iteration 53, loss = 0.39791683\n",
      "Iteration 54, loss = 0.39758068\n",
      "Iteration 55, loss = 0.39742602\n",
      "Iteration 56, loss = 0.39718465\n",
      "Iteration 57, loss = 0.39697820\n",
      "Iteration 58, loss = 0.39690362\n",
      "Iteration 59, loss = 0.39659051\n",
      "Iteration 60, loss = 0.39647923\n",
      "Iteration 61, loss = 0.39641210\n",
      "Iteration 62, loss = 0.39622671\n",
      "Iteration 63, loss = 0.39607690\n",
      "Iteration 64, loss = 0.39594985\n",
      "Iteration 65, loss = 0.39572059\n",
      "Iteration 66, loss = 0.39565845\n",
      "Iteration 67, loss = 0.39557552\n",
      "Iteration 68, loss = 0.39545524\n",
      "Iteration 69, loss = 0.39535731\n",
      "Iteration 70, loss = 0.39520980\n",
      "Iteration 71, loss = 0.39527767\n",
      "Iteration 72, loss = 0.39509732\n",
      "Iteration 73, loss = 0.39488286\n",
      "Iteration 74, loss = 0.39485619\n",
      "Iteration 75, loss = 0.39485989\n",
      "Iteration 76, loss = 0.39472211\n",
      "Iteration 77, loss = 0.39464351\n",
      "Iteration 78, loss = 0.39463017\n",
      "Iteration 79, loss = 0.39466736\n",
      "Iteration 80, loss = 0.39443944\n",
      "Iteration 81, loss = 0.39436435\n",
      "Iteration 82, loss = 0.39453444\n",
      "Iteration 83, loss = 0.39439529\n",
      "Iteration 84, loss = 0.39415814\n",
      "Iteration 85, loss = 0.39415935\n",
      "Iteration 86, loss = 0.39415090\n",
      "Iteration 87, loss = 0.39404338\n",
      "Iteration 88, loss = 0.39395586\n",
      "Iteration 89, loss = 0.39411265\n",
      "Iteration 90, loss = 0.39391704\n",
      "Iteration 91, loss = 0.39387511\n",
      "Iteration 92, loss = 0.39394940\n",
      "Iteration 93, loss = 0.39386962\n",
      "Iteration 94, loss = 0.39367868\n",
      "Iteration 95, loss = 0.39383694\n",
      "Iteration 96, loss = 0.39378562\n",
      "Iteration 97, loss = 0.39356794\n",
      "Iteration 98, loss = 0.39370987\n",
      "Iteration 99, loss = 0.39357397\n",
      "Iteration 100, loss = 0.39353293\n",
      "Iteration 101, loss = 0.39343560\n",
      "Iteration 102, loss = 0.39354556\n",
      "Iteration 103, loss = 0.39329954\n",
      "Iteration 104, loss = 0.39343984\n",
      "Iteration 105, loss = 0.39334460\n",
      "Iteration 106, loss = 0.39344437\n",
      "Iteration 107, loss = 0.39324978\n",
      "Iteration 108, loss = 0.39329903\n",
      "Iteration 109, loss = 0.39330371\n",
      "Iteration 110, loss = 0.39325996\n",
      "Iteration 111, loss = 0.39322771\n",
      "Iteration 112, loss = 0.39316355\n",
      "Iteration 113, loss = 0.39323610\n",
      "Iteration 114, loss = 0.39307995\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.68374381\n",
      "Iteration 2, loss = 0.67236952\n",
      "Iteration 3, loss = 0.65989110\n",
      "Iteration 4, loss = 0.64246563\n",
      "Iteration 5, loss = 0.61880374\n",
      "Iteration 6, loss = 0.58874209\n",
      "Iteration 7, loss = 0.55361965\n",
      "Iteration 8, loss = 0.51603171\n",
      "Iteration 9, loss = 0.48070897\n",
      "Iteration 10, loss = 0.45154861\n",
      "Iteration 11, loss = 0.42944718\n",
      "Iteration 12, loss = 0.41362217\n",
      "Iteration 13, loss = 0.40275416\n",
      "Iteration 14, loss = 0.39506748\n",
      "Iteration 15, loss = 0.39009396\n",
      "Iteration 16, loss = 0.38658073\n",
      "Iteration 17, loss = 0.38411877\n",
      "Iteration 18, loss = 0.38256815\n",
      "Iteration 19, loss = 0.38134026\n",
      "Iteration 20, loss = 0.38042423\n",
      "Iteration 21, loss = 0.37966319\n",
      "Iteration 22, loss = 0.37912234\n",
      "Iteration 23, loss = 0.37893206\n",
      "Iteration 24, loss = 0.37875692\n",
      "Iteration 25, loss = 0.37825042\n",
      "Iteration 26, loss = 0.37842521\n",
      "Iteration 27, loss = 0.37829168\n",
      "Iteration 28, loss = 0.37798201\n",
      "Iteration 29, loss = 0.37795955\n",
      "Iteration 30, loss = 0.37761772\n",
      "Iteration 31, loss = 0.37763038\n",
      "Iteration 32, loss = 0.37776016\n",
      "Iteration 33, loss = 0.37765567\n",
      "Iteration 34, loss = 0.37742118\n",
      "Iteration 35, loss = 0.37746728\n",
      "Iteration 36, loss = 0.37736495\n",
      "Iteration 37, loss = 0.37725294\n",
      "Iteration 38, loss = 0.37718801\n",
      "Iteration 39, loss = 0.37704600\n",
      "Iteration 40, loss = 0.37713757\n",
      "Iteration 41, loss = 0.37707643\n",
      "Iteration 42, loss = 0.37701480\n",
      "Iteration 43, loss = 0.37685119\n",
      "Iteration 44, loss = 0.37686711\n",
      "Iteration 45, loss = 0.37686931\n",
      "Iteration 46, loss = 0.37673952\n",
      "Iteration 47, loss = 0.37684491\n",
      "Iteration 48, loss = 0.37664472\n",
      "Iteration 49, loss = 0.37672091\n",
      "Iteration 50, loss = 0.37681716\n",
      "Iteration 51, loss = 0.37650073\n",
      "Iteration 52, loss = 0.37693498\n",
      "Iteration 53, loss = 0.37679167\n",
      "Iteration 54, loss = 0.37649458\n",
      "Iteration 55, loss = 0.37628425\n",
      "Iteration 56, loss = 0.37652594\n",
      "Iteration 57, loss = 0.37648789\n",
      "Iteration 58, loss = 0.37640250\n",
      "Iteration 59, loss = 0.37638008\n",
      "Iteration 60, loss = 0.37632194\n",
      "Iteration 61, loss = 0.37628092\n",
      "Iteration 62, loss = 0.37634906\n",
      "Iteration 63, loss = 0.37622816\n",
      "Iteration 64, loss = 0.37633617\n",
      "Iteration 65, loss = 0.37618416\n",
      "Iteration 66, loss = 0.37625031\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.68354580\n",
      "Iteration 2, loss = 0.67281980\n",
      "Iteration 3, loss = 0.66058127\n",
      "Iteration 4, loss = 0.64358787\n",
      "Iteration 5, loss = 0.62063214\n",
      "Iteration 6, loss = 0.59144478\n",
      "Iteration 7, loss = 0.55735695\n",
      "Iteration 8, loss = 0.52095308\n",
      "Iteration 9, loss = 0.48678397\n",
      "Iteration 10, loss = 0.45852056\n",
      "Iteration 11, loss = 0.43702895\n",
      "Iteration 12, loss = 0.42150828\n",
      "Iteration 13, loss = 0.41092299\n",
      "Iteration 14, loss = 0.40348728\n",
      "Iteration 15, loss = 0.39847839\n",
      "Iteration 16, loss = 0.39498930\n",
      "Iteration 17, loss = 0.39262491\n",
      "Iteration 18, loss = 0.39115628\n",
      "Iteration 19, loss = 0.38974079\n",
      "Iteration 20, loss = 0.38920016\n",
      "Iteration 21, loss = 0.38852318\n",
      "Iteration 22, loss = 0.38775966\n",
      "Iteration 23, loss = 0.38761939\n",
      "Iteration 24, loss = 0.38746404\n",
      "Iteration 25, loss = 0.38703791\n",
      "Iteration 26, loss = 0.38719534\n",
      "Iteration 27, loss = 0.38673042\n",
      "Iteration 28, loss = 0.38640032\n",
      "Iteration 29, loss = 0.38660679\n",
      "Iteration 30, loss = 0.38632273\n",
      "Iteration 31, loss = 0.38628595\n",
      "Iteration 32, loss = 0.38625678\n",
      "Iteration 33, loss = 0.38616198\n",
      "Iteration 34, loss = 0.38603134\n",
      "Iteration 35, loss = 0.38606099\n",
      "Iteration 36, loss = 0.38590401\n",
      "Iteration 37, loss = 0.38596625\n",
      "Iteration 38, loss = 0.38579584\n",
      "Iteration 39, loss = 0.38566373\n",
      "Iteration 40, loss = 0.38558932\n",
      "Iteration 41, loss = 0.38576039\n",
      "Iteration 42, loss = 0.38580823\n",
      "Iteration 43, loss = 0.38581208\n",
      "Iteration 44, loss = 0.38539301\n",
      "Iteration 45, loss = 0.38533477\n",
      "Iteration 46, loss = 0.38523721\n",
      "Iteration 47, loss = 0.38551614\n",
      "Iteration 48, loss = 0.38508647\n",
      "Iteration 49, loss = 0.38525389\n",
      "Iteration 50, loss = 0.38522761\n",
      "Iteration 51, loss = 0.38520626\n",
      "Iteration 52, loss = 0.38500370\n",
      "Iteration 53, loss = 0.38486867\n",
      "Iteration 54, loss = 0.38489347\n",
      "Iteration 55, loss = 0.38500853\n",
      "Iteration 56, loss = 0.38482138\n",
      "Iteration 57, loss = 0.38466402\n",
      "Iteration 58, loss = 0.38509623\n",
      "Iteration 59, loss = 0.38477777\n",
      "Iteration 60, loss = 0.38461574\n",
      "Iteration 61, loss = 0.38457473\n",
      "Iteration 62, loss = 0.38473186\n",
      "Iteration 63, loss = 0.38454929\n",
      "Iteration 64, loss = 0.38454220\n",
      "Iteration 65, loss = 0.38456693\n",
      "Iteration 66, loss = 0.38475240\n",
      "Iteration 67, loss = 0.38461423\n",
      "Iteration 68, loss = 0.38470010\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.68344007\n",
      "Iteration 2, loss = 0.67320693\n",
      "Iteration 3, loss = 0.66138743\n",
      "Iteration 4, loss = 0.64495215\n",
      "Iteration 5, loss = 0.62293879\n",
      "Iteration 6, loss = 0.59506350\n",
      "Iteration 7, loss = 0.56201988\n",
      "Iteration 8, loss = 0.52708918\n",
      "Iteration 9, loss = 0.49429322\n",
      "Iteration 10, loss = 0.46651393\n",
      "Iteration 11, loss = 0.44556805\n",
      "Iteration 12, loss = 0.43009040\n",
      "Iteration 13, loss = 0.41958598\n",
      "Iteration 14, loss = 0.41224805\n",
      "Iteration 15, loss = 0.40742723\n",
      "Iteration 16, loss = 0.40403876\n",
      "Iteration 17, loss = 0.40171461\n",
      "Iteration 18, loss = 0.40026902\n",
      "Iteration 19, loss = 0.39883449\n",
      "Iteration 20, loss = 0.39811268\n",
      "Iteration 21, loss = 0.39750643\n",
      "Iteration 22, loss = 0.39720927\n",
      "Iteration 23, loss = 0.39654957\n",
      "Iteration 24, loss = 0.39673612\n",
      "Iteration 25, loss = 0.39642741\n",
      "Iteration 26, loss = 0.39584416\n",
      "Iteration 27, loss = 0.39570970\n",
      "Iteration 28, loss = 0.39534710\n",
      "Iteration 29, loss = 0.39531008\n",
      "Iteration 30, loss = 0.39536366\n",
      "Iteration 31, loss = 0.39506653\n",
      "Iteration 32, loss = 0.39496106\n",
      "Iteration 33, loss = 0.39480032\n",
      "Iteration 34, loss = 0.39462333\n",
      "Iteration 35, loss = 0.39434986\n",
      "Iteration 36, loss = 0.39478434\n",
      "Iteration 37, loss = 0.39430130\n",
      "Iteration 38, loss = 0.39427744\n",
      "Iteration 39, loss = 0.39416195\n",
      "Iteration 40, loss = 0.39419214\n",
      "Iteration 41, loss = 0.39389489\n",
      "Iteration 42, loss = 0.39402255\n",
      "Iteration 43, loss = 0.39397108\n",
      "Iteration 44, loss = 0.39362227\n",
      "Iteration 45, loss = 0.39396797\n",
      "Iteration 46, loss = 0.39371986\n",
      "Iteration 47, loss = 0.39351770\n",
      "Iteration 48, loss = 0.39352837\n",
      "Iteration 49, loss = 0.39349637\n",
      "Iteration 50, loss = 0.39356885\n",
      "Iteration 51, loss = 0.39382118\n",
      "Iteration 52, loss = 0.39321813\n",
      "Iteration 53, loss = 0.39321753\n",
      "Iteration 54, loss = 0.39331712\n",
      "Iteration 55, loss = 0.39307346\n",
      "Iteration 56, loss = 0.39308219\n",
      "Iteration 57, loss = 0.39314175\n",
      "Iteration 58, loss = 0.39287237\n",
      "Iteration 59, loss = 0.39307846\n",
      "Iteration 60, loss = 0.39329739\n",
      "Iteration 61, loss = 0.39313694\n",
      "Iteration 62, loss = 0.39302863\n",
      "Iteration 63, loss = 0.39291051\n",
      "Iteration 64, loss = 0.39276848\n",
      "Iteration 65, loss = 0.39276595\n",
      "Iteration 66, loss = 0.39284310\n",
      "Iteration 67, loss = 0.39291991\n",
      "Iteration 68, loss = 0.39282781\n",
      "Iteration 69, loss = 0.39277416\n",
      "Iteration 70, loss = 0.39266875\n",
      "Iteration 71, loss = 0.39276600\n",
      "Iteration 72, loss = 0.39272651\n",
      "Iteration 73, loss = 0.39284481\n",
      "Iteration 74, loss = 0.39259301\n",
      "Iteration 75, loss = 0.39285731\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.67461439\n",
      "Iteration 2, loss = 0.62299109\n",
      "Iteration 3, loss = 0.53867980\n",
      "Iteration 4, loss = 0.46053729\n",
      "Iteration 5, loss = 0.41666552\n",
      "Iteration 6, loss = 0.39608351\n",
      "Iteration 7, loss = 0.38741513\n",
      "Iteration 8, loss = 0.38286564\n",
      "Iteration 9, loss = 0.38100898\n",
      "Iteration 10, loss = 0.38011791\n",
      "Iteration 11, loss = 0.37908973\n",
      "Iteration 12, loss = 0.37885197\n",
      "Iteration 13, loss = 0.37880766\n",
      "Iteration 14, loss = 0.37831516\n",
      "Iteration 15, loss = 0.37834360\n",
      "Iteration 16, loss = 0.37794561\n",
      "Iteration 17, loss = 0.37771661\n",
      "Iteration 18, loss = 0.37762639\n",
      "Iteration 19, loss = 0.37759029\n",
      "Iteration 20, loss = 0.37740081\n",
      "Iteration 21, loss = 0.37702996\n",
      "Iteration 22, loss = 0.37676693\n",
      "Iteration 23, loss = 0.37684211\n",
      "Iteration 24, loss = 0.37717975\n",
      "Iteration 25, loss = 0.37653519\n",
      "Iteration 26, loss = 0.37706408\n",
      "Iteration 27, loss = 0.37701348\n",
      "Iteration 28, loss = 0.37656949\n",
      "Iteration 29, loss = 0.37664346\n",
      "Iteration 30, loss = 0.37633167\n",
      "Iteration 31, loss = 0.37636020\n",
      "Iteration 32, loss = 0.37679071\n",
      "Iteration 33, loss = 0.37662200\n",
      "Iteration 34, loss = 0.37643427\n",
      "Iteration 35, loss = 0.37649346\n",
      "Iteration 36, loss = 0.37636452\n",
      "Iteration 37, loss = 0.37655079\n",
      "Iteration 38, loss = 0.37638678\n",
      "Iteration 39, loss = 0.37613275\n",
      "Iteration 40, loss = 0.37638931\n",
      "Iteration 41, loss = 0.37625024\n",
      "Iteration 42, loss = 0.37638826\n",
      "Iteration 43, loss = 0.37605306\n",
      "Iteration 44, loss = 0.37625534\n",
      "Iteration 45, loss = 0.37629131\n",
      "Iteration 46, loss = 0.37603773\n",
      "Iteration 47, loss = 0.37625689\n",
      "Iteration 48, loss = 0.37605803\n",
      "Iteration 49, loss = 0.37615654\n",
      "Iteration 50, loss = 0.37636326\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.67363922\n",
      "Iteration 2, loss = 0.62348864\n",
      "Iteration 3, loss = 0.54240440\n",
      "Iteration 4, loss = 0.46641557\n",
      "Iteration 5, loss = 0.42286103\n",
      "Iteration 6, loss = 0.40335725\n",
      "Iteration 7, loss = 0.39508085\n",
      "Iteration 8, loss = 0.39135013\n",
      "Iteration 9, loss = 0.39000759\n",
      "Iteration 10, loss = 0.38862010\n",
      "Iteration 11, loss = 0.38798693\n",
      "Iteration 12, loss = 0.38733211\n",
      "Iteration 13, loss = 0.38749169\n",
      "Iteration 14, loss = 0.38721482\n",
      "Iteration 15, loss = 0.38696707\n",
      "Iteration 16, loss = 0.38638286\n",
      "Iteration 17, loss = 0.38638641\n",
      "Iteration 18, loss = 0.38651448\n",
      "Iteration 19, loss = 0.38588649\n",
      "Iteration 20, loss = 0.38609136\n",
      "Iteration 21, loss = 0.38600189\n",
      "Iteration 22, loss = 0.38536904\n",
      "Iteration 23, loss = 0.38549973\n",
      "Iteration 24, loss = 0.38575681\n",
      "Iteration 25, loss = 0.38527480\n",
      "Iteration 26, loss = 0.38586672\n",
      "Iteration 27, loss = 0.38513569\n",
      "Iteration 28, loss = 0.38482587\n",
      "Iteration 29, loss = 0.38515757\n",
      "Iteration 30, loss = 0.38494325\n",
      "Iteration 31, loss = 0.38505823\n",
      "Iteration 32, loss = 0.38505958\n",
      "Iteration 33, loss = 0.38503173\n",
      "Iteration 34, loss = 0.38468561\n",
      "Iteration 35, loss = 0.38514214\n",
      "Iteration 36, loss = 0.38492467\n",
      "Iteration 37, loss = 0.38504382\n",
      "Iteration 38, loss = 0.38498079\n",
      "Iteration 39, loss = 0.38483832\n",
      "Iteration 40, loss = 0.38472418\n",
      "Iteration 41, loss = 0.38502665\n",
      "Iteration 42, loss = 0.38546168\n",
      "Iteration 43, loss = 0.38562592\n",
      "Iteration 44, loss = 0.38460576\n",
      "Iteration 45, loss = 0.38470272\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.67232281\n",
      "Iteration 2, loss = 0.61816277\n",
      "Iteration 3, loss = 0.53640017\n",
      "Iteration 4, loss = 0.46555845\n",
      "Iteration 5, loss = 0.42711531\n",
      "Iteration 6, loss = 0.41028686\n",
      "Iteration 7, loss = 0.40346465\n",
      "Iteration 8, loss = 0.40068610\n",
      "Iteration 9, loss = 0.39835887\n",
      "Iteration 10, loss = 0.39750271\n",
      "Iteration 11, loss = 0.39762884\n",
      "Iteration 12, loss = 0.39642168\n",
      "Iteration 13, loss = 0.39616892\n",
      "Iteration 14, loss = 0.39560457\n",
      "Iteration 15, loss = 0.39540975\n",
      "Iteration 16, loss = 0.39518803\n",
      "Iteration 17, loss = 0.39500832\n",
      "Iteration 18, loss = 0.39493691\n",
      "Iteration 19, loss = 0.39449022\n",
      "Iteration 20, loss = 0.39461537\n",
      "Iteration 21, loss = 0.39416775\n",
      "Iteration 22, loss = 0.39433116\n",
      "Iteration 23, loss = 0.39382882\n",
      "Iteration 24, loss = 0.39438072\n",
      "Iteration 25, loss = 0.39418079\n",
      "Iteration 26, loss = 0.39349038\n",
      "Iteration 27, loss = 0.39352573\n",
      "Iteration 28, loss = 0.39310137\n",
      "Iteration 29, loss = 0.39318885\n",
      "Iteration 30, loss = 0.39348470\n",
      "Iteration 31, loss = 0.39316317\n",
      "Iteration 32, loss = 0.39312409\n",
      "Iteration 33, loss = 0.39293541\n",
      "Iteration 34, loss = 0.39291949\n",
      "Iteration 35, loss = 0.39255033\n",
      "Iteration 36, loss = 0.39349284\n",
      "Iteration 37, loss = 0.39270557\n",
      "Iteration 38, loss = 0.39279196\n",
      "Iteration 39, loss = 0.39266342\n",
      "Iteration 40, loss = 0.39274642\n",
      "Iteration 41, loss = 0.39248916\n",
      "Iteration 42, loss = 0.39275854\n",
      "Iteration 43, loss = 0.39279432\n",
      "Iteration 44, loss = 0.39240315\n",
      "Iteration 45, loss = 0.39302218\n",
      "Iteration 46, loss = 0.39275661\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.80704420\n",
      "Iteration 2, loss = 0.68107882\n",
      "Iteration 3, loss = 0.65701261\n",
      "Iteration 4, loss = 0.62522015\n",
      "Iteration 5, loss = 0.58474033\n",
      "Iteration 6, loss = 0.53951715\n",
      "Iteration 7, loss = 0.49654684\n",
      "Iteration 8, loss = 0.46093430\n",
      "Iteration 9, loss = 0.43566335\n",
      "Iteration 10, loss = 0.41781069\n",
      "Iteration 11, loss = 0.40572996\n",
      "Iteration 12, loss = 0.39784210\n",
      "Iteration 13, loss = 0.39258139\n",
      "Iteration 14, loss = 0.38876262\n",
      "Iteration 15, loss = 0.38676632\n",
      "Iteration 16, loss = 0.38469738\n",
      "Iteration 17, loss = 0.38392400\n",
      "Iteration 18, loss = 0.38271871\n",
      "Iteration 19, loss = 0.38200194\n",
      "Iteration 20, loss = 0.38180668\n",
      "Iteration 21, loss = 0.38105563\n",
      "Iteration 22, loss = 0.38071287\n",
      "Iteration 23, loss = 0.38067433\n",
      "Iteration 24, loss = 0.38049050\n",
      "Iteration 25, loss = 0.37977900\n",
      "Iteration 26, loss = 0.37972264\n",
      "Iteration 27, loss = 0.37985267\n",
      "Iteration 28, loss = 0.37942995\n",
      "Iteration 29, loss = 0.37921610\n",
      "Iteration 30, loss = 0.37896171\n",
      "Iteration 31, loss = 0.37902368\n",
      "Iteration 32, loss = 0.37857039\n",
      "Iteration 33, loss = 0.37877052\n",
      "Iteration 34, loss = 0.37853445\n",
      "Iteration 35, loss = 0.37841259\n",
      "Iteration 36, loss = 0.37834876\n",
      "Iteration 37, loss = 0.37811439\n",
      "Iteration 38, loss = 0.37859561\n",
      "Iteration 39, loss = 0.37789568\n",
      "Iteration 40, loss = 0.37825226\n",
      "Iteration 41, loss = 0.37756289\n",
      "Iteration 42, loss = 0.37761956\n",
      "Iteration 43, loss = 0.37758576\n",
      "Iteration 44, loss = 0.37762242\n",
      "Iteration 45, loss = 0.37755023\n",
      "Iteration 46, loss = 0.37708477\n",
      "Iteration 47, loss = 0.37786169\n",
      "Iteration 48, loss = 0.37714768\n",
      "Iteration 49, loss = 0.37734322\n",
      "Iteration 50, loss = 0.37700782\n",
      "Iteration 51, loss = 0.37688490\n",
      "Iteration 52, loss = 0.37728505\n",
      "Iteration 53, loss = 0.37688546\n",
      "Iteration 54, loss = 0.37688417\n",
      "Iteration 55, loss = 0.37692448\n",
      "Iteration 56, loss = 0.37700810\n",
      "Iteration 57, loss = 0.37684312\n",
      "Iteration 58, loss = 0.37687178\n",
      "Iteration 59, loss = 0.37634536\n",
      "Iteration 60, loss = 0.37669727\n",
      "Iteration 61, loss = 0.37649042\n",
      "Iteration 62, loss = 0.37643435\n",
      "Iteration 63, loss = 0.37663669\n",
      "Iteration 64, loss = 0.37649638\n",
      "Iteration 65, loss = 0.37660071\n",
      "Iteration 66, loss = 0.37657094\n",
      "Iteration 67, loss = 0.37666972\n",
      "Iteration 68, loss = 0.37651282\n",
      "Iteration 69, loss = 0.37629121\n",
      "Iteration 70, loss = 0.37627476\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.80986559\n",
      "Iteration 2, loss = 0.68100364\n",
      "Iteration 3, loss = 0.65691257\n",
      "Iteration 4, loss = 0.62561764\n",
      "Iteration 5, loss = 0.58587582\n",
      "Iteration 6, loss = 0.54193963\n",
      "Iteration 7, loss = 0.50029195\n",
      "Iteration 8, loss = 0.46653570\n",
      "Iteration 9, loss = 0.44196171\n",
      "Iteration 10, loss = 0.42498093\n",
      "Iteration 11, loss = 0.41346604\n",
      "Iteration 12, loss = 0.40587077\n",
      "Iteration 13, loss = 0.40118205\n",
      "Iteration 14, loss = 0.39757368\n",
      "Iteration 15, loss = 0.39522469\n",
      "Iteration 16, loss = 0.39354922\n",
      "Iteration 17, loss = 0.39236080\n",
      "Iteration 18, loss = 0.39149756\n",
      "Iteration 19, loss = 0.39078726\n",
      "Iteration 20, loss = 0.39035949\n",
      "Iteration 21, loss = 0.38989551\n",
      "Iteration 22, loss = 0.38947103\n",
      "Iteration 23, loss = 0.38943118\n",
      "Iteration 24, loss = 0.38917815\n",
      "Iteration 25, loss = 0.38884426\n",
      "Iteration 26, loss = 0.38842732\n",
      "Iteration 27, loss = 0.38849474\n",
      "Iteration 28, loss = 0.38801850\n",
      "Iteration 29, loss = 0.38774161\n",
      "Iteration 30, loss = 0.38767880\n",
      "Iteration 31, loss = 0.38748650\n",
      "Iteration 32, loss = 0.38768629\n",
      "Iteration 33, loss = 0.38758119\n",
      "Iteration 34, loss = 0.38740772\n",
      "Iteration 35, loss = 0.38709139\n",
      "Iteration 36, loss = 0.38714194\n",
      "Iteration 37, loss = 0.38668754\n",
      "Iteration 38, loss = 0.38656943\n",
      "Iteration 39, loss = 0.38650008\n",
      "Iteration 40, loss = 0.38658340\n",
      "Iteration 41, loss = 0.38646639\n",
      "Iteration 42, loss = 0.38635203\n",
      "Iteration 43, loss = 0.38576063\n",
      "Iteration 44, loss = 0.38613731\n",
      "Iteration 45, loss = 0.38594377\n",
      "Iteration 46, loss = 0.38581996\n",
      "Iteration 47, loss = 0.38572640\n",
      "Iteration 48, loss = 0.38546740\n",
      "Iteration 49, loss = 0.38559944\n",
      "Iteration 50, loss = 0.38543775\n",
      "Iteration 51, loss = 0.38535955\n",
      "Iteration 52, loss = 0.38558229\n",
      "Iteration 53, loss = 0.38513805\n",
      "Iteration 54, loss = 0.38527540\n",
      "Iteration 55, loss = 0.38544745\n",
      "Iteration 56, loss = 0.38512324\n",
      "Iteration 57, loss = 0.38525620\n",
      "Iteration 58, loss = 0.38575458\n",
      "Iteration 59, loss = 0.38516602\n",
      "Iteration 60, loss = 0.38483176\n",
      "Iteration 61, loss = 0.38485310\n",
      "Iteration 62, loss = 0.38538254\n",
      "Iteration 63, loss = 0.38490114\n",
      "Iteration 64, loss = 0.38500629\n",
      "Iteration 65, loss = 0.38482681\n",
      "Iteration 66, loss = 0.38495918\n",
      "Iteration 67, loss = 0.38491754\n",
      "Iteration 68, loss = 0.38456884\n",
      "Iteration 69, loss = 0.38467027\n",
      "Iteration 70, loss = 0.38463862\n",
      "Iteration 71, loss = 0.38468090\n",
      "Iteration 72, loss = 0.38484599\n",
      "Iteration 73, loss = 0.38490863\n",
      "Iteration 74, loss = 0.38454856\n",
      "Iteration 75, loss = 0.38448292\n",
      "Iteration 76, loss = 0.38459904\n",
      "Iteration 77, loss = 0.38454882\n",
      "Iteration 78, loss = 0.38455813\n",
      "Iteration 79, loss = 0.38462854\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.80897827\n",
      "Iteration 2, loss = 0.68119360\n",
      "Iteration 3, loss = 0.65776397\n",
      "Iteration 4, loss = 0.62731306\n",
      "Iteration 5, loss = 0.58872527\n",
      "Iteration 6, loss = 0.54623306\n",
      "Iteration 7, loss = 0.50605251\n",
      "Iteration 8, loss = 0.47341743\n",
      "Iteration 9, loss = 0.44948765\n",
      "Iteration 10, loss = 0.43310892\n",
      "Iteration 11, loss = 0.42219521\n",
      "Iteration 12, loss = 0.41480990\n",
      "Iteration 13, loss = 0.41003085\n",
      "Iteration 14, loss = 0.40663843\n",
      "Iteration 15, loss = 0.40469296\n",
      "Iteration 16, loss = 0.40321527\n",
      "Iteration 17, loss = 0.40193043\n",
      "Iteration 18, loss = 0.40092194\n",
      "Iteration 19, loss = 0.40010410\n",
      "Iteration 20, loss = 0.39983900\n",
      "Iteration 21, loss = 0.39925853\n",
      "Iteration 22, loss = 0.39880033\n",
      "Iteration 23, loss = 0.39854363\n",
      "Iteration 24, loss = 0.39819112\n",
      "Iteration 25, loss = 0.39799247\n",
      "Iteration 26, loss = 0.39755187\n",
      "Iteration 27, loss = 0.39757782\n",
      "Iteration 28, loss = 0.39741068\n",
      "Iteration 29, loss = 0.39700164\n",
      "Iteration 30, loss = 0.39665926\n",
      "Iteration 31, loss = 0.39673683\n",
      "Iteration 32, loss = 0.39673015\n",
      "Iteration 33, loss = 0.39596533\n",
      "Iteration 34, loss = 0.39587924\n",
      "Iteration 35, loss = 0.39568578\n",
      "Iteration 36, loss = 0.39561830\n",
      "Iteration 37, loss = 0.39553817\n",
      "Iteration 38, loss = 0.39569770\n",
      "Iteration 39, loss = 0.39518043\n",
      "Iteration 40, loss = 0.39512723\n",
      "Iteration 41, loss = 0.39487376\n",
      "Iteration 42, loss = 0.39470558\n",
      "Iteration 43, loss = 0.39454013\n",
      "Iteration 44, loss = 0.39450779\n",
      "Iteration 45, loss = 0.39425406\n",
      "Iteration 46, loss = 0.39418354\n",
      "Iteration 47, loss = 0.39413121\n",
      "Iteration 48, loss = 0.39399968\n",
      "Iteration 49, loss = 0.39427468\n",
      "Iteration 50, loss = 0.39433146\n",
      "Iteration 51, loss = 0.39408948\n",
      "Iteration 52, loss = 0.39378507\n",
      "Iteration 53, loss = 0.39353630\n",
      "Iteration 54, loss = 0.39367937\n",
      "Iteration 55, loss = 0.39385397\n",
      "Iteration 56, loss = 0.39349548\n",
      "Iteration 57, loss = 0.39363432\n",
      "Iteration 58, loss = 0.39358821\n",
      "Iteration 59, loss = 0.39346745\n",
      "Iteration 60, loss = 0.39319410\n",
      "Iteration 61, loss = 0.39343946\n",
      "Iteration 62, loss = 0.39338094\n",
      "Iteration 63, loss = 0.39333778\n",
      "Iteration 64, loss = 0.39338281\n",
      "Iteration 65, loss = 0.39300581\n",
      "Iteration 66, loss = 0.39319220\n",
      "Iteration 67, loss = 0.39326836\n",
      "Iteration 68, loss = 0.39291449\n",
      "Iteration 69, loss = 0.39316462\n",
      "Iteration 70, loss = 0.39293952\n",
      "Iteration 71, loss = 0.39272033\n",
      "Iteration 72, loss = 0.39285192\n",
      "Iteration 73, loss = 0.39282408\n",
      "Iteration 74, loss = 0.39298768\n",
      "Iteration 75, loss = 0.39298645\n",
      "Iteration 76, loss = 0.39267813\n",
      "Iteration 77, loss = 0.39291271\n",
      "Iteration 78, loss = 0.39309410\n",
      "Iteration 79, loss = 0.39281902\n",
      "Iteration 80, loss = 0.39252582\n",
      "Iteration 81, loss = 0.39323244\n",
      "Iteration 82, loss = 0.39251668\n",
      "Iteration 83, loss = 0.39252728\n",
      "Iteration 84, loss = 0.39291748\n",
      "Iteration 85, loss = 0.39259984\n",
      "Iteration 86, loss = 0.39266628\n",
      "Iteration 87, loss = 0.39242148\n",
      "Iteration 88, loss = 0.39248366\n",
      "Iteration 89, loss = 0.39265563\n",
      "Iteration 90, loss = 0.39252505\n",
      "Iteration 91, loss = 0.39259517\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.87064442\n",
      "Iteration 2, loss = 0.72546811\n",
      "Iteration 3, loss = 0.68293512\n",
      "Iteration 4, loss = 0.60712059\n",
      "Iteration 5, loss = 0.49979063\n",
      "Iteration 6, loss = 0.42752819\n",
      "Iteration 7, loss = 0.39817507\n",
      "Iteration 8, loss = 0.38689825\n",
      "Iteration 9, loss = 0.38299695\n",
      "Iteration 10, loss = 0.38108003\n",
      "Iteration 11, loss = 0.38036067\n",
      "Iteration 12, loss = 0.37966214\n",
      "Iteration 13, loss = 0.37899567\n",
      "Iteration 14, loss = 0.37886502\n",
      "Iteration 15, loss = 0.37885265\n",
      "Iteration 16, loss = 0.37758562\n",
      "Iteration 17, loss = 0.37825807\n",
      "Iteration 18, loss = 0.37823993\n",
      "Iteration 19, loss = 0.37752026\n",
      "Iteration 20, loss = 0.37804671\n",
      "Iteration 21, loss = 0.37706985\n",
      "Iteration 22, loss = 0.37709875\n",
      "Iteration 23, loss = 0.37750471\n",
      "Iteration 24, loss = 0.37776984\n",
      "Iteration 25, loss = 0.37659612\n",
      "Iteration 26, loss = 0.37652175\n",
      "Iteration 27, loss = 0.37706202\n",
      "Iteration 28, loss = 0.37673164\n",
      "Iteration 29, loss = 0.37669378\n",
      "Iteration 30, loss = 0.37672249\n",
      "Iteration 31, loss = 0.37706633\n",
      "Iteration 32, loss = 0.37595297\n",
      "Iteration 33, loss = 0.37735406\n",
      "Iteration 34, loss = 0.37627673\n",
      "Iteration 35, loss = 0.37667054\n",
      "Iteration 36, loss = 0.37636401\n",
      "Iteration 37, loss = 0.37628978\n",
      "Iteration 38, loss = 0.37739554\n",
      "Iteration 39, loss = 0.37644076\n",
      "Iteration 40, loss = 0.37715979\n",
      "Iteration 41, loss = 0.37654003\n",
      "Iteration 42, loss = 0.37641412\n",
      "Iteration 43, loss = 0.37623366\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.87525960\n",
      "Iteration 2, loss = 0.72613716\n",
      "Iteration 3, loss = 0.68466572\n",
      "Iteration 4, loss = 0.61473313\n",
      "Iteration 5, loss = 0.51337051\n",
      "Iteration 6, loss = 0.44020675\n",
      "Iteration 7, loss = 0.40836399\n",
      "Iteration 8, loss = 0.39697185\n",
      "Iteration 9, loss = 0.39304030\n",
      "Iteration 10, loss = 0.39009120\n",
      "Iteration 11, loss = 0.38882382\n",
      "Iteration 12, loss = 0.38817106\n",
      "Iteration 13, loss = 0.38837884\n",
      "Iteration 14, loss = 0.38776593\n",
      "Iteration 15, loss = 0.38720960\n",
      "Iteration 16, loss = 0.38662928\n",
      "Iteration 17, loss = 0.38639455\n",
      "Iteration 18, loss = 0.38659930\n",
      "Iteration 19, loss = 0.38599642\n",
      "Iteration 20, loss = 0.38589869\n",
      "Iteration 21, loss = 0.38553534\n",
      "Iteration 22, loss = 0.38559410\n",
      "Iteration 23, loss = 0.38571607\n",
      "Iteration 24, loss = 0.38602812\n",
      "Iteration 25, loss = 0.38532896\n",
      "Iteration 26, loss = 0.38474735\n",
      "Iteration 27, loss = 0.38539185\n",
      "Iteration 28, loss = 0.38476328\n",
      "Iteration 29, loss = 0.38475637\n",
      "Iteration 30, loss = 0.38504628\n",
      "Iteration 31, loss = 0.38471473\n",
      "Iteration 32, loss = 0.38565833\n",
      "Iteration 33, loss = 0.38572016\n",
      "Iteration 34, loss = 0.38537241\n",
      "Iteration 35, loss = 0.38449720\n",
      "Iteration 36, loss = 0.38572375\n",
      "Iteration 37, loss = 0.38470530\n",
      "Iteration 38, loss = 0.38451743\n",
      "Iteration 39, loss = 0.38457921\n",
      "Iteration 40, loss = 0.38603745\n",
      "Iteration 41, loss = 0.38537111\n",
      "Iteration 42, loss = 0.38574303\n",
      "Iteration 43, loss = 0.38399133\n",
      "Iteration 44, loss = 0.38472531\n",
      "Iteration 45, loss = 0.38452108\n",
      "Iteration 46, loss = 0.38468186\n",
      "Iteration 47, loss = 0.38465799\n",
      "Iteration 48, loss = 0.38409975\n",
      "Iteration 49, loss = 0.38430394\n",
      "Iteration 50, loss = 0.38440108\n",
      "Iteration 51, loss = 0.38440481\n",
      "Iteration 52, loss = 0.38473967\n",
      "Iteration 53, loss = 0.38416436\n",
      "Iteration 54, loss = 0.38441978\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.87178015\n",
      "Iteration 2, loss = 0.72488606\n",
      "Iteration 3, loss = 0.68418534\n",
      "Iteration 4, loss = 0.61600633\n",
      "Iteration 5, loss = 0.51854368\n",
      "Iteration 6, loss = 0.44859548\n",
      "Iteration 7, loss = 0.41755146\n",
      "Iteration 8, loss = 0.40587412\n",
      "Iteration 9, loss = 0.40110147\n",
      "Iteration 10, loss = 0.39919811\n",
      "Iteration 11, loss = 0.39793798\n",
      "Iteration 12, loss = 0.39711981\n",
      "Iteration 13, loss = 0.39672591\n",
      "Iteration 14, loss = 0.39579653\n",
      "Iteration 15, loss = 0.39567534\n",
      "Iteration 16, loss = 0.39554046\n",
      "Iteration 17, loss = 0.39520195\n",
      "Iteration 18, loss = 0.39483324\n",
      "Iteration 19, loss = 0.39417109\n",
      "Iteration 20, loss = 0.39424333\n",
      "Iteration 21, loss = 0.39404704\n",
      "Iteration 22, loss = 0.39360284\n",
      "Iteration 23, loss = 0.39363051\n",
      "Iteration 24, loss = 0.39365638\n",
      "Iteration 25, loss = 0.39354582\n",
      "Iteration 26, loss = 0.39307163\n",
      "Iteration 27, loss = 0.39348250\n",
      "Iteration 28, loss = 0.39360386\n",
      "Iteration 29, loss = 0.39339295\n",
      "Iteration 30, loss = 0.39316634\n",
      "Iteration 31, loss = 0.39339585\n",
      "Iteration 32, loss = 0.39435062\n",
      "Iteration 33, loss = 0.39250975\n",
      "Iteration 34, loss = 0.39278764\n",
      "Iteration 35, loss = 0.39261973\n",
      "Iteration 36, loss = 0.39255131\n",
      "Iteration 37, loss = 0.39280357\n",
      "Iteration 38, loss = 0.39455515\n",
      "Iteration 39, loss = 0.39273521\n",
      "Iteration 40, loss = 0.39302254\n",
      "Iteration 41, loss = 0.39266221\n",
      "Iteration 42, loss = 0.39247469\n",
      "Iteration 43, loss = 0.39233218\n",
      "Iteration 44, loss = 0.39255858\n",
      "Iteration 45, loss = 0.39227500\n",
      "Iteration 46, loss = 0.39226280\n",
      "Iteration 47, loss = 0.39241044\n",
      "Iteration 48, loss = 0.39237515\n",
      "Iteration 49, loss = 0.39330372\n",
      "Iteration 50, loss = 0.39356311\n",
      "Iteration 51, loss = 0.39317984\n",
      "Iteration 52, loss = 0.39255297\n",
      "Iteration 53, loss = 0.39226953\n",
      "Iteration 54, loss = 0.39233108\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.71326859\n",
      "Iteration 2, loss = 0.69052538\n",
      "Iteration 3, loss = 0.68291451\n",
      "Iteration 4, loss = 0.67402709\n",
      "Iteration 5, loss = 0.66146816\n",
      "Iteration 6, loss = 0.64354900\n",
      "Iteration 7, loss = 0.61917680\n",
      "Iteration 8, loss = 0.58722532\n",
      "Iteration 9, loss = 0.55037675\n",
      "Iteration 10, loss = 0.51213342\n",
      "Iteration 11, loss = 0.47758117\n",
      "Iteration 12, loss = 0.44963664\n",
      "Iteration 13, loss = 0.42930585\n",
      "Iteration 14, loss = 0.41441123\n",
      "Iteration 15, loss = 0.40427453\n",
      "Iteration 16, loss = 0.39742860\n",
      "Iteration 17, loss = 0.39263565\n",
      "Iteration 18, loss = 0.38951704\n",
      "Iteration 19, loss = 0.38685408\n",
      "Iteration 20, loss = 0.38600940\n",
      "Iteration 21, loss = 0.38420816\n",
      "Iteration 22, loss = 0.38301389\n",
      "Iteration 23, loss = 0.38264003\n",
      "Iteration 24, loss = 0.38203543\n",
      "Iteration 25, loss = 0.38171991\n",
      "Iteration 26, loss = 0.38096698\n",
      "Iteration 27, loss = 0.38088866\n",
      "Iteration 28, loss = 0.38038245\n",
      "Iteration 29, loss = 0.38044137\n",
      "Iteration 30, loss = 0.38037814\n",
      "Iteration 31, loss = 0.37979368\n",
      "Iteration 32, loss = 0.37982488\n",
      "Iteration 33, loss = 0.37942533\n",
      "Iteration 34, loss = 0.37934016\n",
      "Iteration 35, loss = 0.37940165\n",
      "Iteration 36, loss = 0.37908164\n",
      "Iteration 37, loss = 0.37913711\n",
      "Iteration 38, loss = 0.37874891\n",
      "Iteration 39, loss = 0.37854343\n",
      "Iteration 40, loss = 0.37845168\n",
      "Iteration 41, loss = 0.37873814\n",
      "Iteration 42, loss = 0.37826570\n",
      "Iteration 43, loss = 0.37810758\n",
      "Iteration 44, loss = 0.37780598\n",
      "Iteration 45, loss = 0.37792131\n",
      "Iteration 46, loss = 0.37786120\n",
      "Iteration 47, loss = 0.37805207\n",
      "Iteration 48, loss = 0.37774399\n",
      "Iteration 49, loss = 0.37790524\n",
      "Iteration 50, loss = 0.37767781\n",
      "Iteration 51, loss = 0.37745656\n",
      "Iteration 52, loss = 0.37713729\n",
      "Iteration 53, loss = 0.37741010\n",
      "Iteration 54, loss = 0.37724540\n",
      "Iteration 55, loss = 0.37710155\n",
      "Iteration 56, loss = 0.37710417\n",
      "Iteration 57, loss = 0.37698147\n",
      "Iteration 58, loss = 0.37692927\n",
      "Iteration 59, loss = 0.37726547\n",
      "Iteration 60, loss = 0.37694886\n",
      "Iteration 61, loss = 0.37681645\n",
      "Iteration 62, loss = 0.37675306\n",
      "Iteration 63, loss = 0.37651734\n",
      "Iteration 64, loss = 0.37668751\n",
      "Iteration 65, loss = 0.37674390\n",
      "Iteration 66, loss = 0.37666371\n",
      "Iteration 67, loss = 0.37635645\n",
      "Iteration 68, loss = 0.37676946\n",
      "Iteration 69, loss = 0.37660145\n",
      "Iteration 70, loss = 0.37655632\n",
      "Iteration 71, loss = 0.37688469\n",
      "Iteration 72, loss = 0.37619047\n",
      "Iteration 73, loss = 0.37638305\n",
      "Iteration 74, loss = 0.37648983\n",
      "Iteration 75, loss = 0.37643091\n",
      "Iteration 76, loss = 0.37616261\n",
      "Iteration 77, loss = 0.37616488\n",
      "Iteration 78, loss = 0.37631646\n",
      "Iteration 79, loss = 0.37621046\n",
      "Iteration 80, loss = 0.37643346\n",
      "Iteration 81, loss = 0.37623135\n",
      "Iteration 82, loss = 0.37615808\n",
      "Iteration 83, loss = 0.37615325\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.71513128\n",
      "Iteration 2, loss = 0.69011447\n",
      "Iteration 3, loss = 0.68252040\n",
      "Iteration 4, loss = 0.67329897\n",
      "Iteration 5, loss = 0.66085719\n",
      "Iteration 6, loss = 0.64278529\n",
      "Iteration 7, loss = 0.61821903\n",
      "Iteration 8, loss = 0.58666396\n",
      "Iteration 9, loss = 0.55066852\n",
      "Iteration 10, loss = 0.51368826\n",
      "Iteration 11, loss = 0.48086054\n",
      "Iteration 12, loss = 0.45440511\n",
      "Iteration 13, loss = 0.43504236\n",
      "Iteration 14, loss = 0.42160208\n",
      "Iteration 15, loss = 0.41197309\n",
      "Iteration 16, loss = 0.40530449\n",
      "Iteration 17, loss = 0.40083001\n",
      "Iteration 18, loss = 0.39778162\n",
      "Iteration 19, loss = 0.39581777\n",
      "Iteration 20, loss = 0.39423328\n",
      "Iteration 21, loss = 0.39312793\n",
      "Iteration 22, loss = 0.39199017\n",
      "Iteration 23, loss = 0.39127992\n",
      "Iteration 24, loss = 0.39094609\n",
      "Iteration 25, loss = 0.39039271\n",
      "Iteration 26, loss = 0.39037601\n",
      "Iteration 27, loss = 0.38926209\n",
      "Iteration 28, loss = 0.38923217\n",
      "Iteration 29, loss = 0.38930569\n",
      "Iteration 30, loss = 0.38944677\n",
      "Iteration 31, loss = 0.38874345\n",
      "Iteration 32, loss = 0.38845883\n",
      "Iteration 33, loss = 0.38842869\n",
      "Iteration 34, loss = 0.38803434\n",
      "Iteration 35, loss = 0.38770400\n",
      "Iteration 36, loss = 0.38778970\n",
      "Iteration 37, loss = 0.38773389\n",
      "Iteration 38, loss = 0.38754121\n",
      "Iteration 39, loss = 0.38777561\n",
      "Iteration 40, loss = 0.38712501\n",
      "Iteration 41, loss = 0.38757622\n",
      "Iteration 42, loss = 0.38699419\n",
      "Iteration 43, loss = 0.38669941\n",
      "Iteration 44, loss = 0.38668442\n",
      "Iteration 45, loss = 0.38661219\n",
      "Iteration 46, loss = 0.38634968\n",
      "Iteration 47, loss = 0.38658356\n",
      "Iteration 48, loss = 0.38630506\n",
      "Iteration 49, loss = 0.38618334\n",
      "Iteration 50, loss = 0.38611906\n",
      "Iteration 51, loss = 0.38579115\n",
      "Iteration 52, loss = 0.38607128\n",
      "Iteration 53, loss = 0.38568996\n",
      "Iteration 54, loss = 0.38588697\n",
      "Iteration 55, loss = 0.38546538\n",
      "Iteration 56, loss = 0.38552381\n",
      "Iteration 57, loss = 0.38566891\n",
      "Iteration 58, loss = 0.38532916\n",
      "Iteration 59, loss = 0.38537742\n",
      "Iteration 60, loss = 0.38569309\n",
      "Iteration 61, loss = 0.38524173\n",
      "Iteration 62, loss = 0.38525608\n",
      "Iteration 63, loss = 0.38496045\n",
      "Iteration 64, loss = 0.38521596\n",
      "Iteration 65, loss = 0.38512684\n",
      "Iteration 66, loss = 0.38542189\n",
      "Iteration 67, loss = 0.38501977\n",
      "Iteration 68, loss = 0.38491121\n",
      "Iteration 69, loss = 0.38506359\n",
      "Iteration 70, loss = 0.38493780\n",
      "Iteration 71, loss = 0.38513113\n",
      "Iteration 72, loss = 0.38459911\n",
      "Iteration 73, loss = 0.38470762\n",
      "Iteration 74, loss = 0.38478107\n",
      "Iteration 75, loss = 0.38451554\n",
      "Iteration 76, loss = 0.38531913\n",
      "Iteration 77, loss = 0.38534808\n",
      "Iteration 78, loss = 0.38494411\n",
      "Iteration 79, loss = 0.38470806\n",
      "Iteration 80, loss = 0.38489397\n",
      "Iteration 81, loss = 0.38456305\n",
      "Iteration 82, loss = 0.38449775\n",
      "Iteration 83, loss = 0.38454357\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.71669606\n",
      "Iteration 2, loss = 0.69012618\n",
      "Iteration 3, loss = 0.68266048\n",
      "Iteration 4, loss = 0.67369893\n",
      "Iteration 5, loss = 0.66137841\n",
      "Iteration 6, loss = 0.64387344\n",
      "Iteration 7, loss = 0.61997183\n",
      "Iteration 8, loss = 0.58964326\n",
      "Iteration 9, loss = 0.55474640\n",
      "Iteration 10, loss = 0.51943700\n",
      "Iteration 11, loss = 0.48735642\n",
      "Iteration 12, loss = 0.46226887\n",
      "Iteration 13, loss = 0.44357620\n",
      "Iteration 14, loss = 0.43004342\n",
      "Iteration 15, loss = 0.42080609\n",
      "Iteration 16, loss = 0.41436186\n",
      "Iteration 17, loss = 0.41029514\n",
      "Iteration 18, loss = 0.40708983\n",
      "Iteration 19, loss = 0.40561662\n",
      "Iteration 20, loss = 0.40351271\n",
      "Iteration 21, loss = 0.40234337\n",
      "Iteration 22, loss = 0.40151150\n",
      "Iteration 23, loss = 0.40103146\n",
      "Iteration 24, loss = 0.40035349\n",
      "Iteration 25, loss = 0.40012692\n",
      "Iteration 26, loss = 0.39919572\n",
      "Iteration 27, loss = 0.39922606\n",
      "Iteration 28, loss = 0.39875870\n",
      "Iteration 29, loss = 0.39837869\n",
      "Iteration 30, loss = 0.39805986\n",
      "Iteration 31, loss = 0.39786299\n",
      "Iteration 32, loss = 0.39778058\n",
      "Iteration 33, loss = 0.39730841\n",
      "Iteration 34, loss = 0.39713251\n",
      "Iteration 35, loss = 0.39731171\n",
      "Iteration 36, loss = 0.39628014\n",
      "Iteration 37, loss = 0.39729490\n",
      "Iteration 38, loss = 0.39618520\n",
      "Iteration 39, loss = 0.39613657\n",
      "Iteration 40, loss = 0.39602628\n",
      "Iteration 41, loss = 0.39593043\n",
      "Iteration 42, loss = 0.39558502\n",
      "Iteration 43, loss = 0.39565870\n",
      "Iteration 44, loss = 0.39557488\n",
      "Iteration 45, loss = 0.39560253\n",
      "Iteration 46, loss = 0.39491971\n",
      "Iteration 47, loss = 0.39544571\n",
      "Iteration 48, loss = 0.39480750\n",
      "Iteration 49, loss = 0.39473020\n",
      "Iteration 50, loss = 0.39447323\n",
      "Iteration 51, loss = 0.39459929\n",
      "Iteration 52, loss = 0.39470057\n",
      "Iteration 53, loss = 0.39434429\n",
      "Iteration 54, loss = 0.39463463\n",
      "Iteration 55, loss = 0.39445774\n",
      "Iteration 56, loss = 0.39392107\n",
      "Iteration 57, loss = 0.39393480\n",
      "Iteration 58, loss = 0.39387864\n",
      "Iteration 59, loss = 0.39386641\n",
      "Iteration 60, loss = 0.39388442\n",
      "Iteration 61, loss = 0.39371086\n",
      "Iteration 62, loss = 0.39350325\n",
      "Iteration 63, loss = 0.39363560\n",
      "Iteration 64, loss = 0.39336535\n",
      "Iteration 65, loss = 0.39321473\n",
      "Iteration 66, loss = 0.39348453\n",
      "Iteration 67, loss = 0.39356510\n",
      "Iteration 68, loss = 0.39344104\n",
      "Iteration 69, loss = 0.39332103\n",
      "Iteration 70, loss = 0.39312328\n",
      "Iteration 71, loss = 0.39334327\n",
      "Iteration 72, loss = 0.39328931\n",
      "Iteration 73, loss = 0.39340668\n",
      "Iteration 74, loss = 0.39336387\n",
      "Iteration 75, loss = 0.39288540\n",
      "Iteration 76, loss = 0.39290995\n",
      "Iteration 77, loss = 0.39307871\n",
      "Iteration 78, loss = 0.39291247\n",
      "Iteration 79, loss = 0.39283217\n",
      "Iteration 80, loss = 0.39287814\n",
      "Iteration 81, loss = 0.39306410\n",
      "Iteration 82, loss = 0.39282797\n",
      "Iteration 83, loss = 0.39272090\n",
      "Iteration 84, loss = 0.39305976\n",
      "Iteration 85, loss = 0.39266334\n",
      "Iteration 86, loss = 0.39264040\n",
      "Iteration 87, loss = 0.39267174\n",
      "Iteration 88, loss = 0.39261380\n",
      "Iteration 89, loss = 0.39284553\n",
      "Iteration 90, loss = 0.39278950\n",
      "Iteration 91, loss = 0.39316414\n",
      "Iteration 92, loss = 0.39264980\n",
      "Iteration 93, loss = 0.39240776\n",
      "Iteration 94, loss = 0.39270629\n",
      "Iteration 95, loss = 0.39303933\n",
      "Iteration 96, loss = 0.39259561\n",
      "Iteration 97, loss = 0.39272852\n",
      "Iteration 98, loss = 0.39243716\n",
      "Iteration 99, loss = 0.39286648\n",
      "Iteration 100, loss = 0.39248793\n",
      "Iteration 101, loss = 0.39228454\n",
      "Iteration 102, loss = 0.39276660\n",
      "Iteration 103, loss = 0.39198386\n",
      "Iteration 104, loss = 0.39271425\n",
      "Iteration 105, loss = 0.39234713\n",
      "Iteration 106, loss = 0.39255489\n",
      "Iteration 107, loss = 0.39237941\n",
      "Iteration 108, loss = 0.39226369\n",
      "Iteration 109, loss = 0.39265721\n",
      "Iteration 110, loss = 0.39255979\n",
      "Iteration 111, loss = 0.39229369\n",
      "Iteration 112, loss = 0.39229169\n",
      "Iteration 113, loss = 0.39221335\n",
      "Iteration 114, loss = 0.39266348\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70778367\n",
      "Iteration 2, loss = 0.64828872\n",
      "Iteration 3, loss = 0.52524397\n",
      "Iteration 4, loss = 0.42510676\n",
      "Iteration 5, loss = 0.39341788\n",
      "Iteration 6, loss = 0.38529696\n",
      "Iteration 7, loss = 0.38219486\n",
      "Iteration 8, loss = 0.38164440\n",
      "Iteration 9, loss = 0.38059049\n",
      "Iteration 10, loss = 0.37971509\n",
      "Iteration 11, loss = 0.37931155\n",
      "Iteration 12, loss = 0.37855373\n",
      "Iteration 13, loss = 0.37998998\n",
      "Iteration 14, loss = 0.37850351\n",
      "Iteration 15, loss = 0.37860239\n",
      "Iteration 16, loss = 0.37828313\n",
      "Iteration 17, loss = 0.37752564\n",
      "Iteration 18, loss = 0.37739540\n",
      "Iteration 19, loss = 0.37705293\n",
      "Iteration 20, loss = 0.37920781\n",
      "Iteration 21, loss = 0.37700210\n",
      "Iteration 22, loss = 0.37709744\n",
      "Iteration 23, loss = 0.37718193\n",
      "Iteration 24, loss = 0.37708625\n",
      "Iteration 25, loss = 0.37676292\n",
      "Iteration 26, loss = 0.37638893\n",
      "Iteration 27, loss = 0.37652416\n",
      "Iteration 28, loss = 0.37633639\n",
      "Iteration 29, loss = 0.37713486\n",
      "Iteration 30, loss = 0.37689268\n",
      "Iteration 31, loss = 0.37645778\n",
      "Iteration 32, loss = 0.37691996\n",
      "Iteration 33, loss = 0.37623308\n",
      "Iteration 34, loss = 0.37666664\n",
      "Iteration 35, loss = 0.37635848\n",
      "Iteration 36, loss = 0.37660182\n",
      "Iteration 37, loss = 0.37687087\n",
      "Iteration 38, loss = 0.37675587\n",
      "Iteration 39, loss = 0.37654775\n",
      "Iteration 40, loss = 0.37654639\n",
      "Iteration 41, loss = 0.37744511\n",
      "Iteration 42, loss = 0.37607396\n",
      "Iteration 43, loss = 0.37608480\n",
      "Iteration 44, loss = 0.37594498\n",
      "Iteration 45, loss = 0.37622783\n",
      "Iteration 46, loss = 0.37600506\n",
      "Iteration 47, loss = 0.37699382\n",
      "Iteration 48, loss = 0.37639784\n",
      "Iteration 49, loss = 0.37645479\n",
      "Iteration 50, loss = 0.37651304\n",
      "Iteration 51, loss = 0.37611105\n",
      "Iteration 52, loss = 0.37583114\n",
      "Iteration 53, loss = 0.37633456\n",
      "Iteration 54, loss = 0.37651662\n",
      "Iteration 55, loss = 0.37600994\n",
      "Iteration 56, loss = 0.37624058\n",
      "Iteration 57, loss = 0.37588183\n",
      "Iteration 58, loss = 0.37609192\n",
      "Iteration 59, loss = 0.37630607\n",
      "Iteration 60, loss = 0.37659833\n",
      "Iteration 61, loss = 0.37605816\n",
      "Iteration 62, loss = 0.37597855\n",
      "Iteration 63, loss = 0.37582013\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.71053381\n",
      "Iteration 2, loss = 0.65113696\n",
      "Iteration 3, loss = 0.53310907\n",
      "Iteration 4, loss = 0.43463011\n",
      "Iteration 5, loss = 0.40313341\n",
      "Iteration 6, loss = 0.39567589\n",
      "Iteration 7, loss = 0.39160791\n",
      "Iteration 8, loss = 0.39058558\n",
      "Iteration 9, loss = 0.38957687\n",
      "Iteration 10, loss = 0.38845395\n",
      "Iteration 11, loss = 0.38840406\n",
      "Iteration 12, loss = 0.38736797\n",
      "Iteration 13, loss = 0.38783986\n",
      "Iteration 14, loss = 0.38752483\n",
      "Iteration 15, loss = 0.38737482\n",
      "Iteration 16, loss = 0.38663338\n",
      "Iteration 17, loss = 0.38651284\n",
      "Iteration 18, loss = 0.38576059\n",
      "Iteration 19, loss = 0.38625165\n",
      "Iteration 20, loss = 0.38633474\n",
      "Iteration 21, loss = 0.38568539\n",
      "Iteration 22, loss = 0.38659363\n",
      "Iteration 23, loss = 0.38550831\n",
      "Iteration 24, loss = 0.38608093\n",
      "Iteration 25, loss = 0.38517140\n",
      "Iteration 26, loss = 0.38569449\n",
      "Iteration 27, loss = 0.38472376\n",
      "Iteration 28, loss = 0.38457144\n",
      "Iteration 29, loss = 0.38519093\n",
      "Iteration 30, loss = 0.38557572\n",
      "Iteration 31, loss = 0.38605193\n",
      "Iteration 32, loss = 0.38492505\n",
      "Iteration 33, loss = 0.38512594\n",
      "Iteration 34, loss = 0.38480111\n",
      "Iteration 35, loss = 0.38436528\n",
      "Iteration 36, loss = 0.38517073\n",
      "Iteration 37, loss = 0.38505525\n",
      "Iteration 38, loss = 0.38511305\n",
      "Iteration 39, loss = 0.38639216\n",
      "Iteration 40, loss = 0.38451744\n",
      "Iteration 41, loss = 0.38633003\n",
      "Iteration 42, loss = 0.38485528\n",
      "Iteration 43, loss = 0.38450712\n",
      "Iteration 44, loss = 0.38479227\n",
      "Iteration 45, loss = 0.38484953\n",
      "Iteration 46, loss = 0.38418843\n",
      "Iteration 47, loss = 0.38548715\n",
      "Iteration 48, loss = 0.38458881\n",
      "Iteration 49, loss = 0.38450796\n",
      "Iteration 50, loss = 0.38484140\n",
      "Iteration 51, loss = 0.38388044\n",
      "Iteration 52, loss = 0.38484240\n",
      "Iteration 53, loss = 0.38421622\n",
      "Iteration 54, loss = 0.38499713\n",
      "Iteration 55, loss = 0.38409844\n",
      "Iteration 56, loss = 0.38450137\n",
      "Iteration 57, loss = 0.38477506\n",
      "Iteration 58, loss = 0.38455101\n",
      "Iteration 59, loss = 0.38448069\n",
      "Iteration 60, loss = 0.38525039\n",
      "Iteration 61, loss = 0.38483476\n",
      "Iteration 62, loss = 0.38439184\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.71282735\n",
      "Iteration 2, loss = 0.65562822\n",
      "Iteration 3, loss = 0.54660883\n",
      "Iteration 4, loss = 0.44951704\n",
      "Iteration 5, loss = 0.41555339\n",
      "Iteration 6, loss = 0.40543872\n",
      "Iteration 7, loss = 0.40125947\n",
      "Iteration 8, loss = 0.39968939\n",
      "Iteration 9, loss = 0.39887478\n",
      "Iteration 10, loss = 0.39863049\n",
      "Iteration 11, loss = 0.39684108\n",
      "Iteration 12, loss = 0.39747805\n",
      "Iteration 13, loss = 0.39755177\n",
      "Iteration 14, loss = 0.39585256\n",
      "Iteration 15, loss = 0.39568098\n",
      "Iteration 16, loss = 0.39482393\n",
      "Iteration 17, loss = 0.39507990\n",
      "Iteration 18, loss = 0.39443819\n",
      "Iteration 19, loss = 0.39552363\n",
      "Iteration 20, loss = 0.39454178\n",
      "Iteration 21, loss = 0.39405438\n",
      "Iteration 22, loss = 0.39391421\n",
      "Iteration 23, loss = 0.39416266\n",
      "Iteration 24, loss = 0.39354165\n",
      "Iteration 25, loss = 0.39492420\n",
      "Iteration 26, loss = 0.39298110\n",
      "Iteration 27, loss = 0.39359070\n",
      "Iteration 28, loss = 0.39289290\n",
      "Iteration 29, loss = 0.39305013\n",
      "Iteration 30, loss = 0.39286105\n",
      "Iteration 31, loss = 0.39305716\n",
      "Iteration 32, loss = 0.39336558\n",
      "Iteration 33, loss = 0.39267452\n",
      "Iteration 34, loss = 0.39306017\n",
      "Iteration 35, loss = 0.39319684\n",
      "Iteration 36, loss = 0.39331738\n",
      "Iteration 37, loss = 0.39378526\n",
      "Iteration 38, loss = 0.39292924\n",
      "Iteration 39, loss = 0.39277147\n",
      "Iteration 40, loss = 0.39285794\n",
      "Iteration 41, loss = 0.39333942\n",
      "Iteration 42, loss = 0.39257256\n",
      "Iteration 43, loss = 0.39329236\n",
      "Iteration 44, loss = 0.39280667\n",
      "Iteration 45, loss = 0.39290924\n",
      "Iteration 46, loss = 0.39223631\n",
      "Iteration 47, loss = 0.39352006\n",
      "Iteration 48, loss = 0.39277123\n",
      "Iteration 49, loss = 0.39265604\n",
      "Iteration 50, loss = 0.39244978\n",
      "Iteration 51, loss = 0.39279857\n",
      "Iteration 52, loss = 0.39384847\n",
      "Iteration 53, loss = 0.39284535\n",
      "Iteration 54, loss = 0.39328273\n",
      "Iteration 55, loss = 0.39330664\n",
      "Iteration 56, loss = 0.39227530\n",
      "Iteration 57, loss = 0.39284207\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.67776433\n",
      "Iteration 2, loss = 0.66063632\n",
      "Iteration 3, loss = 0.64652522\n",
      "Iteration 4, loss = 0.63331269\n",
      "Iteration 5, loss = 0.62099495\n",
      "Iteration 6, loss = 0.60948657\n",
      "Iteration 7, loss = 0.59877332\n",
      "Iteration 8, loss = 0.58876504\n",
      "Iteration 9, loss = 0.57935828\n",
      "Iteration 10, loss = 0.57054885\n",
      "Iteration 11, loss = 0.56233498\n",
      "Iteration 12, loss = 0.55456744\n",
      "Iteration 13, loss = 0.54725492\n",
      "Iteration 14, loss = 0.54044482\n",
      "Iteration 15, loss = 0.53397881\n",
      "Iteration 16, loss = 0.52794604\n",
      "Iteration 17, loss = 0.52221220\n",
      "Iteration 18, loss = 0.51681651\n",
      "Iteration 19, loss = 0.51166502\n",
      "Iteration 20, loss = 0.50683707\n",
      "Iteration 21, loss = 0.50223881\n",
      "Iteration 22, loss = 0.49788550\n",
      "Iteration 23, loss = 0.49377467\n",
      "Iteration 24, loss = 0.48990196\n",
      "Iteration 25, loss = 0.48609963\n",
      "Iteration 26, loss = 0.48251458\n",
      "Iteration 27, loss = 0.47914464\n",
      "Iteration 28, loss = 0.47590354\n",
      "Iteration 29, loss = 0.47280768\n",
      "Iteration 30, loss = 0.46987168\n",
      "Iteration 31, loss = 0.46710015\n",
      "Iteration 32, loss = 0.46440287\n",
      "Iteration 33, loss = 0.46183968\n",
      "Iteration 34, loss = 0.45938156\n",
      "Iteration 35, loss = 0.45700412\n",
      "Iteration 36, loss = 0.45478186\n",
      "Iteration 37, loss = 0.45259425\n",
      "Iteration 38, loss = 0.45053171\n",
      "Iteration 39, loss = 0.44851967\n",
      "Iteration 40, loss = 0.44659524\n",
      "Iteration 41, loss = 0.44474659\n",
      "Iteration 42, loss = 0.44297037\n",
      "Iteration 43, loss = 0.44125683\n",
      "Iteration 44, loss = 0.43960147\n",
      "Iteration 45, loss = 0.43801912\n",
      "Iteration 46, loss = 0.43650491\n",
      "Iteration 47, loss = 0.43501999\n",
      "Iteration 48, loss = 0.43358443\n",
      "Iteration 49, loss = 0.43221532\n",
      "Iteration 50, loss = 0.43088285\n",
      "Iteration 51, loss = 0.42962541\n",
      "Iteration 52, loss = 0.42840236\n",
      "Iteration 53, loss = 0.42720670\n",
      "Iteration 54, loss = 0.42603171\n",
      "Iteration 55, loss = 0.42492394\n",
      "Iteration 56, loss = 0.42384430\n",
      "Iteration 57, loss = 0.42283626\n",
      "Iteration 58, loss = 0.42178173\n",
      "Iteration 59, loss = 0.42080328\n",
      "Iteration 60, loss = 0.41986894\n",
      "Iteration 61, loss = 0.41895692\n",
      "Iteration 62, loss = 0.41803256\n",
      "Iteration 63, loss = 0.41716676\n",
      "Iteration 64, loss = 0.41631973\n",
      "Iteration 65, loss = 0.41549342\n",
      "Iteration 66, loss = 0.41469583\n",
      "Iteration 67, loss = 0.41394136\n",
      "Iteration 68, loss = 0.41317517\n",
      "Iteration 69, loss = 0.41246991\n",
      "Iteration 70, loss = 0.41175049\n",
      "Iteration 71, loss = 0.41104936\n",
      "Iteration 72, loss = 0.41038046\n",
      "Iteration 73, loss = 0.40971676\n",
      "Iteration 74, loss = 0.40909474\n",
      "Iteration 75, loss = 0.40847052\n",
      "Iteration 76, loss = 0.40786412\n",
      "Iteration 77, loss = 0.40728490\n",
      "Iteration 78, loss = 0.40670071\n",
      "Iteration 79, loss = 0.40615710\n",
      "Iteration 80, loss = 0.40564772\n",
      "Iteration 81, loss = 0.40511254\n",
      "Iteration 82, loss = 0.40457028\n",
      "Iteration 83, loss = 0.40408120\n",
      "Iteration 84, loss = 0.40358898\n",
      "Iteration 85, loss = 0.40310793\n",
      "Iteration 86, loss = 0.40265852\n",
      "Iteration 87, loss = 0.40219655\n",
      "Iteration 88, loss = 0.40178864\n",
      "Iteration 89, loss = 0.40131225\n",
      "Iteration 90, loss = 0.40088550\n",
      "Iteration 91, loss = 0.40048377\n",
      "Iteration 92, loss = 0.40007001\n",
      "Iteration 93, loss = 0.39968484\n",
      "Iteration 94, loss = 0.39930127\n",
      "Iteration 95, loss = 0.39892842\n",
      "Iteration 96, loss = 0.39854888\n",
      "Iteration 97, loss = 0.39820963\n",
      "Iteration 98, loss = 0.39786071\n",
      "Iteration 99, loss = 0.39754024\n",
      "Iteration 100, loss = 0.39718179\n",
      "Iteration 101, loss = 0.39684150\n",
      "Iteration 102, loss = 0.39654589\n",
      "Iteration 103, loss = 0.39629722\n",
      "Iteration 104, loss = 0.39591461\n",
      "Iteration 105, loss = 0.39560121\n",
      "Iteration 106, loss = 0.39535156\n",
      "Iteration 107, loss = 0.39502459\n",
      "Iteration 108, loss = 0.39477089\n",
      "Iteration 109, loss = 0.39447865\n",
      "Iteration 110, loss = 0.39421159\n",
      "Iteration 111, loss = 0.39392623\n",
      "Iteration 112, loss = 0.39368000\n",
      "Iteration 113, loss = 0.39341042\n",
      "Iteration 114, loss = 0.39317467\n",
      "Iteration 115, loss = 0.39292443\n",
      "Iteration 116, loss = 0.39271219\n",
      "Iteration 117, loss = 0.39246449\n",
      "Iteration 118, loss = 0.39224510\n",
      "Iteration 119, loss = 0.39202432\n",
      "Iteration 120, loss = 0.39181726\n",
      "Iteration 121, loss = 0.39158953\n",
      "Iteration 122, loss = 0.39136315\n",
      "Iteration 123, loss = 0.39114571\n",
      "Iteration 124, loss = 0.39094563\n",
      "Iteration 125, loss = 0.39073500\n",
      "Iteration 126, loss = 0.39057604\n",
      "Iteration 127, loss = 0.39039745\n",
      "Iteration 128, loss = 0.39017014\n",
      "Iteration 129, loss = 0.38999396\n",
      "Iteration 130, loss = 0.38983815\n",
      "Iteration 131, loss = 0.38962273\n",
      "Iteration 132, loss = 0.38945639\n",
      "Iteration 133, loss = 0.38927352\n",
      "Iteration 134, loss = 0.38910766\n",
      "Iteration 135, loss = 0.38895139\n",
      "Iteration 136, loss = 0.38877342\n",
      "Iteration 137, loss = 0.38864632\n",
      "Iteration 138, loss = 0.38847025\n",
      "Iteration 139, loss = 0.38835148\n",
      "Iteration 140, loss = 0.38816969\n",
      "Iteration 141, loss = 0.38801733\n",
      "Iteration 142, loss = 0.38786785\n",
      "Iteration 143, loss = 0.38772734\n",
      "Iteration 144, loss = 0.38760399\n",
      "Iteration 145, loss = 0.38747156\n",
      "Iteration 146, loss = 0.38732179\n",
      "Iteration 147, loss = 0.38720053\n",
      "Iteration 148, loss = 0.38706528\n",
      "Iteration 149, loss = 0.38697199\n",
      "Iteration 150, loss = 0.38679789\n",
      "Iteration 151, loss = 0.38666618\n",
      "Iteration 152, loss = 0.38656590\n",
      "Iteration 153, loss = 0.38643398\n",
      "Iteration 154, loss = 0.38630760\n",
      "Iteration 155, loss = 0.38619406\n",
      "Iteration 156, loss = 0.38609176\n",
      "Iteration 157, loss = 0.38596669\n",
      "Iteration 158, loss = 0.38586753\n",
      "Iteration 159, loss = 0.38575970\n",
      "Iteration 160, loss = 0.38566525\n",
      "Iteration 161, loss = 0.38553390\n",
      "Iteration 162, loss = 0.38544266\n",
      "Iteration 163, loss = 0.38534956\n",
      "Iteration 164, loss = 0.38523519\n",
      "Iteration 165, loss = 0.38516624\n",
      "Iteration 166, loss = 0.38502745\n",
      "Iteration 167, loss = 0.38495444\n",
      "Iteration 168, loss = 0.38485734\n",
      "Iteration 169, loss = 0.38478608\n",
      "Iteration 170, loss = 0.38466439\n",
      "Iteration 171, loss = 0.38458218\n",
      "Iteration 172, loss = 0.38447559\n",
      "Iteration 173, loss = 0.38438394\n",
      "Iteration 174, loss = 0.38430582\n",
      "Iteration 175, loss = 0.38421676\n",
      "Iteration 176, loss = 0.38412863\n",
      "Iteration 177, loss = 0.38404799\n",
      "Iteration 178, loss = 0.38397198\n",
      "Iteration 179, loss = 0.38389312\n",
      "Iteration 180, loss = 0.38381293\n",
      "Iteration 181, loss = 0.38374205\n",
      "Iteration 182, loss = 0.38366970\n",
      "Iteration 183, loss = 0.38358445\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.67842286\n",
      "Iteration 2, loss = 0.66118009\n",
      "Iteration 3, loss = 0.64709378\n",
      "Iteration 4, loss = 0.63403520\n",
      "Iteration 5, loss = 0.62191247\n",
      "Iteration 6, loss = 0.61048913\n",
      "Iteration 7, loss = 0.59995457\n",
      "Iteration 8, loss = 0.59006204\n",
      "Iteration 9, loss = 0.58084302\n",
      "Iteration 10, loss = 0.57217690\n",
      "Iteration 11, loss = 0.56408570\n",
      "Iteration 12, loss = 0.55650059\n",
      "Iteration 13, loss = 0.54937666\n",
      "Iteration 14, loss = 0.54270253\n",
      "Iteration 15, loss = 0.53638752\n",
      "Iteration 16, loss = 0.53048329\n",
      "Iteration 17, loss = 0.52489866\n",
      "Iteration 18, loss = 0.51961874\n",
      "Iteration 19, loss = 0.51466351\n",
      "Iteration 20, loss = 0.50992529\n",
      "Iteration 21, loss = 0.50552050\n",
      "Iteration 22, loss = 0.50130394\n",
      "Iteration 23, loss = 0.49724506\n",
      "Iteration 24, loss = 0.49343476\n",
      "Iteration 25, loss = 0.48983606\n",
      "Iteration 26, loss = 0.48638220\n",
      "Iteration 27, loss = 0.48314153\n",
      "Iteration 28, loss = 0.48001560\n",
      "Iteration 29, loss = 0.47700763\n",
      "Iteration 30, loss = 0.47420877\n",
      "Iteration 31, loss = 0.47148471\n",
      "Iteration 32, loss = 0.46888863\n",
      "Iteration 33, loss = 0.46640674\n",
      "Iteration 34, loss = 0.46400130\n",
      "Iteration 35, loss = 0.46173003\n",
      "Iteration 36, loss = 0.45955865\n",
      "Iteration 37, loss = 0.45746876\n",
      "Iteration 38, loss = 0.45548242\n",
      "Iteration 39, loss = 0.45357913\n",
      "Iteration 40, loss = 0.45174056\n",
      "Iteration 41, loss = 0.44998917\n",
      "Iteration 42, loss = 0.44826816\n",
      "Iteration 43, loss = 0.44660501\n",
      "Iteration 44, loss = 0.44510641\n",
      "Iteration 45, loss = 0.44349463\n",
      "Iteration 46, loss = 0.44204690\n",
      "Iteration 47, loss = 0.44064173\n",
      "Iteration 48, loss = 0.43926781\n",
      "Iteration 49, loss = 0.43796354\n",
      "Iteration 50, loss = 0.43669220\n",
      "Iteration 51, loss = 0.43551167\n",
      "Iteration 52, loss = 0.43428536\n",
      "Iteration 53, loss = 0.43315886\n",
      "Iteration 54, loss = 0.43204535\n",
      "Iteration 55, loss = 0.43101336\n",
      "Iteration 56, loss = 0.42993613\n",
      "Iteration 57, loss = 0.42895219\n",
      "Iteration 58, loss = 0.42798013\n",
      "Iteration 59, loss = 0.42703398\n",
      "Iteration 60, loss = 0.42614566\n",
      "Iteration 61, loss = 0.42529175\n",
      "Iteration 62, loss = 0.42438652\n",
      "Iteration 63, loss = 0.42356772\n",
      "Iteration 64, loss = 0.42276897\n",
      "Iteration 65, loss = 0.42198312\n",
      "Iteration 66, loss = 0.42122804\n",
      "Iteration 67, loss = 0.42052613\n",
      "Iteration 68, loss = 0.41980283\n",
      "Iteration 69, loss = 0.41910936\n",
      "Iteration 70, loss = 0.41843095\n",
      "Iteration 71, loss = 0.41778881\n",
      "Iteration 72, loss = 0.41712035\n",
      "Iteration 73, loss = 0.41648896\n",
      "Iteration 74, loss = 0.41590072\n",
      "Iteration 75, loss = 0.41529499\n",
      "Iteration 76, loss = 0.41476032\n",
      "Iteration 77, loss = 0.41421794\n",
      "Iteration 78, loss = 0.41363672\n",
      "Iteration 79, loss = 0.41310187\n",
      "Iteration 80, loss = 0.41261515\n",
      "Iteration 81, loss = 0.41213400\n",
      "Iteration 82, loss = 0.41161268\n",
      "Iteration 83, loss = 0.41113078\n",
      "Iteration 84, loss = 0.41067446\n",
      "Iteration 85, loss = 0.41020425\n",
      "Iteration 86, loss = 0.40977748\n",
      "Iteration 87, loss = 0.40934245\n",
      "Iteration 88, loss = 0.40895906\n",
      "Iteration 89, loss = 0.40850759\n",
      "Iteration 90, loss = 0.40814252\n",
      "Iteration 91, loss = 0.40773285\n",
      "Iteration 92, loss = 0.40733422\n",
      "Iteration 93, loss = 0.40696372\n",
      "Iteration 94, loss = 0.40663332\n",
      "Iteration 95, loss = 0.40624799\n",
      "Iteration 96, loss = 0.40591216\n",
      "Iteration 97, loss = 0.40561203\n",
      "Iteration 98, loss = 0.40525734\n",
      "Iteration 99, loss = 0.40494970\n",
      "Iteration 100, loss = 0.40460782\n",
      "Iteration 101, loss = 0.40428718\n",
      "Iteration 102, loss = 0.40398389\n",
      "Iteration 103, loss = 0.40367816\n",
      "Iteration 104, loss = 0.40344754\n",
      "Iteration 105, loss = 0.40310574\n",
      "Iteration 106, loss = 0.40284546\n",
      "Iteration 107, loss = 0.40257616\n",
      "Iteration 108, loss = 0.40231959\n",
      "Iteration 109, loss = 0.40207047\n",
      "Iteration 110, loss = 0.40183718\n",
      "Iteration 111, loss = 0.40154793\n",
      "Iteration 112, loss = 0.40128956\n",
      "Iteration 113, loss = 0.40105688\n",
      "Iteration 114, loss = 0.40081065\n",
      "Iteration 115, loss = 0.40061983\n",
      "Iteration 116, loss = 0.40043122\n",
      "Iteration 117, loss = 0.40013816\n",
      "Iteration 118, loss = 0.39994165\n",
      "Iteration 119, loss = 0.39975910\n",
      "Iteration 120, loss = 0.39952452\n",
      "Iteration 121, loss = 0.39931801\n",
      "Iteration 122, loss = 0.39912785\n",
      "Iteration 123, loss = 0.39892112\n",
      "Iteration 124, loss = 0.39872371\n",
      "Iteration 125, loss = 0.39853847\n",
      "Iteration 126, loss = 0.39836239\n",
      "Iteration 127, loss = 0.39818278\n",
      "Iteration 128, loss = 0.39798875\n",
      "Iteration 129, loss = 0.39783536\n",
      "Iteration 130, loss = 0.39765507\n",
      "Iteration 131, loss = 0.39748357\n",
      "Iteration 132, loss = 0.39730952\n",
      "Iteration 133, loss = 0.39715948\n",
      "Iteration 134, loss = 0.39700202\n",
      "Iteration 135, loss = 0.39684141\n",
      "Iteration 136, loss = 0.39671418\n",
      "Iteration 137, loss = 0.39653484\n",
      "Iteration 138, loss = 0.39639101\n",
      "Iteration 139, loss = 0.39627779\n",
      "Iteration 140, loss = 0.39610949\n",
      "Iteration 141, loss = 0.39596966\n",
      "Iteration 142, loss = 0.39583800\n",
      "Iteration 143, loss = 0.39572510\n",
      "Iteration 144, loss = 0.39558373\n",
      "Iteration 145, loss = 0.39547241\n",
      "Iteration 146, loss = 0.39532981\n",
      "Iteration 147, loss = 0.39519616\n",
      "Iteration 148, loss = 0.39513663\n",
      "Iteration 149, loss = 0.39498539\n",
      "Iteration 150, loss = 0.39483069\n",
      "Iteration 151, loss = 0.39473530\n",
      "Iteration 152, loss = 0.39460494\n",
      "Iteration 153, loss = 0.39449023\n",
      "Iteration 154, loss = 0.39437480\n",
      "Iteration 155, loss = 0.39428436\n",
      "Iteration 156, loss = 0.39419915\n",
      "Iteration 157, loss = 0.39404752\n",
      "Iteration 158, loss = 0.39394974\n",
      "Iteration 159, loss = 0.39385373\n",
      "Iteration 160, loss = 0.39374875\n",
      "Iteration 161, loss = 0.39364120\n",
      "Iteration 162, loss = 0.39356205\n",
      "Iteration 163, loss = 0.39347084\n",
      "Iteration 164, loss = 0.39336814\n",
      "Iteration 165, loss = 0.39327065\n",
      "Iteration 166, loss = 0.39317573\n",
      "Iteration 167, loss = 0.39307986\n",
      "Iteration 168, loss = 0.39299184\n",
      "Iteration 169, loss = 0.39290955\n",
      "Iteration 170, loss = 0.39282553\n",
      "Iteration 171, loss = 0.39272333\n",
      "Iteration 172, loss = 0.39265047\n",
      "Iteration 173, loss = 0.39258341\n",
      "Iteration 174, loss = 0.39248991\n",
      "Iteration 175, loss = 0.39242216\n",
      "Iteration 176, loss = 0.39235994\n",
      "Iteration 177, loss = 0.39225855\n",
      "Iteration 178, loss = 0.39218902\n",
      "Iteration 179, loss = 0.39211731\n",
      "Iteration 180, loss = 0.39206413\n",
      "Iteration 181, loss = 0.39202550\n",
      "Iteration 182, loss = 0.39188102\n",
      "Iteration 183, loss = 0.39180972\n",
      "Iteration 184, loss = 0.39173781\n",
      "Iteration 185, loss = 0.39167444\n",
      "Iteration 186, loss = 0.39161666\n",
      "Iteration 187, loss = 0.39155008\n",
      "Iteration 188, loss = 0.39148133\n",
      "Iteration 189, loss = 0.39141231\n",
      "Iteration 190, loss = 0.39136440\n",
      "Iteration 191, loss = 0.39129908\n",
      "Iteration 192, loss = 0.39122614\n",
      "Iteration 193, loss = 0.39116941\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.67874501\n",
      "Iteration 2, loss = 0.66173364\n",
      "Iteration 3, loss = 0.64800290\n",
      "Iteration 4, loss = 0.63519636\n",
      "Iteration 5, loss = 0.62326522\n",
      "Iteration 6, loss = 0.61216538\n",
      "Iteration 7, loss = 0.60180466\n",
      "Iteration 8, loss = 0.59219783\n",
      "Iteration 9, loss = 0.58317566\n",
      "Iteration 10, loss = 0.57474821\n",
      "Iteration 11, loss = 0.56681271\n",
      "Iteration 12, loss = 0.55940499\n",
      "Iteration 13, loss = 0.55246691\n",
      "Iteration 14, loss = 0.54593780\n",
      "Iteration 15, loss = 0.53979130\n",
      "Iteration 16, loss = 0.53400183\n",
      "Iteration 17, loss = 0.52857679\n",
      "Iteration 18, loss = 0.52341133\n",
      "Iteration 19, loss = 0.51858167\n",
      "Iteration 20, loss = 0.51401116\n",
      "Iteration 21, loss = 0.50965721\n",
      "Iteration 22, loss = 0.50561335\n",
      "Iteration 23, loss = 0.50166312\n",
      "Iteration 24, loss = 0.49800827\n",
      "Iteration 25, loss = 0.49442495\n",
      "Iteration 26, loss = 0.49110019\n",
      "Iteration 27, loss = 0.48789690\n",
      "Iteration 28, loss = 0.48488801\n",
      "Iteration 29, loss = 0.48197818\n",
      "Iteration 30, loss = 0.47923166\n",
      "Iteration 31, loss = 0.47663306\n",
      "Iteration 32, loss = 0.47411134\n",
      "Iteration 33, loss = 0.47171206\n",
      "Iteration 34, loss = 0.46942504\n",
      "Iteration 35, loss = 0.46723024\n",
      "Iteration 36, loss = 0.46512376\n",
      "Iteration 37, loss = 0.46317870\n",
      "Iteration 38, loss = 0.46119994\n",
      "Iteration 39, loss = 0.45936851\n",
      "Iteration 40, loss = 0.45757869\n",
      "Iteration 41, loss = 0.45585369\n",
      "Iteration 42, loss = 0.45420829\n",
      "Iteration 43, loss = 0.45261947\n",
      "Iteration 44, loss = 0.45109449\n",
      "Iteration 45, loss = 0.44968506\n",
      "Iteration 46, loss = 0.44821601\n",
      "Iteration 47, loss = 0.44688862\n",
      "Iteration 48, loss = 0.44558301\n",
      "Iteration 49, loss = 0.44432022\n",
      "Iteration 50, loss = 0.44309208\n",
      "Iteration 51, loss = 0.44195314\n",
      "Iteration 52, loss = 0.44084871\n",
      "Iteration 53, loss = 0.43968641\n",
      "Iteration 54, loss = 0.43862143\n",
      "Iteration 55, loss = 0.43760822\n",
      "Iteration 56, loss = 0.43659906\n",
      "Iteration 57, loss = 0.43564487\n",
      "Iteration 58, loss = 0.43475162\n",
      "Iteration 59, loss = 0.43381893\n",
      "Iteration 60, loss = 0.43295597\n",
      "Iteration 61, loss = 0.43215303\n",
      "Iteration 62, loss = 0.43129029\n",
      "Iteration 63, loss = 0.43051863\n",
      "Iteration 64, loss = 0.42974041\n",
      "Iteration 65, loss = 0.42898796\n",
      "Iteration 66, loss = 0.42828614\n",
      "Iteration 67, loss = 0.42757845\n",
      "Iteration 68, loss = 0.42688380\n",
      "Iteration 69, loss = 0.42623340\n",
      "Iteration 70, loss = 0.42562285\n",
      "Iteration 71, loss = 0.42495288\n",
      "Iteration 72, loss = 0.42436750\n",
      "Iteration 73, loss = 0.42376714\n",
      "Iteration 74, loss = 0.42318505\n",
      "Iteration 75, loss = 0.42263816\n",
      "Iteration 76, loss = 0.42208974\n",
      "Iteration 77, loss = 0.42156258\n",
      "Iteration 78, loss = 0.42106519\n",
      "Iteration 79, loss = 0.42056068\n",
      "Iteration 80, loss = 0.42006051\n",
      "Iteration 81, loss = 0.41958130\n",
      "Iteration 82, loss = 0.41917738\n",
      "Iteration 83, loss = 0.41867084\n",
      "Iteration 84, loss = 0.41824718\n",
      "Iteration 85, loss = 0.41783418\n",
      "Iteration 86, loss = 0.41739017\n",
      "Iteration 87, loss = 0.41698052\n",
      "Iteration 88, loss = 0.41658507\n",
      "Iteration 89, loss = 0.41624203\n",
      "Iteration 90, loss = 0.41583122\n",
      "Iteration 91, loss = 0.41545095\n",
      "Iteration 92, loss = 0.41512291\n",
      "Iteration 93, loss = 0.41473021\n",
      "Iteration 94, loss = 0.41439999\n",
      "Iteration 95, loss = 0.41408232\n",
      "Iteration 96, loss = 0.41376593\n",
      "Iteration 97, loss = 0.41341281\n",
      "Iteration 98, loss = 0.41312076\n",
      "Iteration 99, loss = 0.41281053\n",
      "Iteration 100, loss = 0.41249097\n",
      "Iteration 101, loss = 0.41219678\n",
      "Iteration 102, loss = 0.41193620\n",
      "Iteration 103, loss = 0.41162957\n",
      "Iteration 104, loss = 0.41137562\n",
      "Iteration 105, loss = 0.41109381\n",
      "Iteration 106, loss = 0.41085516\n",
      "Iteration 107, loss = 0.41058099\n",
      "Iteration 108, loss = 0.41035817\n",
      "Iteration 109, loss = 0.41007731\n",
      "Iteration 110, loss = 0.40985605\n",
      "Iteration 111, loss = 0.40961247\n",
      "Iteration 112, loss = 0.40938632\n",
      "Iteration 113, loss = 0.40915958\n",
      "Iteration 114, loss = 0.40893336\n",
      "Iteration 115, loss = 0.40870813\n",
      "Iteration 116, loss = 0.40850911\n",
      "Iteration 117, loss = 0.40832004\n",
      "Iteration 118, loss = 0.40808887\n",
      "Iteration 119, loss = 0.40792147\n",
      "Iteration 120, loss = 0.40771514\n",
      "Iteration 121, loss = 0.40752871\n",
      "Iteration 122, loss = 0.40732612\n",
      "Iteration 123, loss = 0.40717833\n",
      "Iteration 124, loss = 0.40700580\n",
      "Iteration 125, loss = 0.40677250\n",
      "Iteration 126, loss = 0.40661132\n",
      "Iteration 127, loss = 0.40644870\n",
      "Iteration 128, loss = 0.40627815\n",
      "Iteration 129, loss = 0.40611429\n",
      "Iteration 130, loss = 0.40597668\n",
      "Iteration 131, loss = 0.40582088\n",
      "Iteration 132, loss = 0.40564071\n",
      "Iteration 133, loss = 0.40554144\n",
      "Iteration 134, loss = 0.40535742\n",
      "Iteration 135, loss = 0.40520039\n",
      "Iteration 136, loss = 0.40509489\n",
      "Iteration 137, loss = 0.40492082\n",
      "Iteration 138, loss = 0.40481223\n",
      "Iteration 139, loss = 0.40464237\n",
      "Iteration 140, loss = 0.40450260\n",
      "Iteration 141, loss = 0.40437558\n",
      "Iteration 142, loss = 0.40425793\n",
      "Iteration 143, loss = 0.40413527\n",
      "Iteration 144, loss = 0.40401057\n",
      "Iteration 145, loss = 0.40390495\n",
      "Iteration 146, loss = 0.40381400\n",
      "Iteration 147, loss = 0.40369560\n",
      "Iteration 148, loss = 0.40351612\n",
      "Iteration 149, loss = 0.40342058\n",
      "Iteration 150, loss = 0.40329610\n",
      "Iteration 151, loss = 0.40324935\n",
      "Iteration 152, loss = 0.40311460\n",
      "Iteration 153, loss = 0.40299409\n",
      "Iteration 154, loss = 0.40291306\n",
      "Iteration 155, loss = 0.40278273\n",
      "Iteration 156, loss = 0.40264848\n",
      "Iteration 157, loss = 0.40261342\n",
      "Iteration 158, loss = 0.40248141\n",
      "Iteration 159, loss = 0.40237226\n",
      "Iteration 160, loss = 0.40228339\n",
      "Iteration 161, loss = 0.40218129\n",
      "Iteration 162, loss = 0.40212451\n",
      "Iteration 163, loss = 0.40203084\n",
      "Iteration 164, loss = 0.40193795\n",
      "Iteration 165, loss = 0.40183749\n",
      "Iteration 166, loss = 0.40173782\n",
      "Iteration 167, loss = 0.40165596\n",
      "Iteration 168, loss = 0.40157305\n",
      "Iteration 169, loss = 0.40149247\n",
      "Iteration 170, loss = 0.40140231\n",
      "Iteration 171, loss = 0.40132240\n",
      "Iteration 172, loss = 0.40125712\n",
      "Iteration 173, loss = 0.40118906\n",
      "Iteration 174, loss = 0.40110589\n",
      "Iteration 175, loss = 0.40100776\n",
      "Iteration 176, loss = 0.40095634\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.66730588\n",
      "Iteration 2, loss = 0.63167947\n",
      "Iteration 3, loss = 0.60149760\n",
      "Iteration 4, loss = 0.57565974\n",
      "Iteration 5, loss = 0.55338334\n",
      "Iteration 6, loss = 0.53392587\n",
      "Iteration 7, loss = 0.51710678\n",
      "Iteration 8, loss = 0.50241430\n",
      "Iteration 9, loss = 0.48965953\n",
      "Iteration 10, loss = 0.47839677\n",
      "Iteration 11, loss = 0.46871175\n",
      "Iteration 12, loss = 0.46002567\n",
      "Iteration 13, loss = 0.45230430\n",
      "Iteration 14, loss = 0.44553425\n",
      "Iteration 15, loss = 0.43935483\n",
      "Iteration 16, loss = 0.43408972\n",
      "Iteration 17, loss = 0.42926002\n",
      "Iteration 18, loss = 0.42492747\n",
      "Iteration 19, loss = 0.42082091\n",
      "Iteration 20, loss = 0.41726793\n",
      "Iteration 21, loss = 0.41393651\n",
      "Iteration 22, loss = 0.41121393\n",
      "Iteration 23, loss = 0.40845833\n",
      "Iteration 24, loss = 0.40622536\n",
      "Iteration 25, loss = 0.40389753\n",
      "Iteration 26, loss = 0.40167400\n",
      "Iteration 27, loss = 0.40009550\n",
      "Iteration 28, loss = 0.39824669\n",
      "Iteration 29, loss = 0.39674831\n",
      "Iteration 30, loss = 0.39531221\n",
      "Iteration 31, loss = 0.39403384\n",
      "Iteration 32, loss = 0.39282505\n",
      "Iteration 33, loss = 0.39171726\n",
      "Iteration 34, loss = 0.39070680\n",
      "Iteration 35, loss = 0.38973274\n",
      "Iteration 36, loss = 0.38881724\n",
      "Iteration 37, loss = 0.38804000\n",
      "Iteration 38, loss = 0.38736484\n",
      "Iteration 39, loss = 0.38670839\n",
      "Iteration 40, loss = 0.38611560\n",
      "Iteration 41, loss = 0.38544463\n",
      "Iteration 42, loss = 0.38482152\n",
      "Iteration 43, loss = 0.38442529\n",
      "Iteration 44, loss = 0.38391119\n",
      "Iteration 45, loss = 0.38351153\n",
      "Iteration 46, loss = 0.38299581\n",
      "Iteration 47, loss = 0.38268211\n",
      "Iteration 48, loss = 0.38229544\n",
      "Iteration 49, loss = 0.38190424\n",
      "Iteration 50, loss = 0.38168479\n",
      "Iteration 51, loss = 0.38138662\n",
      "Iteration 52, loss = 0.38118284\n",
      "Iteration 53, loss = 0.38093909\n",
      "Iteration 54, loss = 0.38058779\n",
      "Iteration 55, loss = 0.38037936\n",
      "Iteration 56, loss = 0.38020199\n",
      "Iteration 57, loss = 0.38035242\n",
      "Iteration 58, loss = 0.37980091\n",
      "Iteration 59, loss = 0.37969747\n",
      "Iteration 60, loss = 0.37949153\n",
      "Iteration 61, loss = 0.37945528\n",
      "Iteration 62, loss = 0.37917176\n",
      "Iteration 63, loss = 0.37910724\n",
      "Iteration 64, loss = 0.37891820\n",
      "Iteration 65, loss = 0.37874633\n",
      "Iteration 66, loss = 0.37865758\n",
      "Iteration 67, loss = 0.37855320\n",
      "Iteration 68, loss = 0.37846742\n",
      "Iteration 69, loss = 0.37841861\n",
      "Iteration 70, loss = 0.37839655\n",
      "Iteration 71, loss = 0.37821967\n",
      "Iteration 72, loss = 0.37813290\n",
      "Iteration 73, loss = 0.37797856\n",
      "Iteration 74, loss = 0.37796731\n",
      "Iteration 75, loss = 0.37785927\n",
      "Iteration 76, loss = 0.37771952\n",
      "Iteration 77, loss = 0.37784397\n",
      "Iteration 78, loss = 0.37763461\n",
      "Iteration 79, loss = 0.37768098\n",
      "Iteration 80, loss = 0.37763315\n",
      "Iteration 81, loss = 0.37768821\n",
      "Iteration 82, loss = 0.37738271\n",
      "Iteration 83, loss = 0.37741264\n",
      "Iteration 84, loss = 0.37740014\n",
      "Iteration 85, loss = 0.37730988\n",
      "Iteration 86, loss = 0.37728020\n",
      "Iteration 87, loss = 0.37740296\n",
      "Iteration 88, loss = 0.37721387\n",
      "Iteration 89, loss = 0.37713517\n",
      "Iteration 90, loss = 0.37706585\n",
      "Iteration 91, loss = 0.37694868\n",
      "Iteration 92, loss = 0.37705683\n",
      "Iteration 93, loss = 0.37702348\n",
      "Iteration 94, loss = 0.37684708\n",
      "Iteration 95, loss = 0.37695947\n",
      "Iteration 96, loss = 0.37686905\n",
      "Iteration 97, loss = 0.37693763\n",
      "Iteration 98, loss = 0.37688360\n",
      "Iteration 99, loss = 0.37697587\n",
      "Iteration 100, loss = 0.37689411\n",
      "Iteration 101, loss = 0.37671309\n",
      "Iteration 102, loss = 0.37672806\n",
      "Iteration 103, loss = 0.37732489\n",
      "Iteration 104, loss = 0.37666323\n",
      "Iteration 105, loss = 0.37661274\n",
      "Iteration 106, loss = 0.37649432\n",
      "Iteration 107, loss = 0.37671653\n",
      "Iteration 108, loss = 0.37666990\n",
      "Iteration 109, loss = 0.37663801\n",
      "Iteration 110, loss = 0.37643339\n",
      "Iteration 111, loss = 0.37654144\n",
      "Iteration 112, loss = 0.37651847\n",
      "Iteration 113, loss = 0.37646228\n",
      "Iteration 114, loss = 0.37645650\n",
      "Iteration 115, loss = 0.37643242\n",
      "Iteration 116, loss = 0.37645149\n",
      "Iteration 117, loss = 0.37649656\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.66999875\n",
      "Iteration 2, loss = 0.63643715\n",
      "Iteration 3, loss = 0.60696780\n",
      "Iteration 4, loss = 0.58140057\n",
      "Iteration 5, loss = 0.55895685\n",
      "Iteration 6, loss = 0.53925042\n",
      "Iteration 7, loss = 0.52237677\n",
      "Iteration 8, loss = 0.50756326\n",
      "Iteration 9, loss = 0.49501007\n",
      "Iteration 10, loss = 0.48375053\n",
      "Iteration 11, loss = 0.47398308\n",
      "Iteration 12, loss = 0.46536494\n",
      "Iteration 13, loss = 0.45786343\n",
      "Iteration 14, loss = 0.45111433\n",
      "Iteration 15, loss = 0.44511527\n",
      "Iteration 16, loss = 0.43998116\n",
      "Iteration 17, loss = 0.43532002\n",
      "Iteration 18, loss = 0.43101705\n",
      "Iteration 19, loss = 0.42723181\n",
      "Iteration 20, loss = 0.42378443\n",
      "Iteration 21, loss = 0.42082392\n",
      "Iteration 22, loss = 0.41814924\n",
      "Iteration 23, loss = 0.41548724\n",
      "Iteration 24, loss = 0.41311489\n",
      "Iteration 25, loss = 0.41106718\n",
      "Iteration 26, loss = 0.40922429\n",
      "Iteration 27, loss = 0.40755195\n",
      "Iteration 28, loss = 0.40599215\n",
      "Iteration 29, loss = 0.40441894\n",
      "Iteration 30, loss = 0.40320899\n",
      "Iteration 31, loss = 0.40186806\n",
      "Iteration 32, loss = 0.40090047\n",
      "Iteration 33, loss = 0.39970747\n",
      "Iteration 34, loss = 0.39875301\n",
      "Iteration 35, loss = 0.39793227\n",
      "Iteration 36, loss = 0.39711625\n",
      "Iteration 37, loss = 0.39632204\n",
      "Iteration 38, loss = 0.39564906\n",
      "Iteration 39, loss = 0.39502803\n",
      "Iteration 40, loss = 0.39440714\n",
      "Iteration 41, loss = 0.39403835\n",
      "Iteration 42, loss = 0.39332389\n",
      "Iteration 43, loss = 0.39294171\n",
      "Iteration 44, loss = 0.39275217\n",
      "Iteration 45, loss = 0.39199494\n",
      "Iteration 46, loss = 0.39162001\n",
      "Iteration 47, loss = 0.39130442\n",
      "Iteration 48, loss = 0.39090462\n",
      "Iteration 49, loss = 0.39061304\n",
      "Iteration 50, loss = 0.39031849\n",
      "Iteration 51, loss = 0.39020383\n",
      "Iteration 52, loss = 0.38978780\n",
      "Iteration 53, loss = 0.38956084\n",
      "Iteration 54, loss = 0.38931133\n",
      "Iteration 55, loss = 0.38927684\n",
      "Iteration 56, loss = 0.38880247\n",
      "Iteration 57, loss = 0.38870335\n",
      "Iteration 58, loss = 0.38853787\n",
      "Iteration 59, loss = 0.38837451\n",
      "Iteration 60, loss = 0.38816160\n",
      "Iteration 61, loss = 0.38812787\n",
      "Iteration 62, loss = 0.38792010\n",
      "Iteration 63, loss = 0.38769239\n",
      "Iteration 64, loss = 0.38759428\n",
      "Iteration 65, loss = 0.38745605\n",
      "Iteration 66, loss = 0.38735031\n",
      "Iteration 67, loss = 0.38734363\n",
      "Iteration 68, loss = 0.38711260\n",
      "Iteration 69, loss = 0.38696068\n",
      "Iteration 70, loss = 0.38720332\n",
      "Iteration 71, loss = 0.38686083\n",
      "Iteration 72, loss = 0.38662287\n",
      "Iteration 73, loss = 0.38670693\n",
      "Iteration 74, loss = 0.38663379\n",
      "Iteration 75, loss = 0.38642643\n",
      "Iteration 76, loss = 0.38645707\n",
      "Iteration 77, loss = 0.38673265\n",
      "Iteration 78, loss = 0.38630696\n",
      "Iteration 79, loss = 0.38624771\n",
      "Iteration 80, loss = 0.38625631\n",
      "Iteration 81, loss = 0.38628622\n",
      "Iteration 82, loss = 0.38609462\n",
      "Iteration 83, loss = 0.38596825\n",
      "Iteration 84, loss = 0.38594331\n",
      "Iteration 85, loss = 0.38582567\n",
      "Iteration 86, loss = 0.38582940\n",
      "Iteration 87, loss = 0.38578182\n",
      "Iteration 88, loss = 0.38562376\n",
      "Iteration 89, loss = 0.38577898\n",
      "Iteration 90, loss = 0.38572816\n",
      "Iteration 91, loss = 0.38577627\n",
      "Iteration 92, loss = 0.38555661\n",
      "Iteration 93, loss = 0.38547591\n",
      "Iteration 94, loss = 0.38563881\n",
      "Iteration 95, loss = 0.38552056\n",
      "Iteration 96, loss = 0.38539300\n",
      "Iteration 97, loss = 0.38547703\n",
      "Iteration 98, loss = 0.38549134\n",
      "Iteration 99, loss = 0.38558450\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.67209004\n",
      "Iteration 2, loss = 0.63979493\n",
      "Iteration 3, loss = 0.61038368\n",
      "Iteration 4, loss = 0.58466782\n",
      "Iteration 5, loss = 0.56257612\n",
      "Iteration 6, loss = 0.54345720\n",
      "Iteration 7, loss = 0.52680157\n",
      "Iteration 8, loss = 0.51243693\n",
      "Iteration 9, loss = 0.50005250\n",
      "Iteration 10, loss = 0.48910146\n",
      "Iteration 11, loss = 0.47961230\n",
      "Iteration 12, loss = 0.47109369\n",
      "Iteration 13, loss = 0.46377443\n",
      "Iteration 14, loss = 0.45738832\n",
      "Iteration 15, loss = 0.45159886\n",
      "Iteration 16, loss = 0.44641325\n",
      "Iteration 17, loss = 0.44189018\n",
      "Iteration 18, loss = 0.43778343\n",
      "Iteration 19, loss = 0.43405271\n",
      "Iteration 20, loss = 0.43084090\n",
      "Iteration 21, loss = 0.42778875\n",
      "Iteration 22, loss = 0.42521445\n",
      "Iteration 23, loss = 0.42277690\n",
      "Iteration 24, loss = 0.42065358\n",
      "Iteration 25, loss = 0.41851780\n",
      "Iteration 26, loss = 0.41678873\n",
      "Iteration 27, loss = 0.41513656\n",
      "Iteration 28, loss = 0.41360488\n",
      "Iteration 29, loss = 0.41222229\n",
      "Iteration 30, loss = 0.41089142\n",
      "Iteration 31, loss = 0.40993697\n",
      "Iteration 32, loss = 0.40868934\n",
      "Iteration 33, loss = 0.40769867\n",
      "Iteration 34, loss = 0.40679556\n",
      "Iteration 35, loss = 0.40600705\n",
      "Iteration 36, loss = 0.40524073\n",
      "Iteration 37, loss = 0.40478945\n",
      "Iteration 38, loss = 0.40383220\n",
      "Iteration 39, loss = 0.40345908\n",
      "Iteration 40, loss = 0.40286522\n",
      "Iteration 41, loss = 0.40222555\n",
      "Iteration 42, loss = 0.40170017\n",
      "Iteration 43, loss = 0.40115960\n",
      "Iteration 44, loss = 0.40082635\n",
      "Iteration 45, loss = 0.40061083\n",
      "Iteration 46, loss = 0.40003530\n",
      "Iteration 47, loss = 0.39968825\n",
      "Iteration 48, loss = 0.39934384\n",
      "Iteration 49, loss = 0.39907816\n",
      "Iteration 50, loss = 0.39874400\n",
      "Iteration 51, loss = 0.39850990\n",
      "Iteration 52, loss = 0.39842544\n",
      "Iteration 53, loss = 0.39805631\n",
      "Iteration 54, loss = 0.39772966\n",
      "Iteration 55, loss = 0.39758254\n",
      "Iteration 56, loss = 0.39734867\n",
      "Iteration 57, loss = 0.39714917\n",
      "Iteration 58, loss = 0.39707630\n",
      "Iteration 59, loss = 0.39676988\n",
      "Iteration 60, loss = 0.39666161\n",
      "Iteration 61, loss = 0.39659763\n",
      "Iteration 62, loss = 0.39641460\n",
      "Iteration 63, loss = 0.39626527\n",
      "Iteration 64, loss = 0.39614019\n",
      "Iteration 65, loss = 0.39591040\n",
      "Iteration 66, loss = 0.39584734\n",
      "Iteration 67, loss = 0.39576495\n",
      "Iteration 68, loss = 0.39564474\n",
      "Iteration 69, loss = 0.39554581\n",
      "Iteration 70, loss = 0.39539573\n",
      "Iteration 71, loss = 0.39546554\n",
      "Iteration 72, loss = 0.39528116\n",
      "Iteration 73, loss = 0.39506519\n",
      "Iteration 74, loss = 0.39503670\n",
      "Iteration 75, loss = 0.39503881\n",
      "Iteration 76, loss = 0.39489813\n",
      "Iteration 77, loss = 0.39481787\n",
      "Iteration 78, loss = 0.39480233\n",
      "Iteration 79, loss = 0.39483736\n",
      "Iteration 80, loss = 0.39460593\n",
      "Iteration 81, loss = 0.39452834\n",
      "Iteration 82, loss = 0.39469716\n",
      "Iteration 83, loss = 0.39455556\n",
      "Iteration 84, loss = 0.39431450\n",
      "Iteration 85, loss = 0.39431231\n",
      "Iteration 86, loss = 0.39430162\n",
      "Iteration 87, loss = 0.39419099\n",
      "Iteration 88, loss = 0.39410117\n",
      "Iteration 89, loss = 0.39425601\n",
      "Iteration 90, loss = 0.39405707\n",
      "Iteration 91, loss = 0.39401265\n",
      "Iteration 92, loss = 0.39408384\n",
      "Iteration 93, loss = 0.39400185\n",
      "Iteration 94, loss = 0.39380868\n",
      "Iteration 95, loss = 0.39396357\n",
      "Iteration 96, loss = 0.39390969\n",
      "Iteration 97, loss = 0.39368967\n",
      "Iteration 98, loss = 0.39382962\n",
      "Iteration 99, loss = 0.39369155\n",
      "Iteration 100, loss = 0.39364831\n",
      "Iteration 101, loss = 0.39354795\n",
      "Iteration 102, loss = 0.39365602\n",
      "Iteration 103, loss = 0.39340730\n",
      "Iteration 104, loss = 0.39354525\n",
      "Iteration 105, loss = 0.39344807\n",
      "Iteration 106, loss = 0.39354631\n",
      "Iteration 107, loss = 0.39334861\n",
      "Iteration 108, loss = 0.39339593\n",
      "Iteration 109, loss = 0.39339864\n",
      "Iteration 110, loss = 0.39335309\n",
      "Iteration 111, loss = 0.39331863\n",
      "Iteration 112, loss = 0.39325258\n",
      "Iteration 113, loss = 0.39332316\n",
      "Iteration 114, loss = 0.39316502\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69314023\n",
      "Iteration 2, loss = 0.69228889\n",
      "Iteration 3, loss = 0.69207904\n",
      "Iteration 4, loss = 0.69189603\n",
      "Iteration 5, loss = 0.69165909\n",
      "Iteration 6, loss = 0.69144683\n",
      "Iteration 7, loss = 0.69119664\n",
      "Iteration 8, loss = 0.69090602\n",
      "Iteration 9, loss = 0.69052024\n",
      "Iteration 10, loss = 0.69011167\n",
      "Iteration 11, loss = 0.68970003\n",
      "Iteration 12, loss = 0.68923660\n",
      "Iteration 13, loss = 0.68868356\n",
      "Iteration 14, loss = 0.68806994\n",
      "Iteration 15, loss = 0.68742811\n",
      "Iteration 16, loss = 0.68663536\n",
      "Iteration 17, loss = 0.68579617\n",
      "Iteration 18, loss = 0.68492685\n",
      "Iteration 19, loss = 0.68391248\n",
      "Iteration 20, loss = 0.68281603\n",
      "Iteration 21, loss = 0.68162192\n",
      "Iteration 22, loss = 0.68026523\n",
      "Iteration 23, loss = 0.67874998\n",
      "Iteration 24, loss = 0.67715760\n",
      "Iteration 25, loss = 0.67533835\n",
      "Iteration 26, loss = 0.67330939\n",
      "Iteration 27, loss = 0.67114160\n",
      "Iteration 28, loss = 0.66875213\n",
      "Iteration 29, loss = 0.66608944\n",
      "Iteration 30, loss = 0.66318286\n",
      "Iteration 31, loss = 0.66000803\n",
      "Iteration 32, loss = 0.65645788\n",
      "Iteration 33, loss = 0.65259094\n",
      "Iteration 34, loss = 0.64832949\n",
      "Iteration 35, loss = 0.64370810\n",
      "Iteration 36, loss = 0.63866537\n",
      "Iteration 37, loss = 0.63320799\n",
      "Iteration 38, loss = 0.62735163\n",
      "Iteration 39, loss = 0.62103123\n",
      "Iteration 40, loss = 0.61434793\n",
      "Iteration 41, loss = 0.60731303\n",
      "Iteration 42, loss = 0.59977837\n",
      "Iteration 43, loss = 0.59203953\n",
      "Iteration 44, loss = 0.58404326\n",
      "Iteration 45, loss = 0.57582509\n",
      "Iteration 46, loss = 0.56747328\n",
      "Iteration 47, loss = 0.55901054\n",
      "Iteration 48, loss = 0.55046574\n",
      "Iteration 49, loss = 0.54198789\n",
      "Iteration 50, loss = 0.53355461\n",
      "Iteration 51, loss = 0.52535222\n",
      "Iteration 52, loss = 0.51725713\n",
      "Iteration 53, loss = 0.50937490\n",
      "Iteration 54, loss = 0.50174797\n",
      "Iteration 55, loss = 0.49444142\n",
      "Iteration 56, loss = 0.48742118\n",
      "Iteration 57, loss = 0.48072288\n",
      "Iteration 58, loss = 0.47435264\n",
      "Iteration 59, loss = 0.46834430\n",
      "Iteration 60, loss = 0.46269931\n",
      "Iteration 61, loss = 0.45731719\n",
      "Iteration 62, loss = 0.45224157\n",
      "Iteration 63, loss = 0.44755471\n",
      "Iteration 64, loss = 0.44311828\n",
      "Iteration 65, loss = 0.43898780\n",
      "Iteration 66, loss = 0.43508998\n",
      "Iteration 67, loss = 0.43153463\n",
      "Iteration 68, loss = 0.42815096\n",
      "Iteration 69, loss = 0.42501035\n",
      "Iteration 70, loss = 0.42214033\n",
      "Iteration 71, loss = 0.41941852\n",
      "Iteration 72, loss = 0.41686786\n",
      "Iteration 73, loss = 0.41448344\n",
      "Iteration 74, loss = 0.41223093\n",
      "Iteration 75, loss = 0.41019560\n",
      "Iteration 76, loss = 0.40825472\n",
      "Iteration 77, loss = 0.40658519\n",
      "Iteration 78, loss = 0.40489627\n",
      "Iteration 79, loss = 0.40336448\n",
      "Iteration 80, loss = 0.40197188\n",
      "Iteration 81, loss = 0.40068195\n",
      "Iteration 82, loss = 0.39941482\n",
      "Iteration 83, loss = 0.39828813\n",
      "Iteration 84, loss = 0.39706643\n",
      "Iteration 85, loss = 0.39629396\n",
      "Iteration 86, loss = 0.39520789\n",
      "Iteration 87, loss = 0.39437138\n",
      "Iteration 88, loss = 0.39353372\n",
      "Iteration 89, loss = 0.39276412\n",
      "Iteration 90, loss = 0.39210402\n",
      "Iteration 91, loss = 0.39140693\n",
      "Iteration 92, loss = 0.39079178\n",
      "Iteration 93, loss = 0.39023116\n",
      "Iteration 94, loss = 0.38970623\n",
      "Iteration 95, loss = 0.38943385\n",
      "Iteration 96, loss = 0.38875664\n",
      "Iteration 97, loss = 0.38826749\n",
      "Iteration 98, loss = 0.38798034\n",
      "Iteration 99, loss = 0.38745226\n",
      "Iteration 100, loss = 0.38710501\n",
      "Iteration 101, loss = 0.38683324\n",
      "Iteration 102, loss = 0.38648956\n",
      "Iteration 103, loss = 0.38628385\n",
      "Iteration 104, loss = 0.38581799\n",
      "Iteration 105, loss = 0.38558841\n",
      "Iteration 106, loss = 0.38524807\n",
      "Iteration 107, loss = 0.38530618\n",
      "Iteration 108, loss = 0.38479993\n",
      "Iteration 109, loss = 0.38480998\n",
      "Iteration 110, loss = 0.38442812\n",
      "Iteration 111, loss = 0.38432367\n",
      "Iteration 112, loss = 0.38402514\n",
      "Iteration 113, loss = 0.38392758\n",
      "Iteration 114, loss = 0.38374635\n",
      "Iteration 115, loss = 0.38366859\n",
      "Iteration 116, loss = 0.38349186\n",
      "Iteration 117, loss = 0.38332665\n",
      "Iteration 118, loss = 0.38321186\n",
      "Iteration 119, loss = 0.38318932\n",
      "Iteration 120, loss = 0.38297323\n",
      "Iteration 121, loss = 0.38286481\n",
      "Iteration 122, loss = 0.38275059\n",
      "Iteration 123, loss = 0.38268945\n",
      "Iteration 124, loss = 0.38255751\n",
      "Iteration 125, loss = 0.38250800\n",
      "Iteration 126, loss = 0.38231783\n",
      "Iteration 127, loss = 0.38231740\n",
      "Iteration 128, loss = 0.38229341\n",
      "Iteration 129, loss = 0.38212995\n",
      "Iteration 130, loss = 0.38207133\n",
      "Iteration 131, loss = 0.38198796\n",
      "Iteration 132, loss = 0.38200218\n",
      "Iteration 133, loss = 0.38189275\n",
      "Iteration 134, loss = 0.38174486\n",
      "Iteration 135, loss = 0.38173286\n",
      "Iteration 136, loss = 0.38167922\n",
      "Iteration 137, loss = 0.38162981\n",
      "Iteration 138, loss = 0.38156708\n",
      "Iteration 139, loss = 0.38155842\n",
      "Iteration 140, loss = 0.38150709\n",
      "Iteration 141, loss = 0.38138101\n",
      "Iteration 142, loss = 0.38136061\n",
      "Iteration 143, loss = 0.38127069\n",
      "Iteration 144, loss = 0.38128766\n",
      "Iteration 145, loss = 0.38118173\n",
      "Iteration 146, loss = 0.38119633\n",
      "Iteration 147, loss = 0.38112393\n",
      "Iteration 148, loss = 0.38109234\n",
      "Iteration 149, loss = 0.38108655\n",
      "Iteration 150, loss = 0.38107240\n",
      "Iteration 151, loss = 0.38097380\n",
      "Iteration 152, loss = 0.38089359\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69299712\n",
      "Iteration 2, loss = 0.69232412\n",
      "Iteration 3, loss = 0.69208932\n",
      "Iteration 4, loss = 0.69191288\n",
      "Iteration 5, loss = 0.69169086\n",
      "Iteration 6, loss = 0.69146389\n",
      "Iteration 7, loss = 0.69124142\n",
      "Iteration 8, loss = 0.69096860\n",
      "Iteration 9, loss = 0.69056068\n",
      "Iteration 10, loss = 0.69018409\n",
      "Iteration 11, loss = 0.68976991\n",
      "Iteration 12, loss = 0.68930064\n",
      "Iteration 13, loss = 0.68876544\n",
      "Iteration 14, loss = 0.68814347\n",
      "Iteration 15, loss = 0.68754072\n",
      "Iteration 16, loss = 0.68677417\n",
      "Iteration 17, loss = 0.68594998\n",
      "Iteration 18, loss = 0.68509779\n",
      "Iteration 19, loss = 0.68408361\n",
      "Iteration 20, loss = 0.68304799\n",
      "Iteration 21, loss = 0.68184394\n",
      "Iteration 22, loss = 0.68053487\n",
      "Iteration 23, loss = 0.67905726\n",
      "Iteration 24, loss = 0.67746861\n",
      "Iteration 25, loss = 0.67571349\n",
      "Iteration 26, loss = 0.67383955\n",
      "Iteration 27, loss = 0.67166025\n",
      "Iteration 28, loss = 0.66934749\n",
      "Iteration 29, loss = 0.66677720\n",
      "Iteration 30, loss = 0.66394146\n",
      "Iteration 31, loss = 0.66080589\n",
      "Iteration 32, loss = 0.65740902\n",
      "Iteration 33, loss = 0.65377399\n",
      "Iteration 34, loss = 0.64956651\n",
      "Iteration 35, loss = 0.64513115\n",
      "Iteration 36, loss = 0.64021744\n",
      "Iteration 37, loss = 0.63495438\n",
      "Iteration 38, loss = 0.62927057\n",
      "Iteration 39, loss = 0.62322601\n",
      "Iteration 40, loss = 0.61677061\n",
      "Iteration 41, loss = 0.61002685\n",
      "Iteration 42, loss = 0.60277995\n",
      "Iteration 43, loss = 0.59527688\n",
      "Iteration 44, loss = 0.58753878\n",
      "Iteration 45, loss = 0.57959343\n",
      "Iteration 46, loss = 0.57149254\n",
      "Iteration 47, loss = 0.56331778\n",
      "Iteration 48, loss = 0.55504804\n",
      "Iteration 49, loss = 0.54684175\n",
      "Iteration 50, loss = 0.53863874\n",
      "Iteration 51, loss = 0.53059749\n",
      "Iteration 52, loss = 0.52273918\n",
      "Iteration 53, loss = 0.51511248\n",
      "Iteration 54, loss = 0.50769655\n",
      "Iteration 55, loss = 0.50058624\n",
      "Iteration 56, loss = 0.49371943\n",
      "Iteration 57, loss = 0.48712118\n",
      "Iteration 58, loss = 0.48095254\n",
      "Iteration 59, loss = 0.47513755\n",
      "Iteration 60, loss = 0.46952974\n",
      "Iteration 61, loss = 0.46422604\n",
      "Iteration 62, loss = 0.45933321\n",
      "Iteration 63, loss = 0.45465915\n",
      "Iteration 64, loss = 0.45030548\n",
      "Iteration 65, loss = 0.44626653\n",
      "Iteration 66, loss = 0.44250184\n",
      "Iteration 67, loss = 0.43899739\n",
      "Iteration 68, loss = 0.43560020\n",
      "Iteration 69, loss = 0.43251207\n",
      "Iteration 70, loss = 0.42961625\n",
      "Iteration 71, loss = 0.42700170\n",
      "Iteration 72, loss = 0.42439003\n",
      "Iteration 73, loss = 0.42213671\n",
      "Iteration 74, loss = 0.41997425\n",
      "Iteration 75, loss = 0.41792184\n",
      "Iteration 76, loss = 0.41606700\n",
      "Iteration 77, loss = 0.41440913\n",
      "Iteration 78, loss = 0.41264379\n",
      "Iteration 79, loss = 0.41111184\n",
      "Iteration 80, loss = 0.40972103\n",
      "Iteration 81, loss = 0.40839661\n",
      "Iteration 82, loss = 0.40736217\n",
      "Iteration 83, loss = 0.40629253\n",
      "Iteration 84, loss = 0.40504857\n",
      "Iteration 85, loss = 0.40408355\n",
      "Iteration 86, loss = 0.40307060\n",
      "Iteration 87, loss = 0.40219971\n",
      "Iteration 88, loss = 0.40135085\n",
      "Iteration 89, loss = 0.40060241\n",
      "Iteration 90, loss = 0.40002868\n",
      "Iteration 91, loss = 0.39928094\n",
      "Iteration 92, loss = 0.39865939\n",
      "Iteration 93, loss = 0.39815566\n",
      "Iteration 94, loss = 0.39759262\n",
      "Iteration 95, loss = 0.39719499\n",
      "Iteration 96, loss = 0.39654179\n",
      "Iteration 97, loss = 0.39618036\n",
      "Iteration 98, loss = 0.39573464\n",
      "Iteration 99, loss = 0.39542485\n",
      "Iteration 100, loss = 0.39503089\n",
      "Iteration 101, loss = 0.39463064\n",
      "Iteration 102, loss = 0.39431595\n",
      "Iteration 103, loss = 0.39408406\n",
      "Iteration 104, loss = 0.39377788\n",
      "Iteration 105, loss = 0.39347645\n",
      "Iteration 106, loss = 0.39315236\n",
      "Iteration 107, loss = 0.39323577\n",
      "Iteration 108, loss = 0.39280154\n",
      "Iteration 109, loss = 0.39262633\n",
      "Iteration 110, loss = 0.39243710\n",
      "Iteration 111, loss = 0.39226348\n",
      "Iteration 112, loss = 0.39196540\n",
      "Iteration 113, loss = 0.39187698\n",
      "Iteration 114, loss = 0.39163288\n",
      "Iteration 115, loss = 0.39157092\n",
      "Iteration 116, loss = 0.39158643\n",
      "Iteration 117, loss = 0.39128316\n",
      "Iteration 118, loss = 0.39113263\n",
      "Iteration 119, loss = 0.39103950\n",
      "Iteration 120, loss = 0.39091675\n",
      "Iteration 121, loss = 0.39073106\n",
      "Iteration 122, loss = 0.39060481\n",
      "Iteration 123, loss = 0.39062056\n",
      "Iteration 124, loss = 0.39044091\n",
      "Iteration 125, loss = 0.39038133\n",
      "Iteration 126, loss = 0.39030863\n",
      "Iteration 127, loss = 0.39017459\n",
      "Iteration 128, loss = 0.39012080\n",
      "Iteration 129, loss = 0.39001661\n",
      "Iteration 130, loss = 0.39000851\n",
      "Iteration 131, loss = 0.38992028\n",
      "Iteration 132, loss = 0.38980504\n",
      "Iteration 133, loss = 0.38989503\n",
      "Iteration 134, loss = 0.38974980\n",
      "Iteration 135, loss = 0.38968153\n",
      "Iteration 136, loss = 0.38953787\n",
      "Iteration 137, loss = 0.38950618\n",
      "Iteration 138, loss = 0.38947783\n",
      "Iteration 139, loss = 0.38942720\n",
      "Iteration 140, loss = 0.38926285\n",
      "Iteration 141, loss = 0.38926522\n",
      "Iteration 142, loss = 0.38932236\n",
      "Iteration 143, loss = 0.38913341\n",
      "Iteration 144, loss = 0.38917788\n",
      "Iteration 145, loss = 0.38907637\n",
      "Iteration 146, loss = 0.38904228\n",
      "Iteration 147, loss = 0.38895905\n",
      "Iteration 148, loss = 0.38901192\n",
      "Iteration 149, loss = 0.38891937\n",
      "Iteration 150, loss = 0.38886665\n",
      "Iteration 151, loss = 0.38886388\n",
      "Iteration 152, loss = 0.38878400\n",
      "Iteration 153, loss = 0.38879676\n",
      "Iteration 154, loss = 0.38870681\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69290217\n",
      "Iteration 2, loss = 0.69228937\n",
      "Iteration 3, loss = 0.69212296\n",
      "Iteration 4, loss = 0.69194071\n",
      "Iteration 5, loss = 0.69174784\n",
      "Iteration 6, loss = 0.69158023\n",
      "Iteration 7, loss = 0.69122146\n",
      "Iteration 8, loss = 0.69095664\n",
      "Iteration 9, loss = 0.69069945\n",
      "Iteration 10, loss = 0.69031453\n",
      "Iteration 11, loss = 0.68987529\n",
      "Iteration 12, loss = 0.68942700\n",
      "Iteration 13, loss = 0.68888461\n",
      "Iteration 14, loss = 0.68830069\n",
      "Iteration 15, loss = 0.68767211\n",
      "Iteration 16, loss = 0.68702361\n",
      "Iteration 17, loss = 0.68620365\n",
      "Iteration 18, loss = 0.68538710\n",
      "Iteration 19, loss = 0.68440304\n",
      "Iteration 20, loss = 0.68339852\n",
      "Iteration 21, loss = 0.68219269\n",
      "Iteration 22, loss = 0.68093181\n",
      "Iteration 23, loss = 0.67957576\n",
      "Iteration 24, loss = 0.67804901\n",
      "Iteration 25, loss = 0.67637500\n",
      "Iteration 26, loss = 0.67454594\n",
      "Iteration 27, loss = 0.67250605\n",
      "Iteration 28, loss = 0.67034469\n",
      "Iteration 29, loss = 0.66784161\n",
      "Iteration 30, loss = 0.66517029\n",
      "Iteration 31, loss = 0.66224812\n",
      "Iteration 32, loss = 0.65901371\n",
      "Iteration 33, loss = 0.65543075\n",
      "Iteration 34, loss = 0.65156479\n",
      "Iteration 35, loss = 0.64735813\n",
      "Iteration 36, loss = 0.64275339\n",
      "Iteration 37, loss = 0.63774749\n",
      "Iteration 38, loss = 0.63247612\n",
      "Iteration 39, loss = 0.62669928\n",
      "Iteration 40, loss = 0.62056708\n",
      "Iteration 41, loss = 0.61404375\n",
      "Iteration 42, loss = 0.60725580\n",
      "Iteration 43, loss = 0.60011914\n",
      "Iteration 44, loss = 0.59273932\n",
      "Iteration 45, loss = 0.58513015\n",
      "Iteration 46, loss = 0.57736542\n",
      "Iteration 47, loss = 0.56945736\n",
      "Iteration 48, loss = 0.56149437\n",
      "Iteration 49, loss = 0.55361367\n",
      "Iteration 50, loss = 0.54575040\n",
      "Iteration 51, loss = 0.53782322\n",
      "Iteration 52, loss = 0.53016599\n",
      "Iteration 53, loss = 0.52270913\n",
      "Iteration 54, loss = 0.51549288\n",
      "Iteration 55, loss = 0.50843950\n",
      "Iteration 56, loss = 0.50175018\n",
      "Iteration 57, loss = 0.49531288\n",
      "Iteration 58, loss = 0.48927589\n",
      "Iteration 59, loss = 0.48349533\n",
      "Iteration 60, loss = 0.47799830\n",
      "Iteration 61, loss = 0.47278571\n",
      "Iteration 62, loss = 0.46786251\n",
      "Iteration 63, loss = 0.46329710\n",
      "Iteration 64, loss = 0.45895947\n",
      "Iteration 65, loss = 0.45499372\n",
      "Iteration 66, loss = 0.45117086\n",
      "Iteration 67, loss = 0.44762567\n",
      "Iteration 68, loss = 0.44432928\n",
      "Iteration 69, loss = 0.44132310\n",
      "Iteration 70, loss = 0.43844079\n",
      "Iteration 71, loss = 0.43579081\n",
      "Iteration 72, loss = 0.43332718\n",
      "Iteration 73, loss = 0.43104325\n",
      "Iteration 74, loss = 0.42888395\n",
      "Iteration 75, loss = 0.42689893\n",
      "Iteration 76, loss = 0.42504872\n",
      "Iteration 77, loss = 0.42326952\n",
      "Iteration 78, loss = 0.42168669\n",
      "Iteration 79, loss = 0.42015908\n",
      "Iteration 80, loss = 0.41875026\n",
      "Iteration 81, loss = 0.41749997\n",
      "Iteration 82, loss = 0.41627140\n",
      "Iteration 83, loss = 0.41515330\n",
      "Iteration 84, loss = 0.41415221\n",
      "Iteration 85, loss = 0.41317328\n",
      "Iteration 86, loss = 0.41221190\n",
      "Iteration 87, loss = 0.41145870\n",
      "Iteration 88, loss = 0.41059643\n",
      "Iteration 89, loss = 0.40989450\n",
      "Iteration 90, loss = 0.40915346\n",
      "Iteration 91, loss = 0.40851743\n",
      "Iteration 92, loss = 0.40789970\n",
      "Iteration 93, loss = 0.40729891\n",
      "Iteration 94, loss = 0.40678883\n",
      "Iteration 95, loss = 0.40628748\n",
      "Iteration 96, loss = 0.40586920\n",
      "Iteration 97, loss = 0.40553488\n",
      "Iteration 98, loss = 0.40501699\n",
      "Iteration 99, loss = 0.40462615\n",
      "Iteration 100, loss = 0.40426416\n",
      "Iteration 101, loss = 0.40393294\n",
      "Iteration 102, loss = 0.40361498\n",
      "Iteration 103, loss = 0.40330074\n",
      "Iteration 104, loss = 0.40298479\n",
      "Iteration 105, loss = 0.40271530\n",
      "Iteration 106, loss = 0.40255388\n",
      "Iteration 107, loss = 0.40237980\n",
      "Iteration 108, loss = 0.40206474\n",
      "Iteration 109, loss = 0.40185659\n",
      "Iteration 110, loss = 0.40156396\n",
      "Iteration 111, loss = 0.40144099\n",
      "Iteration 112, loss = 0.40125760\n",
      "Iteration 113, loss = 0.40111644\n",
      "Iteration 114, loss = 0.40088993\n",
      "Iteration 115, loss = 0.40086045\n",
      "Iteration 116, loss = 0.40059509\n",
      "Iteration 117, loss = 0.40043491\n",
      "Iteration 118, loss = 0.40029759\n",
      "Iteration 119, loss = 0.40024088\n",
      "Iteration 120, loss = 0.40006580\n",
      "Iteration 121, loss = 0.40002845\n",
      "Iteration 122, loss = 0.39988344\n",
      "Iteration 123, loss = 0.39980980\n",
      "Iteration 124, loss = 0.39964247\n",
      "Iteration 125, loss = 0.39949671\n",
      "Iteration 126, loss = 0.39940388\n",
      "Iteration 127, loss = 0.39939427\n",
      "Iteration 128, loss = 0.39926297\n",
      "Iteration 129, loss = 0.39912520\n",
      "Iteration 130, loss = 0.39905273\n",
      "Iteration 131, loss = 0.39901838\n",
      "Iteration 132, loss = 0.39891405\n",
      "Iteration 133, loss = 0.39891217\n",
      "Iteration 134, loss = 0.39876276\n",
      "Iteration 135, loss = 0.39873766\n",
      "Iteration 136, loss = 0.39864063\n",
      "Iteration 137, loss = 0.39863443\n",
      "Iteration 138, loss = 0.39847246\n",
      "Iteration 139, loss = 0.39844533\n",
      "Iteration 140, loss = 0.39841245\n",
      "Iteration 141, loss = 0.39835247\n",
      "Iteration 142, loss = 0.39827666\n",
      "Iteration 143, loss = 0.39821692\n",
      "Iteration 144, loss = 0.39818229\n",
      "Iteration 145, loss = 0.39822975\n",
      "Iteration 146, loss = 0.39799561\n",
      "Iteration 147, loss = 0.39807929\n",
      "Iteration 148, loss = 0.39783834\n",
      "Iteration 149, loss = 0.39790135\n",
      "Iteration 150, loss = 0.39789318\n",
      "Iteration 151, loss = 0.39802215\n",
      "Iteration 152, loss = 0.39776375\n",
      "Iteration 153, loss = 0.39780189\n",
      "Iteration 154, loss = 0.39764676\n",
      "Iteration 155, loss = 0.39757902\n",
      "Iteration 156, loss = 0.39752240\n",
      "Iteration 157, loss = 0.39760038\n",
      "Iteration 158, loss = 0.39755992\n",
      "Iteration 159, loss = 0.39752563\n",
      "Iteration 160, loss = 0.39740843\n",
      "Iteration 161, loss = 0.39732707\n",
      "Iteration 162, loss = 0.39734872\n",
      "Iteration 163, loss = 0.39720620\n",
      "Iteration 164, loss = 0.39743699\n",
      "Iteration 165, loss = 0.39728475\n",
      "Iteration 166, loss = 0.39720732\n",
      "Iteration 167, loss = 0.39709884\n",
      "Iteration 168, loss = 0.39717971\n",
      "Iteration 169, loss = 0.39704512\n",
      "Iteration 170, loss = 0.39697566\n",
      "Iteration 171, loss = 0.39702821\n",
      "Iteration 172, loss = 0.39704048\n",
      "Iteration 173, loss = 0.39691534\n",
      "Iteration 174, loss = 0.39683096\n",
      "Iteration 175, loss = 0.39691511\n",
      "Iteration 176, loss = 0.39702991\n",
      "Iteration 177, loss = 0.39683030\n",
      "Iteration 178, loss = 0.39674924\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69175526\n",
      "Iteration 2, loss = 0.68608228\n",
      "Iteration 3, loss = 0.67234236\n",
      "Iteration 4, loss = 0.64517209\n",
      "Iteration 5, loss = 0.60697965\n",
      "Iteration 6, loss = 0.56448449\n",
      "Iteration 7, loss = 0.52559559\n",
      "Iteration 8, loss = 0.49248835\n",
      "Iteration 9, loss = 0.46598128\n",
      "Iteration 10, loss = 0.44593201\n",
      "Iteration 11, loss = 0.43071314\n",
      "Iteration 12, loss = 0.41926794\n",
      "Iteration 13, loss = 0.41096074\n",
      "Iteration 14, loss = 0.40416150\n",
      "Iteration 15, loss = 0.39940231\n",
      "Iteration 16, loss = 0.39574318\n",
      "Iteration 17, loss = 0.39293143\n",
      "Iteration 18, loss = 0.39097857\n",
      "Iteration 19, loss = 0.38934011\n",
      "Iteration 20, loss = 0.38791671\n",
      "Iteration 21, loss = 0.38689086\n",
      "Iteration 22, loss = 0.38600690\n",
      "Iteration 23, loss = 0.38548171\n",
      "Iteration 24, loss = 0.38500581\n",
      "Iteration 25, loss = 0.38437506\n",
      "Iteration 26, loss = 0.38436058\n",
      "Iteration 27, loss = 0.38403796\n",
      "Iteration 28, loss = 0.38366109\n",
      "Iteration 29, loss = 0.38352266\n",
      "Iteration 30, loss = 0.38314397\n",
      "Iteration 31, loss = 0.38312191\n",
      "Iteration 32, loss = 0.38307489\n",
      "Iteration 33, loss = 0.38306523\n",
      "Iteration 34, loss = 0.38261136\n",
      "Iteration 35, loss = 0.38259801\n",
      "Iteration 36, loss = 0.38257639\n",
      "Iteration 37, loss = 0.38239569\n",
      "Iteration 38, loss = 0.38225890\n",
      "Iteration 39, loss = 0.38205846\n",
      "Iteration 40, loss = 0.38219074\n",
      "Iteration 41, loss = 0.38204011\n",
      "Iteration 42, loss = 0.38191987\n",
      "Iteration 43, loss = 0.38187072\n",
      "Iteration 44, loss = 0.38188188\n",
      "Iteration 45, loss = 0.38170765\n",
      "Iteration 46, loss = 0.38159217\n",
      "Iteration 47, loss = 0.38166420\n",
      "Iteration 48, loss = 0.38137250\n",
      "Iteration 49, loss = 0.38151765\n",
      "Iteration 50, loss = 0.38147095\n",
      "Iteration 51, loss = 0.38120950\n",
      "Iteration 52, loss = 0.38168983\n",
      "Iteration 53, loss = 0.38137616\n",
      "Iteration 54, loss = 0.38113044\n",
      "Iteration 55, loss = 0.38087197\n",
      "Iteration 56, loss = 0.38113794\n",
      "Iteration 57, loss = 0.38103463\n",
      "Iteration 58, loss = 0.38086963\n",
      "Iteration 59, loss = 0.38089574\n",
      "Iteration 60, loss = 0.38076886\n",
      "Iteration 61, loss = 0.38074637\n",
      "Iteration 62, loss = 0.38072687\n",
      "Iteration 63, loss = 0.38067354\n",
      "Iteration 64, loss = 0.38080946\n",
      "Iteration 65, loss = 0.38058983\n",
      "Iteration 66, loss = 0.38052431\n",
      "Iteration 67, loss = 0.38057668\n",
      "Iteration 68, loss = 0.38057943\n",
      "Iteration 69, loss = 0.38033566\n",
      "Iteration 70, loss = 0.38045216\n",
      "Iteration 71, loss = 0.38069931\n",
      "Iteration 72, loss = 0.38059824\n",
      "Iteration 73, loss = 0.38036194\n",
      "Iteration 74, loss = 0.38014798\n",
      "Iteration 75, loss = 0.38014827\n",
      "Iteration 76, loss = 0.37999124\n",
      "Iteration 77, loss = 0.38032340\n",
      "Iteration 78, loss = 0.38006332\n",
      "Iteration 79, loss = 0.38016161\n",
      "Iteration 80, loss = 0.38045084\n",
      "Iteration 81, loss = 0.37975271\n",
      "Iteration 82, loss = 0.37993630\n",
      "Iteration 83, loss = 0.37994249\n",
      "Iteration 84, loss = 0.38000610\n",
      "Iteration 85, loss = 0.38022348\n",
      "Iteration 86, loss = 0.37992000\n",
      "Iteration 87, loss = 0.37991901\n",
      "Iteration 88, loss = 0.37987802\n",
      "Iteration 89, loss = 0.37986707\n",
      "Iteration 90, loss = 0.37994590\n",
      "Iteration 91, loss = 0.37995467\n",
      "Iteration 92, loss = 0.37983313\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69160097\n",
      "Iteration 2, loss = 0.68636470\n",
      "Iteration 3, loss = 0.67288104\n",
      "Iteration 4, loss = 0.64535666\n",
      "Iteration 5, loss = 0.60551294\n",
      "Iteration 6, loss = 0.56229114\n",
      "Iteration 7, loss = 0.52317782\n",
      "Iteration 8, loss = 0.49163688\n",
      "Iteration 9, loss = 0.46729818\n",
      "Iteration 10, loss = 0.44890314\n",
      "Iteration 11, loss = 0.43503980\n",
      "Iteration 12, loss = 0.42438051\n",
      "Iteration 13, loss = 0.41682390\n",
      "Iteration 14, loss = 0.41072651\n",
      "Iteration 15, loss = 0.40631899\n",
      "Iteration 16, loss = 0.40292738\n",
      "Iteration 17, loss = 0.40024790\n",
      "Iteration 18, loss = 0.39856323\n",
      "Iteration 19, loss = 0.39665992\n",
      "Iteration 20, loss = 0.39569843\n",
      "Iteration 21, loss = 0.39477477\n",
      "Iteration 22, loss = 0.39365893\n",
      "Iteration 23, loss = 0.39315499\n",
      "Iteration 24, loss = 0.39277429\n",
      "Iteration 25, loss = 0.39217624\n",
      "Iteration 26, loss = 0.39217984\n",
      "Iteration 27, loss = 0.39158883\n",
      "Iteration 28, loss = 0.39125114\n",
      "Iteration 29, loss = 0.39126775\n",
      "Iteration 30, loss = 0.39098609\n",
      "Iteration 31, loss = 0.39080490\n",
      "Iteration 32, loss = 0.39067291\n",
      "Iteration 33, loss = 0.39074669\n",
      "Iteration 34, loss = 0.39045514\n",
      "Iteration 35, loss = 0.39039657\n",
      "Iteration 36, loss = 0.39030802\n",
      "Iteration 37, loss = 0.39033748\n",
      "Iteration 38, loss = 0.39004821\n",
      "Iteration 39, loss = 0.38980397\n",
      "Iteration 40, loss = 0.38990547\n",
      "Iteration 41, loss = 0.39007200\n",
      "Iteration 42, loss = 0.39046244\n",
      "Iteration 43, loss = 0.38997232\n",
      "Iteration 44, loss = 0.38962161\n",
      "Iteration 45, loss = 0.38936648\n",
      "Iteration 46, loss = 0.38924229\n",
      "Iteration 47, loss = 0.38933890\n",
      "Iteration 48, loss = 0.38913427\n",
      "Iteration 49, loss = 0.38940589\n",
      "Iteration 50, loss = 0.38920689\n",
      "Iteration 51, loss = 0.38894972\n",
      "Iteration 52, loss = 0.38896308\n",
      "Iteration 53, loss = 0.38871369\n",
      "Iteration 54, loss = 0.38878186\n",
      "Iteration 55, loss = 0.38884067\n",
      "Iteration 56, loss = 0.38859358\n",
      "Iteration 57, loss = 0.38848825\n",
      "Iteration 58, loss = 0.38882932\n",
      "Iteration 59, loss = 0.38850304\n",
      "Iteration 60, loss = 0.38826704\n",
      "Iteration 61, loss = 0.38824199\n",
      "Iteration 62, loss = 0.38834253\n",
      "Iteration 63, loss = 0.38813885\n",
      "Iteration 64, loss = 0.38807834\n",
      "Iteration 65, loss = 0.38818126\n",
      "Iteration 66, loss = 0.38841430\n",
      "Iteration 67, loss = 0.38828848\n",
      "Iteration 68, loss = 0.38810896\n",
      "Iteration 69, loss = 0.38789052\n",
      "Iteration 70, loss = 0.38783686\n",
      "Iteration 71, loss = 0.38817661\n",
      "Iteration 72, loss = 0.38775156\n",
      "Iteration 73, loss = 0.38790902\n",
      "Iteration 74, loss = 0.38832487\n",
      "Iteration 75, loss = 0.38789590\n",
      "Iteration 76, loss = 0.38774987\n",
      "Iteration 77, loss = 0.38807852\n",
      "Iteration 78, loss = 0.38781464\n",
      "Iteration 79, loss = 0.38765677\n",
      "Iteration 80, loss = 0.38788516\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69139547\n",
      "Iteration 2, loss = 0.68553815\n",
      "Iteration 3, loss = 0.67147244\n",
      "Iteration 4, loss = 0.64409372\n",
      "Iteration 5, loss = 0.60586320\n",
      "Iteration 6, loss = 0.56541587\n",
      "Iteration 7, loss = 0.52858693\n",
      "Iteration 8, loss = 0.49833520\n",
      "Iteration 9, loss = 0.47458960\n",
      "Iteration 10, loss = 0.45633522\n",
      "Iteration 11, loss = 0.44297879\n",
      "Iteration 12, loss = 0.43241784\n",
      "Iteration 13, loss = 0.42479722\n",
      "Iteration 14, loss = 0.41898725\n",
      "Iteration 15, loss = 0.41470993\n",
      "Iteration 16, loss = 0.41136684\n",
      "Iteration 17, loss = 0.40887230\n",
      "Iteration 18, loss = 0.40730908\n",
      "Iteration 19, loss = 0.40537536\n",
      "Iteration 20, loss = 0.40431807\n",
      "Iteration 21, loss = 0.40336555\n",
      "Iteration 22, loss = 0.40294288\n",
      "Iteration 23, loss = 0.40196139\n",
      "Iteration 24, loss = 0.40205402\n",
      "Iteration 25, loss = 0.40145687\n",
      "Iteration 26, loss = 0.40088440\n",
      "Iteration 27, loss = 0.40057352\n",
      "Iteration 28, loss = 0.40010267\n",
      "Iteration 29, loss = 0.39993501\n",
      "Iteration 30, loss = 0.39984650\n",
      "Iteration 31, loss = 0.39951820\n",
      "Iteration 32, loss = 0.39955005\n",
      "Iteration 33, loss = 0.39922645\n",
      "Iteration 34, loss = 0.39895584\n",
      "Iteration 35, loss = 0.39872725\n",
      "Iteration 36, loss = 0.39903652\n",
      "Iteration 37, loss = 0.39860309\n",
      "Iteration 38, loss = 0.39855955\n",
      "Iteration 39, loss = 0.39840413\n",
      "Iteration 40, loss = 0.39832213\n",
      "Iteration 41, loss = 0.39803833\n",
      "Iteration 42, loss = 0.39827838\n",
      "Iteration 43, loss = 0.39821938\n",
      "Iteration 44, loss = 0.39774256\n",
      "Iteration 45, loss = 0.39826533\n",
      "Iteration 46, loss = 0.39775748\n",
      "Iteration 47, loss = 0.39745943\n",
      "Iteration 48, loss = 0.39750821\n",
      "Iteration 49, loss = 0.39741913\n",
      "Iteration 50, loss = 0.39763218\n",
      "Iteration 51, loss = 0.39765176\n",
      "Iteration 52, loss = 0.39703132\n",
      "Iteration 53, loss = 0.39699713\n",
      "Iteration 54, loss = 0.39719196\n",
      "Iteration 55, loss = 0.39696078\n",
      "Iteration 56, loss = 0.39683825\n",
      "Iteration 57, loss = 0.39677660\n",
      "Iteration 58, loss = 0.39676403\n",
      "Iteration 59, loss = 0.39680877\n",
      "Iteration 60, loss = 0.39688892\n",
      "Iteration 61, loss = 0.39673316\n",
      "Iteration 62, loss = 0.39654618\n",
      "Iteration 63, loss = 0.39656999\n",
      "Iteration 64, loss = 0.39630380\n",
      "Iteration 65, loss = 0.39654170\n",
      "Iteration 66, loss = 0.39631601\n",
      "Iteration 67, loss = 0.39641269\n",
      "Iteration 68, loss = 0.39618064\n",
      "Iteration 69, loss = 0.39633735\n",
      "Iteration 70, loss = 0.39614476\n",
      "Iteration 71, loss = 0.39614814\n",
      "Iteration 72, loss = 0.39601546\n",
      "Iteration 73, loss = 0.39631914\n",
      "Iteration 74, loss = 0.39605681\n",
      "Iteration 75, loss = 0.39611191\n",
      "Iteration 76, loss = 0.39593405\n",
      "Iteration 77, loss = 0.39578774\n",
      "Iteration 78, loss = 0.39578728\n",
      "Iteration 79, loss = 0.39562889\n",
      "Iteration 80, loss = 0.39563940\n",
      "Iteration 81, loss = 0.39583615\n",
      "Iteration 82, loss = 0.39573789\n",
      "Iteration 83, loss = 0.39553391\n",
      "Iteration 84, loss = 0.39589355\n",
      "Iteration 85, loss = 0.39613198\n",
      "Iteration 86, loss = 0.39566078\n",
      "Iteration 87, loss = 0.39568019\n",
      "Iteration 88, loss = 0.39563834\n",
      "Iteration 89, loss = 0.39595021\n",
      "Iteration 90, loss = 0.39547856\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.73936050\n",
      "Iteration 2, loss = 0.69346295\n",
      "Iteration 3, loss = 0.69286193\n",
      "Iteration 4, loss = 0.69241185\n",
      "Iteration 5, loss = 0.69197547\n",
      "Iteration 6, loss = 0.69149947\n",
      "Iteration 7, loss = 0.69099091\n",
      "Iteration 8, loss = 0.69038717\n",
      "Iteration 9, loss = 0.68974772\n",
      "Iteration 10, loss = 0.68901217\n",
      "Iteration 11, loss = 0.68814371\n",
      "Iteration 12, loss = 0.68719552\n",
      "Iteration 13, loss = 0.68605252\n",
      "Iteration 14, loss = 0.68485091\n",
      "Iteration 15, loss = 0.68340624\n",
      "Iteration 16, loss = 0.68182852\n",
      "Iteration 17, loss = 0.68008602\n",
      "Iteration 18, loss = 0.67801043\n",
      "Iteration 19, loss = 0.67568362\n",
      "Iteration 20, loss = 0.67317388\n",
      "Iteration 21, loss = 0.67036826\n",
      "Iteration 22, loss = 0.66720674\n",
      "Iteration 23, loss = 0.66375403\n",
      "Iteration 24, loss = 0.65991371\n",
      "Iteration 25, loss = 0.65571410\n",
      "Iteration 26, loss = 0.65115569\n",
      "Iteration 27, loss = 0.64611604\n",
      "Iteration 28, loss = 0.64065141\n",
      "Iteration 29, loss = 0.63473415\n",
      "Iteration 30, loss = 0.62836326\n",
      "Iteration 31, loss = 0.62147239\n",
      "Iteration 32, loss = 0.61411056\n",
      "Iteration 33, loss = 0.60629343\n",
      "Iteration 34, loss = 0.59802285\n",
      "Iteration 35, loss = 0.58939603\n",
      "Iteration 36, loss = 0.58029991\n",
      "Iteration 37, loss = 0.57096600\n",
      "Iteration 38, loss = 0.56139748\n",
      "Iteration 39, loss = 0.55167543\n",
      "Iteration 40, loss = 0.54194598\n",
      "Iteration 41, loss = 0.53213748\n",
      "Iteration 42, loss = 0.52259622\n",
      "Iteration 43, loss = 0.51314086\n",
      "Iteration 44, loss = 0.50405121\n",
      "Iteration 45, loss = 0.49518691\n",
      "Iteration 46, loss = 0.48686505\n",
      "Iteration 47, loss = 0.47890905\n",
      "Iteration 48, loss = 0.47132574\n",
      "Iteration 49, loss = 0.46425421\n",
      "Iteration 50, loss = 0.45755571\n",
      "Iteration 51, loss = 0.45141263\n",
      "Iteration 52, loss = 0.44577172\n",
      "Iteration 53, loss = 0.44040530\n",
      "Iteration 54, loss = 0.43555615\n",
      "Iteration 55, loss = 0.43097794\n",
      "Iteration 56, loss = 0.42686909\n",
      "Iteration 57, loss = 0.42304843\n",
      "Iteration 58, loss = 0.41953556\n",
      "Iteration 59, loss = 0.41629455\n",
      "Iteration 60, loss = 0.41337391\n",
      "Iteration 61, loss = 0.41064991\n",
      "Iteration 62, loss = 0.40818162\n",
      "Iteration 63, loss = 0.40595549\n",
      "Iteration 64, loss = 0.40392185\n",
      "Iteration 65, loss = 0.40196382\n",
      "Iteration 66, loss = 0.40029665\n",
      "Iteration 67, loss = 0.39871398\n",
      "Iteration 68, loss = 0.39726461\n",
      "Iteration 69, loss = 0.39590139\n",
      "Iteration 70, loss = 0.39464077\n",
      "Iteration 71, loss = 0.39353056\n",
      "Iteration 72, loss = 0.39247658\n",
      "Iteration 73, loss = 0.39161037\n",
      "Iteration 74, loss = 0.39076227\n",
      "Iteration 75, loss = 0.38991502\n",
      "Iteration 76, loss = 0.38913325\n",
      "Iteration 77, loss = 0.38851832\n",
      "Iteration 78, loss = 0.38791154\n",
      "Iteration 79, loss = 0.38733072\n",
      "Iteration 80, loss = 0.38678768\n",
      "Iteration 81, loss = 0.38641862\n",
      "Iteration 82, loss = 0.38584635\n",
      "Iteration 83, loss = 0.38565751\n",
      "Iteration 84, loss = 0.38506914\n",
      "Iteration 85, loss = 0.38472770\n",
      "Iteration 86, loss = 0.38432480\n",
      "Iteration 87, loss = 0.38413777\n",
      "Iteration 88, loss = 0.38375423\n",
      "Iteration 89, loss = 0.38354091\n",
      "Iteration 90, loss = 0.38343240\n",
      "Iteration 91, loss = 0.38306564\n",
      "Iteration 92, loss = 0.38290098\n",
      "Iteration 93, loss = 0.38270761\n",
      "Iteration 94, loss = 0.38245073\n",
      "Iteration 95, loss = 0.38219457\n",
      "Iteration 96, loss = 0.38212230\n",
      "Iteration 97, loss = 0.38192026\n",
      "Iteration 98, loss = 0.38181485\n",
      "Iteration 99, loss = 0.38159042\n",
      "Iteration 100, loss = 0.38143781\n",
      "Iteration 101, loss = 0.38137862\n",
      "Iteration 102, loss = 0.38122557\n",
      "Iteration 103, loss = 0.38116400\n",
      "Iteration 104, loss = 0.38100501\n",
      "Iteration 105, loss = 0.38104738\n",
      "Iteration 106, loss = 0.38076656\n",
      "Iteration 107, loss = 0.38099001\n",
      "Iteration 108, loss = 0.38072946\n",
      "Iteration 109, loss = 0.38068810\n",
      "Iteration 110, loss = 0.38055305\n",
      "Iteration 111, loss = 0.38041637\n",
      "Iteration 112, loss = 0.38039856\n",
      "Iteration 113, loss = 0.38030037\n",
      "Iteration 114, loss = 0.38018901\n",
      "Iteration 115, loss = 0.38033820\n",
      "Iteration 116, loss = 0.38015699\n",
      "Iteration 117, loss = 0.38015454\n",
      "Iteration 118, loss = 0.37994915\n",
      "Iteration 119, loss = 0.37988193\n",
      "Iteration 120, loss = 0.37991214\n",
      "Iteration 121, loss = 0.37984367\n",
      "Iteration 122, loss = 0.37963132\n",
      "Iteration 123, loss = 0.37976171\n",
      "Iteration 124, loss = 0.37971992\n",
      "Iteration 125, loss = 0.37970544\n",
      "Iteration 126, loss = 0.37950434\n",
      "Iteration 127, loss = 0.37959737\n",
      "Iteration 128, loss = 0.37951135\n",
      "Iteration 129, loss = 0.37944678\n",
      "Iteration 130, loss = 0.37938853\n",
      "Iteration 131, loss = 0.37944605\n",
      "Iteration 132, loss = 0.37931354\n",
      "Iteration 133, loss = 0.37936016\n",
      "Iteration 134, loss = 0.37923716\n",
      "Iteration 135, loss = 0.37924654\n",
      "Iteration 136, loss = 0.37918778\n",
      "Iteration 137, loss = 0.37922712\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.74074269\n",
      "Iteration 2, loss = 0.69341212\n",
      "Iteration 3, loss = 0.69284810\n",
      "Iteration 4, loss = 0.69244000\n",
      "Iteration 5, loss = 0.69196532\n",
      "Iteration 6, loss = 0.69147221\n",
      "Iteration 7, loss = 0.69093689\n",
      "Iteration 8, loss = 0.69032367\n",
      "Iteration 9, loss = 0.68966363\n",
      "Iteration 10, loss = 0.68893712\n",
      "Iteration 11, loss = 0.68806572\n",
      "Iteration 12, loss = 0.68712984\n",
      "Iteration 13, loss = 0.68600167\n",
      "Iteration 14, loss = 0.68476913\n",
      "Iteration 15, loss = 0.68335193\n",
      "Iteration 16, loss = 0.68178993\n",
      "Iteration 17, loss = 0.67994833\n",
      "Iteration 18, loss = 0.67794757\n",
      "Iteration 19, loss = 0.67566709\n",
      "Iteration 20, loss = 0.67305333\n",
      "Iteration 21, loss = 0.67023970\n",
      "Iteration 22, loss = 0.66712578\n",
      "Iteration 23, loss = 0.66369326\n",
      "Iteration 24, loss = 0.65988781\n",
      "Iteration 25, loss = 0.65576576\n",
      "Iteration 26, loss = 0.65131187\n",
      "Iteration 27, loss = 0.64620189\n",
      "Iteration 28, loss = 0.64086817\n",
      "Iteration 29, loss = 0.63500851\n",
      "Iteration 30, loss = 0.62876736\n",
      "Iteration 31, loss = 0.62196867\n",
      "Iteration 32, loss = 0.61482925\n",
      "Iteration 33, loss = 0.60713632\n",
      "Iteration 34, loss = 0.59902189\n",
      "Iteration 35, loss = 0.59057909\n",
      "Iteration 36, loss = 0.58175036\n",
      "Iteration 37, loss = 0.57262025\n",
      "Iteration 38, loss = 0.56334435\n",
      "Iteration 39, loss = 0.55395786\n",
      "Iteration 40, loss = 0.54453475\n",
      "Iteration 41, loss = 0.53506956\n",
      "Iteration 42, loss = 0.52581593\n",
      "Iteration 43, loss = 0.51668691\n",
      "Iteration 44, loss = 0.50788516\n",
      "Iteration 45, loss = 0.49948818\n",
      "Iteration 46, loss = 0.49132540\n",
      "Iteration 47, loss = 0.48373679\n",
      "Iteration 48, loss = 0.47651627\n",
      "Iteration 49, loss = 0.46967881\n",
      "Iteration 50, loss = 0.46324667\n",
      "Iteration 51, loss = 0.45725924\n",
      "Iteration 52, loss = 0.45183526\n",
      "Iteration 53, loss = 0.44667286\n",
      "Iteration 54, loss = 0.44195242\n",
      "Iteration 55, loss = 0.43757905\n",
      "Iteration 56, loss = 0.43352702\n",
      "Iteration 57, loss = 0.42989869\n",
      "Iteration 58, loss = 0.42655771\n",
      "Iteration 59, loss = 0.42340255\n",
      "Iteration 60, loss = 0.42055802\n",
      "Iteration 61, loss = 0.41799152\n",
      "Iteration 62, loss = 0.41574693\n",
      "Iteration 63, loss = 0.41349578\n",
      "Iteration 64, loss = 0.41150178\n",
      "Iteration 65, loss = 0.40964510\n",
      "Iteration 66, loss = 0.40803797\n",
      "Iteration 67, loss = 0.40645901\n",
      "Iteration 68, loss = 0.40506885\n",
      "Iteration 69, loss = 0.40375082\n",
      "Iteration 70, loss = 0.40257914\n",
      "Iteration 71, loss = 0.40166432\n",
      "Iteration 72, loss = 0.40047693\n",
      "Iteration 73, loss = 0.39961258\n",
      "Iteration 74, loss = 0.39869516\n",
      "Iteration 75, loss = 0.39815655\n",
      "Iteration 76, loss = 0.39724418\n",
      "Iteration 77, loss = 0.39667479\n",
      "Iteration 78, loss = 0.39599298\n",
      "Iteration 79, loss = 0.39551854\n",
      "Iteration 80, loss = 0.39490029\n",
      "Iteration 81, loss = 0.39457472\n",
      "Iteration 82, loss = 0.39396363\n",
      "Iteration 83, loss = 0.39366551\n",
      "Iteration 84, loss = 0.39323794\n",
      "Iteration 85, loss = 0.39291596\n",
      "Iteration 86, loss = 0.39265378\n",
      "Iteration 87, loss = 0.39232125\n",
      "Iteration 88, loss = 0.39199358\n",
      "Iteration 89, loss = 0.39178475\n",
      "Iteration 90, loss = 0.39149499\n",
      "Iteration 91, loss = 0.39128912\n",
      "Iteration 92, loss = 0.39113575\n",
      "Iteration 93, loss = 0.39091458\n",
      "Iteration 94, loss = 0.39079102\n",
      "Iteration 95, loss = 0.39052078\n",
      "Iteration 96, loss = 0.39032432\n",
      "Iteration 97, loss = 0.39016692\n",
      "Iteration 98, loss = 0.39002293\n",
      "Iteration 99, loss = 0.38989727\n",
      "Iteration 100, loss = 0.38972844\n",
      "Iteration 101, loss = 0.38961287\n",
      "Iteration 102, loss = 0.38947878\n",
      "Iteration 103, loss = 0.38945995\n",
      "Iteration 104, loss = 0.38933274\n",
      "Iteration 105, loss = 0.38939002\n",
      "Iteration 106, loss = 0.38899819\n",
      "Iteration 107, loss = 0.38920169\n",
      "Iteration 108, loss = 0.38900932\n",
      "Iteration 109, loss = 0.38884741\n",
      "Iteration 110, loss = 0.38872745\n",
      "Iteration 111, loss = 0.38870791\n",
      "Iteration 112, loss = 0.38867087\n",
      "Iteration 113, loss = 0.38854336\n",
      "Iteration 114, loss = 0.38844698\n",
      "Iteration 115, loss = 0.38838069\n",
      "Iteration 116, loss = 0.38837904\n",
      "Iteration 117, loss = 0.38835944\n",
      "Iteration 118, loss = 0.38831949\n",
      "Iteration 119, loss = 0.38819759\n",
      "Iteration 120, loss = 0.38804056\n",
      "Iteration 121, loss = 0.38824456\n",
      "Iteration 122, loss = 0.38800353\n",
      "Iteration 123, loss = 0.38793776\n",
      "Iteration 124, loss = 0.38797175\n",
      "Iteration 125, loss = 0.38788929\n",
      "Iteration 126, loss = 0.38782334\n",
      "Iteration 127, loss = 0.38773105\n",
      "Iteration 128, loss = 0.38771325\n",
      "Iteration 129, loss = 0.38764456\n",
      "Iteration 130, loss = 0.38775353\n",
      "Iteration 131, loss = 0.38764092\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.74037364\n",
      "Iteration 2, loss = 0.69314620\n",
      "Iteration 3, loss = 0.69293661\n",
      "Iteration 4, loss = 0.69245696\n",
      "Iteration 5, loss = 0.69198824\n",
      "Iteration 6, loss = 0.69147232\n",
      "Iteration 7, loss = 0.69095758\n",
      "Iteration 8, loss = 0.69034918\n",
      "Iteration 9, loss = 0.68975372\n",
      "Iteration 10, loss = 0.68897171\n",
      "Iteration 11, loss = 0.68818670\n",
      "Iteration 12, loss = 0.68722029\n",
      "Iteration 13, loss = 0.68614072\n",
      "Iteration 14, loss = 0.68506394\n",
      "Iteration 15, loss = 0.68359091\n",
      "Iteration 16, loss = 0.68201078\n",
      "Iteration 17, loss = 0.68030164\n",
      "Iteration 18, loss = 0.67834182\n",
      "Iteration 19, loss = 0.67612752\n",
      "Iteration 20, loss = 0.67369118\n",
      "Iteration 21, loss = 0.67093592\n",
      "Iteration 22, loss = 0.66791856\n",
      "Iteration 23, loss = 0.66460956\n",
      "Iteration 24, loss = 0.66093107\n",
      "Iteration 25, loss = 0.65693881\n",
      "Iteration 26, loss = 0.65260852\n",
      "Iteration 27, loss = 0.64784016\n",
      "Iteration 28, loss = 0.64270858\n",
      "Iteration 29, loss = 0.63708745\n",
      "Iteration 30, loss = 0.63107846\n",
      "Iteration 31, loss = 0.62467271\n",
      "Iteration 32, loss = 0.61775879\n",
      "Iteration 33, loss = 0.61039610\n",
      "Iteration 34, loss = 0.60267594\n",
      "Iteration 35, loss = 0.59456907\n",
      "Iteration 36, loss = 0.58606094\n",
      "Iteration 37, loss = 0.57732901\n",
      "Iteration 38, loss = 0.56844187\n",
      "Iteration 39, loss = 0.55932444\n",
      "Iteration 40, loss = 0.55020157\n",
      "Iteration 41, loss = 0.54109906\n",
      "Iteration 42, loss = 0.53213636\n",
      "Iteration 43, loss = 0.52336359\n",
      "Iteration 44, loss = 0.51475311\n",
      "Iteration 45, loss = 0.50652373\n",
      "Iteration 46, loss = 0.49861032\n",
      "Iteration 47, loss = 0.49114975\n",
      "Iteration 48, loss = 0.48409358\n",
      "Iteration 49, loss = 0.47733100\n",
      "Iteration 50, loss = 0.47116285\n",
      "Iteration 51, loss = 0.46535437\n",
      "Iteration 52, loss = 0.45990904\n",
      "Iteration 53, loss = 0.45493008\n",
      "Iteration 54, loss = 0.45027009\n",
      "Iteration 55, loss = 0.44603437\n",
      "Iteration 56, loss = 0.44202206\n",
      "Iteration 57, loss = 0.43842532\n",
      "Iteration 58, loss = 0.43517859\n",
      "Iteration 59, loss = 0.43209583\n",
      "Iteration 60, loss = 0.42939384\n",
      "Iteration 61, loss = 0.42675066\n",
      "Iteration 62, loss = 0.42449652\n",
      "Iteration 63, loss = 0.42231996\n",
      "Iteration 64, loss = 0.42033834\n",
      "Iteration 65, loss = 0.41849063\n",
      "Iteration 66, loss = 0.41691952\n",
      "Iteration 67, loss = 0.41539476\n",
      "Iteration 68, loss = 0.41425825\n",
      "Iteration 69, loss = 0.41282333\n",
      "Iteration 70, loss = 0.41161764\n",
      "Iteration 71, loss = 0.41048499\n",
      "Iteration 72, loss = 0.40957658\n",
      "Iteration 73, loss = 0.40873032\n",
      "Iteration 74, loss = 0.40790955\n",
      "Iteration 75, loss = 0.40721724\n",
      "Iteration 76, loss = 0.40640217\n",
      "Iteration 77, loss = 0.40586522\n",
      "Iteration 78, loss = 0.40522749\n",
      "Iteration 79, loss = 0.40464493\n",
      "Iteration 80, loss = 0.40414580\n",
      "Iteration 81, loss = 0.40368651\n",
      "Iteration 82, loss = 0.40318470\n",
      "Iteration 83, loss = 0.40279437\n",
      "Iteration 84, loss = 0.40248787\n",
      "Iteration 85, loss = 0.40209203\n",
      "Iteration 86, loss = 0.40171913\n",
      "Iteration 87, loss = 0.40163549\n",
      "Iteration 88, loss = 0.40117591\n",
      "Iteration 89, loss = 0.40089159\n",
      "Iteration 90, loss = 0.40075160\n",
      "Iteration 91, loss = 0.40050933\n",
      "Iteration 92, loss = 0.40026399\n",
      "Iteration 93, loss = 0.40006983\n",
      "Iteration 94, loss = 0.39990686\n",
      "Iteration 95, loss = 0.39975811\n",
      "Iteration 96, loss = 0.39946912\n",
      "Iteration 97, loss = 0.39932585\n",
      "Iteration 98, loss = 0.39920082\n",
      "Iteration 99, loss = 0.39916711\n",
      "Iteration 100, loss = 0.39888943\n",
      "Iteration 101, loss = 0.39898189\n",
      "Iteration 102, loss = 0.39875449\n",
      "Iteration 103, loss = 0.39857911\n",
      "Iteration 104, loss = 0.39847832\n",
      "Iteration 105, loss = 0.39847658\n",
      "Iteration 106, loss = 0.39826945\n",
      "Iteration 107, loss = 0.39832292\n",
      "Iteration 108, loss = 0.39802905\n",
      "Iteration 109, loss = 0.39810083\n",
      "Iteration 110, loss = 0.39784284\n",
      "Iteration 111, loss = 0.39779760\n",
      "Iteration 112, loss = 0.39772569\n",
      "Iteration 113, loss = 0.39771045\n",
      "Iteration 114, loss = 0.39751815\n",
      "Iteration 115, loss = 0.39753549\n",
      "Iteration 116, loss = 0.39750574\n",
      "Iteration 117, loss = 0.39756902\n",
      "Iteration 118, loss = 0.39724295\n",
      "Iteration 119, loss = 0.39720329\n",
      "Iteration 120, loss = 0.39716825\n",
      "Iteration 121, loss = 0.39725776\n",
      "Iteration 122, loss = 0.39701219\n",
      "Iteration 123, loss = 0.39692013\n",
      "Iteration 124, loss = 0.39706603\n",
      "Iteration 125, loss = 0.39695655\n",
      "Iteration 126, loss = 0.39677174\n",
      "Iteration 127, loss = 0.39684133\n",
      "Iteration 128, loss = 0.39672614\n",
      "Iteration 129, loss = 0.39655887\n",
      "Iteration 130, loss = 0.39664336\n",
      "Iteration 131, loss = 0.39666656\n",
      "Iteration 132, loss = 0.39660017\n",
      "Iteration 133, loss = 0.39651677\n",
      "Iteration 134, loss = 0.39655741\n",
      "Iteration 135, loss = 0.39632835\n",
      "Iteration 136, loss = 0.39645396\n",
      "Iteration 137, loss = 0.39626043\n",
      "Iteration 138, loss = 0.39623326\n",
      "Iteration 139, loss = 0.39621889\n",
      "Iteration 140, loss = 0.39618143\n",
      "Iteration 141, loss = 0.39611154\n",
      "Iteration 142, loss = 0.39609184\n",
      "Iteration 143, loss = 0.39598384\n",
      "Iteration 144, loss = 0.39595282\n",
      "Iteration 145, loss = 0.39593301\n",
      "Iteration 146, loss = 0.39592128\n",
      "Iteration 147, loss = 0.39600565\n",
      "Iteration 148, loss = 0.39585154\n",
      "Iteration 149, loss = 0.39568555\n",
      "Iteration 150, loss = 0.39577630\n",
      "Iteration 151, loss = 0.39558319\n",
      "Iteration 152, loss = 0.39569351\n",
      "Iteration 153, loss = 0.39555857\n",
      "Iteration 154, loss = 0.39572855\n",
      "Iteration 155, loss = 0.39543430\n",
      "Iteration 156, loss = 0.39555157\n",
      "Iteration 157, loss = 0.39538903\n",
      "Iteration 158, loss = 0.39552249\n",
      "Iteration 159, loss = 0.39540903\n",
      "Iteration 160, loss = 0.39533796\n",
      "Iteration 161, loss = 0.39537071\n",
      "Iteration 162, loss = 0.39515980\n",
      "Iteration 163, loss = 0.39526414\n",
      "Iteration 164, loss = 0.39521217\n",
      "Iteration 165, loss = 0.39524095\n",
      "Iteration 166, loss = 0.39517556\n",
      "Iteration 167, loss = 0.39511864\n",
      "Iteration 168, loss = 0.39514010\n",
      "Iteration 169, loss = 0.39504237\n",
      "Iteration 170, loss = 0.39509003\n",
      "Iteration 171, loss = 0.39510417\n",
      "Iteration 172, loss = 0.39494767\n",
      "Iteration 173, loss = 0.39509816\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.73987052\n",
      "Iteration 2, loss = 0.69400166\n",
      "Iteration 3, loss = 0.68180584\n",
      "Iteration 4, loss = 0.66380319\n",
      "Iteration 5, loss = 0.63488048\n",
      "Iteration 6, loss = 0.59344464\n",
      "Iteration 7, loss = 0.54529737\n",
      "Iteration 8, loss = 0.49929038\n",
      "Iteration 9, loss = 0.46279997\n",
      "Iteration 10, loss = 0.43623995\n",
      "Iteration 11, loss = 0.41780036\n",
      "Iteration 12, loss = 0.40588527\n",
      "Iteration 13, loss = 0.39794262\n",
      "Iteration 14, loss = 0.39246240\n",
      "Iteration 15, loss = 0.38946691\n",
      "Iteration 16, loss = 0.38662234\n",
      "Iteration 17, loss = 0.38558527\n",
      "Iteration 18, loss = 0.38395328\n",
      "Iteration 19, loss = 0.38304929\n",
      "Iteration 20, loss = 0.38288614\n",
      "Iteration 21, loss = 0.38195310\n",
      "Iteration 22, loss = 0.38153652\n",
      "Iteration 23, loss = 0.38165060\n",
      "Iteration 24, loss = 0.38134198\n",
      "Iteration 25, loss = 0.38072059\n",
      "Iteration 26, loss = 0.38075458\n",
      "Iteration 27, loss = 0.38104328\n",
      "Iteration 28, loss = 0.38037418\n",
      "Iteration 29, loss = 0.38025419\n",
      "Iteration 30, loss = 0.38012538\n",
      "Iteration 31, loss = 0.38024723\n",
      "Iteration 32, loss = 0.37988561\n",
      "Iteration 33, loss = 0.37994487\n",
      "Iteration 34, loss = 0.37987514\n",
      "Iteration 35, loss = 0.37978927\n",
      "Iteration 36, loss = 0.37982631\n",
      "Iteration 37, loss = 0.37948120\n",
      "Iteration 38, loss = 0.38048400\n",
      "Iteration 39, loss = 0.37928429\n",
      "Iteration 40, loss = 0.37986385\n",
      "Iteration 41, loss = 0.37896325\n",
      "Iteration 42, loss = 0.37910260\n",
      "Iteration 43, loss = 0.37911835\n",
      "Iteration 44, loss = 0.37915692\n",
      "Iteration 45, loss = 0.37904428\n",
      "Iteration 46, loss = 0.37865490\n",
      "Iteration 47, loss = 0.37972777\n",
      "Iteration 48, loss = 0.37872518\n",
      "Iteration 49, loss = 0.37901134\n",
      "Iteration 50, loss = 0.37861482\n",
      "Iteration 51, loss = 0.37856901\n",
      "Iteration 52, loss = 0.37897349\n",
      "Iteration 53, loss = 0.37853676\n",
      "Iteration 54, loss = 0.37864496\n",
      "Iteration 55, loss = 0.37854766\n",
      "Iteration 56, loss = 0.37887298\n",
      "Iteration 57, loss = 0.37860422\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.74259779\n",
      "Iteration 2, loss = 0.69443176\n",
      "Iteration 3, loss = 0.68275460\n",
      "Iteration 4, loss = 0.66606520\n",
      "Iteration 5, loss = 0.63994643\n",
      "Iteration 6, loss = 0.60219148\n",
      "Iteration 7, loss = 0.55689368\n",
      "Iteration 8, loss = 0.51199617\n",
      "Iteration 9, loss = 0.47438096\n",
      "Iteration 10, loss = 0.44680032\n",
      "Iteration 11, loss = 0.42756563\n",
      "Iteration 12, loss = 0.41483638\n",
      "Iteration 13, loss = 0.40688197\n",
      "Iteration 14, loss = 0.40130407\n",
      "Iteration 15, loss = 0.39776180\n",
      "Iteration 16, loss = 0.39512880\n",
      "Iteration 17, loss = 0.39336558\n",
      "Iteration 18, loss = 0.39219498\n",
      "Iteration 19, loss = 0.39128942\n",
      "Iteration 20, loss = 0.39080009\n",
      "Iteration 21, loss = 0.39019205\n",
      "Iteration 22, loss = 0.38975372\n",
      "Iteration 23, loss = 0.38979115\n",
      "Iteration 24, loss = 0.38954229\n",
      "Iteration 25, loss = 0.38917212\n",
      "Iteration 26, loss = 0.38892111\n",
      "Iteration 27, loss = 0.38904496\n",
      "Iteration 28, loss = 0.38851582\n",
      "Iteration 29, loss = 0.38809032\n",
      "Iteration 30, loss = 0.38809648\n",
      "Iteration 31, loss = 0.38795218\n",
      "Iteration 32, loss = 0.38832239\n",
      "Iteration 33, loss = 0.38820273\n",
      "Iteration 34, loss = 0.38824198\n",
      "Iteration 35, loss = 0.38793684\n",
      "Iteration 36, loss = 0.38796734\n",
      "Iteration 37, loss = 0.38733232\n",
      "Iteration 38, loss = 0.38738847\n",
      "Iteration 39, loss = 0.38736821\n",
      "Iteration 40, loss = 0.38731997\n",
      "Iteration 41, loss = 0.38724082\n",
      "Iteration 42, loss = 0.38720819\n",
      "Iteration 43, loss = 0.38665018\n",
      "Iteration 44, loss = 0.38709587\n",
      "Iteration 45, loss = 0.38681702\n",
      "Iteration 46, loss = 0.38680666\n",
      "Iteration 47, loss = 0.38675361\n",
      "Iteration 48, loss = 0.38642900\n",
      "Iteration 49, loss = 0.38653108\n",
      "Iteration 50, loss = 0.38647217\n",
      "Iteration 51, loss = 0.38638338\n",
      "Iteration 52, loss = 0.38669736\n",
      "Iteration 53, loss = 0.38613952\n",
      "Iteration 54, loss = 0.38626607\n",
      "Iteration 55, loss = 0.38652332\n",
      "Iteration 56, loss = 0.38623336\n",
      "Iteration 57, loss = 0.38661822\n",
      "Iteration 58, loss = 0.38716725\n",
      "Iteration 59, loss = 0.38627432\n",
      "Iteration 60, loss = 0.38590485\n",
      "Iteration 61, loss = 0.38593650\n",
      "Iteration 62, loss = 0.38671085\n",
      "Iteration 63, loss = 0.38606440\n",
      "Iteration 64, loss = 0.38621582\n",
      "Iteration 65, loss = 0.38608888\n",
      "Iteration 66, loss = 0.38626535\n",
      "Iteration 67, loss = 0.38622640\n",
      "Iteration 68, loss = 0.38569307\n",
      "Iteration 69, loss = 0.38582228\n",
      "Iteration 70, loss = 0.38590114\n",
      "Iteration 71, loss = 0.38592233\n",
      "Iteration 72, loss = 0.38599338\n",
      "Iteration 73, loss = 0.38613209\n",
      "Iteration 74, loss = 0.38575244\n",
      "Iteration 75, loss = 0.38578393\n",
      "Iteration 76, loss = 0.38585547\n",
      "Iteration 77, loss = 0.38571780\n",
      "Iteration 78, loss = 0.38563043\n",
      "Iteration 79, loss = 0.38582157\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.74036378\n",
      "Iteration 2, loss = 0.69401318\n",
      "Iteration 3, loss = 0.68267161\n",
      "Iteration 4, loss = 0.66642892\n",
      "Iteration 5, loss = 0.64098039\n",
      "Iteration 6, loss = 0.60442368\n",
      "Iteration 7, loss = 0.56033388\n",
      "Iteration 8, loss = 0.51659023\n",
      "Iteration 9, loss = 0.47974661\n",
      "Iteration 10, loss = 0.45289770\n",
      "Iteration 11, loss = 0.43455349\n",
      "Iteration 12, loss = 0.42228464\n",
      "Iteration 13, loss = 0.41441996\n",
      "Iteration 14, loss = 0.40916162\n",
      "Iteration 15, loss = 0.40616170\n",
      "Iteration 16, loss = 0.40417429\n",
      "Iteration 17, loss = 0.40213508\n",
      "Iteration 18, loss = 0.40086540\n",
      "Iteration 19, loss = 0.39995118\n",
      "Iteration 20, loss = 0.39962805\n",
      "Iteration 21, loss = 0.39905151\n",
      "Iteration 22, loss = 0.39847210\n",
      "Iteration 23, loss = 0.39829716\n",
      "Iteration 24, loss = 0.39793409\n",
      "Iteration 25, loss = 0.39784152\n",
      "Iteration 26, loss = 0.39731855\n",
      "Iteration 27, loss = 0.39737864\n",
      "Iteration 28, loss = 0.39731818\n",
      "Iteration 29, loss = 0.39690515\n",
      "Iteration 30, loss = 0.39668088\n",
      "Iteration 31, loss = 0.39695021\n",
      "Iteration 32, loss = 0.39676355\n",
      "Iteration 33, loss = 0.39611182\n",
      "Iteration 34, loss = 0.39602775\n",
      "Iteration 35, loss = 0.39586902\n",
      "Iteration 36, loss = 0.39596008\n",
      "Iteration 37, loss = 0.39591151\n",
      "Iteration 38, loss = 0.39597374\n",
      "Iteration 39, loss = 0.39558068\n",
      "Iteration 40, loss = 0.39556869\n",
      "Iteration 41, loss = 0.39530247\n",
      "Iteration 42, loss = 0.39524046\n",
      "Iteration 43, loss = 0.39507353\n",
      "Iteration 44, loss = 0.39507386\n",
      "Iteration 45, loss = 0.39486612\n",
      "Iteration 46, loss = 0.39479386\n",
      "Iteration 47, loss = 0.39485916\n",
      "Iteration 48, loss = 0.39478618\n",
      "Iteration 49, loss = 0.39507710\n",
      "Iteration 50, loss = 0.39513701\n",
      "Iteration 51, loss = 0.39497492\n",
      "Iteration 52, loss = 0.39458880\n",
      "Iteration 53, loss = 0.39429852\n",
      "Iteration 54, loss = 0.39452352\n",
      "Iteration 55, loss = 0.39469795\n",
      "Iteration 56, loss = 0.39427977\n",
      "Iteration 57, loss = 0.39454402\n",
      "Iteration 58, loss = 0.39462269\n",
      "Iteration 59, loss = 0.39438414\n",
      "Iteration 60, loss = 0.39417746\n",
      "Iteration 61, loss = 0.39434939\n",
      "Iteration 62, loss = 0.39430983\n",
      "Iteration 63, loss = 0.39415266\n",
      "Iteration 64, loss = 0.39428723\n",
      "Iteration 65, loss = 0.39392326\n",
      "Iteration 66, loss = 0.39413462\n",
      "Iteration 67, loss = 0.39433609\n",
      "Iteration 68, loss = 0.39423314\n",
      "Iteration 69, loss = 0.39431521\n",
      "Iteration 70, loss = 0.39391549\n",
      "Iteration 71, loss = 0.39362661\n",
      "Iteration 72, loss = 0.39382848\n",
      "Iteration 73, loss = 0.39384612\n",
      "Iteration 74, loss = 0.39430164\n",
      "Iteration 75, loss = 0.39405371\n",
      "Iteration 76, loss = 0.39355949\n",
      "Iteration 77, loss = 0.39394314\n",
      "Iteration 78, loss = 0.39399426\n",
      "Iteration 79, loss = 0.39373976\n",
      "Iteration 80, loss = 0.39338929\n",
      "Iteration 81, loss = 0.39413305\n",
      "Iteration 82, loss = 0.39342024\n",
      "Iteration 83, loss = 0.39340682\n",
      "Iteration 84, loss = 0.39378256\n",
      "Iteration 85, loss = 0.39358094\n",
      "Iteration 86, loss = 0.39359248\n",
      "Iteration 87, loss = 0.39331207\n",
      "Iteration 88, loss = 0.39333410\n",
      "Iteration 89, loss = 0.39355474\n",
      "Iteration 90, loss = 0.39334898\n",
      "Iteration 91, loss = 0.39348532\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69549968\n",
      "Iteration 2, loss = 0.69311813\n",
      "Iteration 3, loss = 0.69278785\n",
      "Iteration 4, loss = 0.69258932\n",
      "Iteration 5, loss = 0.69226796\n",
      "Iteration 6, loss = 0.69198359\n",
      "Iteration 7, loss = 0.69177200\n",
      "Iteration 8, loss = 0.69134207\n",
      "Iteration 9, loss = 0.69102533\n",
      "Iteration 10, loss = 0.69059880\n",
      "Iteration 11, loss = 0.69011621\n",
      "Iteration 12, loss = 0.68965096\n",
      "Iteration 13, loss = 0.68907050\n",
      "Iteration 14, loss = 0.68849441\n",
      "Iteration 15, loss = 0.68764733\n",
      "Iteration 16, loss = 0.68684304\n",
      "Iteration 17, loss = 0.68586726\n",
      "Iteration 18, loss = 0.68486124\n",
      "Iteration 19, loss = 0.68350887\n",
      "Iteration 20, loss = 0.68212446\n",
      "Iteration 21, loss = 0.68062860\n",
      "Iteration 22, loss = 0.67886588\n",
      "Iteration 23, loss = 0.67690982\n",
      "Iteration 24, loss = 0.67474293\n",
      "Iteration 25, loss = 0.67224408\n",
      "Iteration 26, loss = 0.66955897\n",
      "Iteration 27, loss = 0.66650335\n",
      "Iteration 28, loss = 0.66340682\n",
      "Iteration 29, loss = 0.65963144\n",
      "Iteration 30, loss = 0.65562859\n",
      "Iteration 31, loss = 0.65121612\n",
      "Iteration 32, loss = 0.64650070\n",
      "Iteration 33, loss = 0.64133165\n",
      "Iteration 34, loss = 0.63561893\n",
      "Iteration 35, loss = 0.62951439\n",
      "Iteration 36, loss = 0.62292960\n",
      "Iteration 37, loss = 0.61583366\n",
      "Iteration 38, loss = 0.60822432\n",
      "Iteration 39, loss = 0.60027028\n",
      "Iteration 40, loss = 0.59178489\n",
      "Iteration 41, loss = 0.58298641\n",
      "Iteration 42, loss = 0.57376347\n",
      "Iteration 43, loss = 0.56423707\n",
      "Iteration 44, loss = 0.55461599\n",
      "Iteration 45, loss = 0.54492563\n",
      "Iteration 46, loss = 0.53522287\n",
      "Iteration 47, loss = 0.52562661\n",
      "Iteration 48, loss = 0.51614244\n",
      "Iteration 49, loss = 0.50693986\n",
      "Iteration 50, loss = 0.49807451\n",
      "Iteration 51, loss = 0.48953653\n",
      "Iteration 52, loss = 0.48137470\n",
      "Iteration 53, loss = 0.47369906\n",
      "Iteration 54, loss = 0.46647656\n",
      "Iteration 55, loss = 0.45967714\n",
      "Iteration 56, loss = 0.45343996\n",
      "Iteration 57, loss = 0.44760607\n",
      "Iteration 58, loss = 0.44216965\n",
      "Iteration 59, loss = 0.43723844\n",
      "Iteration 60, loss = 0.43244973\n",
      "Iteration 61, loss = 0.42821467\n",
      "Iteration 62, loss = 0.42435650\n",
      "Iteration 63, loss = 0.42068351\n",
      "Iteration 64, loss = 0.41736805\n",
      "Iteration 65, loss = 0.41436713\n",
      "Iteration 66, loss = 0.41161013\n",
      "Iteration 67, loss = 0.40894748\n",
      "Iteration 68, loss = 0.40674352\n",
      "Iteration 69, loss = 0.40456984\n",
      "Iteration 70, loss = 0.40258338\n",
      "Iteration 71, loss = 0.40089743\n",
      "Iteration 72, loss = 0.39913697\n",
      "Iteration 73, loss = 0.39787938\n",
      "Iteration 74, loss = 0.39624673\n",
      "Iteration 75, loss = 0.39507927\n",
      "Iteration 76, loss = 0.39382016\n",
      "Iteration 77, loss = 0.39278018\n",
      "Iteration 78, loss = 0.39190762\n",
      "Iteration 79, loss = 0.39098090\n",
      "Iteration 80, loss = 0.39015828\n",
      "Iteration 81, loss = 0.38950718\n",
      "Iteration 82, loss = 0.38870701\n",
      "Iteration 83, loss = 0.38799729\n",
      "Iteration 84, loss = 0.38759847\n",
      "Iteration 85, loss = 0.38690652\n",
      "Iteration 86, loss = 0.38650643\n",
      "Iteration 87, loss = 0.38590973\n",
      "Iteration 88, loss = 0.38572826\n",
      "Iteration 89, loss = 0.38516326\n",
      "Iteration 90, loss = 0.38486444\n",
      "Iteration 91, loss = 0.38446494\n",
      "Iteration 92, loss = 0.38411447\n",
      "Iteration 93, loss = 0.38384567\n",
      "Iteration 94, loss = 0.38361714\n",
      "Iteration 95, loss = 0.38337310\n",
      "Iteration 96, loss = 0.38294893\n",
      "Iteration 97, loss = 0.38281446\n",
      "Iteration 98, loss = 0.38252568\n",
      "Iteration 99, loss = 0.38250031\n",
      "Iteration 100, loss = 0.38218536\n",
      "Iteration 101, loss = 0.38209304\n",
      "Iteration 102, loss = 0.38200058\n",
      "Iteration 103, loss = 0.38181015\n",
      "Iteration 104, loss = 0.38164865\n",
      "Iteration 105, loss = 0.38143172\n",
      "Iteration 106, loss = 0.38143565\n",
      "Iteration 107, loss = 0.38115596\n",
      "Iteration 108, loss = 0.38106819\n",
      "Iteration 109, loss = 0.38106810\n",
      "Iteration 110, loss = 0.38093105\n",
      "Iteration 111, loss = 0.38074126\n",
      "Iteration 112, loss = 0.38072571\n",
      "Iteration 113, loss = 0.38059409\n",
      "Iteration 114, loss = 0.38061782\n",
      "Iteration 115, loss = 0.38047649\n",
      "Iteration 116, loss = 0.38036593\n",
      "Iteration 117, loss = 0.38023735\n",
      "Iteration 118, loss = 0.38041902\n",
      "Iteration 119, loss = 0.38023961\n",
      "Iteration 120, loss = 0.38015228\n",
      "Iteration 121, loss = 0.38012212\n",
      "Iteration 122, loss = 0.38004091\n",
      "Iteration 123, loss = 0.38001667\n",
      "Iteration 124, loss = 0.37990360\n",
      "Iteration 125, loss = 0.37984089\n",
      "Iteration 126, loss = 0.37978246\n",
      "Iteration 127, loss = 0.37972396\n",
      "Iteration 128, loss = 0.37966607\n",
      "Iteration 129, loss = 0.37963665\n",
      "Iteration 130, loss = 0.37974646\n",
      "Iteration 131, loss = 0.37967258\n",
      "Iteration 132, loss = 0.37955265\n",
      "Iteration 133, loss = 0.37965900\n",
      "Iteration 134, loss = 0.37946096\n",
      "Iteration 135, loss = 0.37945797\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69603343\n",
      "Iteration 2, loss = 0.69301775\n",
      "Iteration 3, loss = 0.69276445\n",
      "Iteration 4, loss = 0.69252754\n",
      "Iteration 5, loss = 0.69228974\n",
      "Iteration 6, loss = 0.69199972\n",
      "Iteration 7, loss = 0.69172122\n",
      "Iteration 8, loss = 0.69126730\n",
      "Iteration 9, loss = 0.69099014\n",
      "Iteration 10, loss = 0.69053951\n",
      "Iteration 11, loss = 0.69017169\n",
      "Iteration 12, loss = 0.68959509\n",
      "Iteration 13, loss = 0.68898107\n",
      "Iteration 14, loss = 0.68845252\n",
      "Iteration 15, loss = 0.68766460\n",
      "Iteration 16, loss = 0.68671751\n",
      "Iteration 17, loss = 0.68581028\n",
      "Iteration 18, loss = 0.68474033\n",
      "Iteration 19, loss = 0.68344947\n",
      "Iteration 20, loss = 0.68202406\n",
      "Iteration 21, loss = 0.68051894\n",
      "Iteration 22, loss = 0.67878337\n",
      "Iteration 23, loss = 0.67682986\n",
      "Iteration 24, loss = 0.67466654\n",
      "Iteration 25, loss = 0.67216626\n",
      "Iteration 26, loss = 0.66951943\n",
      "Iteration 27, loss = 0.66647434\n",
      "Iteration 28, loss = 0.66332182\n",
      "Iteration 29, loss = 0.65965871\n",
      "Iteration 30, loss = 0.65571140\n",
      "Iteration 31, loss = 0.65137751\n",
      "Iteration 32, loss = 0.64668357\n",
      "Iteration 33, loss = 0.64147904\n",
      "Iteration 34, loss = 0.63594371\n",
      "Iteration 35, loss = 0.62990105\n",
      "Iteration 36, loss = 0.62345348\n",
      "Iteration 37, loss = 0.61645032\n",
      "Iteration 38, loss = 0.60908250\n",
      "Iteration 39, loss = 0.60121946\n",
      "Iteration 40, loss = 0.59296101\n",
      "Iteration 41, loss = 0.58437351\n",
      "Iteration 42, loss = 0.57537194\n",
      "Iteration 43, loss = 0.56616793\n",
      "Iteration 44, loss = 0.55685544\n",
      "Iteration 45, loss = 0.54740777\n",
      "Iteration 46, loss = 0.53801805\n",
      "Iteration 47, loss = 0.52873348\n",
      "Iteration 48, loss = 0.51955724\n",
      "Iteration 49, loss = 0.51066766\n",
      "Iteration 50, loss = 0.50210477\n",
      "Iteration 51, loss = 0.49392608\n",
      "Iteration 52, loss = 0.48616788\n",
      "Iteration 53, loss = 0.47873547\n",
      "Iteration 54, loss = 0.47176518\n",
      "Iteration 55, loss = 0.46521660\n",
      "Iteration 56, loss = 0.45921728\n",
      "Iteration 57, loss = 0.45357466\n",
      "Iteration 58, loss = 0.44826595\n",
      "Iteration 59, loss = 0.44344695\n",
      "Iteration 60, loss = 0.43902092\n",
      "Iteration 61, loss = 0.43498984\n",
      "Iteration 62, loss = 0.43115762\n",
      "Iteration 63, loss = 0.42773376\n",
      "Iteration 64, loss = 0.42456148\n",
      "Iteration 65, loss = 0.42171985\n",
      "Iteration 66, loss = 0.41909534\n",
      "Iteration 67, loss = 0.41642005\n",
      "Iteration 68, loss = 0.41415926\n",
      "Iteration 69, loss = 0.41209896\n",
      "Iteration 70, loss = 0.41022527\n",
      "Iteration 71, loss = 0.40855296\n",
      "Iteration 72, loss = 0.40694981\n",
      "Iteration 73, loss = 0.40555381\n",
      "Iteration 74, loss = 0.40416693\n",
      "Iteration 75, loss = 0.40296176\n",
      "Iteration 76, loss = 0.40191997\n",
      "Iteration 77, loss = 0.40090547\n",
      "Iteration 78, loss = 0.39996726\n",
      "Iteration 79, loss = 0.39904753\n",
      "Iteration 80, loss = 0.39826632\n",
      "Iteration 81, loss = 0.39758109\n",
      "Iteration 82, loss = 0.39687402\n",
      "Iteration 83, loss = 0.39615300\n",
      "Iteration 84, loss = 0.39558264\n",
      "Iteration 85, loss = 0.39510174\n",
      "Iteration 86, loss = 0.39456926\n",
      "Iteration 87, loss = 0.39410198\n",
      "Iteration 88, loss = 0.39382559\n",
      "Iteration 89, loss = 0.39327341\n",
      "Iteration 90, loss = 0.39315783\n",
      "Iteration 91, loss = 0.39261127\n",
      "Iteration 92, loss = 0.39229320\n",
      "Iteration 93, loss = 0.39196721\n",
      "Iteration 94, loss = 0.39186518\n",
      "Iteration 95, loss = 0.39147482\n",
      "Iteration 96, loss = 0.39134127\n",
      "Iteration 97, loss = 0.39104662\n",
      "Iteration 98, loss = 0.39102638\n",
      "Iteration 99, loss = 0.39070619\n",
      "Iteration 100, loss = 0.39049641\n",
      "Iteration 101, loss = 0.39037557\n",
      "Iteration 102, loss = 0.39012612\n",
      "Iteration 103, loss = 0.39007769\n",
      "Iteration 104, loss = 0.38992339\n",
      "Iteration 105, loss = 0.38977093\n",
      "Iteration 106, loss = 0.38975034\n",
      "Iteration 107, loss = 0.38951771\n",
      "Iteration 108, loss = 0.38939598\n",
      "Iteration 109, loss = 0.38928589\n",
      "Iteration 110, loss = 0.38922381\n",
      "Iteration 111, loss = 0.38908420\n",
      "Iteration 112, loss = 0.38915109\n",
      "Iteration 113, loss = 0.38890357\n",
      "Iteration 114, loss = 0.38879305\n",
      "Iteration 115, loss = 0.38872591\n",
      "Iteration 116, loss = 0.38862790\n",
      "Iteration 117, loss = 0.38855380\n",
      "Iteration 118, loss = 0.38860331\n",
      "Iteration 119, loss = 0.38852182\n",
      "Iteration 120, loss = 0.38837397\n",
      "Iteration 121, loss = 0.38824562\n",
      "Iteration 122, loss = 0.38825857\n",
      "Iteration 123, loss = 0.38822145\n",
      "Iteration 124, loss = 0.38826326\n",
      "Iteration 125, loss = 0.38818403\n",
      "Iteration 126, loss = 0.38794932\n",
      "Iteration 127, loss = 0.38814727\n",
      "Iteration 128, loss = 0.38800304\n",
      "Iteration 129, loss = 0.38799401\n",
      "Iteration 130, loss = 0.38799487\n",
      "Iteration 131, loss = 0.38781761\n",
      "Iteration 132, loss = 0.38799357\n",
      "Iteration 133, loss = 0.38775834\n",
      "Iteration 134, loss = 0.38764907\n",
      "Iteration 135, loss = 0.38765020\n",
      "Iteration 136, loss = 0.38754209\n",
      "Iteration 137, loss = 0.38768810\n",
      "Iteration 138, loss = 0.38752404\n",
      "Iteration 139, loss = 0.38745297\n",
      "Iteration 140, loss = 0.38739687\n",
      "Iteration 141, loss = 0.38749338\n",
      "Iteration 142, loss = 0.38734804\n",
      "Iteration 143, loss = 0.38738702\n",
      "Iteration 144, loss = 0.38735835\n",
      "Iteration 145, loss = 0.38729785\n",
      "Iteration 146, loss = 0.38724577\n",
      "Iteration 147, loss = 0.38729211\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69645848\n",
      "Iteration 2, loss = 0.69306284\n",
      "Iteration 3, loss = 0.69279799\n",
      "Iteration 4, loss = 0.69257446\n",
      "Iteration 5, loss = 0.69231476\n",
      "Iteration 6, loss = 0.69199745\n",
      "Iteration 7, loss = 0.69167389\n",
      "Iteration 8, loss = 0.69131966\n",
      "Iteration 9, loss = 0.69096949\n",
      "Iteration 10, loss = 0.69059426\n",
      "Iteration 11, loss = 0.69012400\n",
      "Iteration 12, loss = 0.68960915\n",
      "Iteration 13, loss = 0.68911738\n",
      "Iteration 14, loss = 0.68841409\n",
      "Iteration 15, loss = 0.68766111\n",
      "Iteration 16, loss = 0.68685770\n",
      "Iteration 17, loss = 0.68592072\n",
      "Iteration 18, loss = 0.68485493\n",
      "Iteration 19, loss = 0.68374110\n",
      "Iteration 20, loss = 0.68234201\n",
      "Iteration 21, loss = 0.68087593\n",
      "Iteration 22, loss = 0.67917785\n",
      "Iteration 23, loss = 0.67731056\n",
      "Iteration 24, loss = 0.67519625\n",
      "Iteration 25, loss = 0.67285205\n",
      "Iteration 26, loss = 0.67017966\n",
      "Iteration 27, loss = 0.66742095\n",
      "Iteration 28, loss = 0.66423804\n",
      "Iteration 29, loss = 0.66077493\n",
      "Iteration 30, loss = 0.65699954\n",
      "Iteration 31, loss = 0.65284989\n",
      "Iteration 32, loss = 0.64837650\n",
      "Iteration 33, loss = 0.64341089\n",
      "Iteration 34, loss = 0.63813526\n",
      "Iteration 35, loss = 0.63232272\n",
      "Iteration 36, loss = 0.62624987\n",
      "Iteration 37, loss = 0.61961585\n",
      "Iteration 38, loss = 0.61244895\n",
      "Iteration 39, loss = 0.60498354\n",
      "Iteration 40, loss = 0.59705130\n",
      "Iteration 41, loss = 0.58872539\n",
      "Iteration 42, loss = 0.58024873\n",
      "Iteration 43, loss = 0.57139774\n",
      "Iteration 44, loss = 0.56247960\n",
      "Iteration 45, loss = 0.55340922\n",
      "Iteration 46, loss = 0.54433975\n",
      "Iteration 47, loss = 0.53540848\n",
      "Iteration 48, loss = 0.52647057\n",
      "Iteration 49, loss = 0.51782921\n",
      "Iteration 50, loss = 0.50960262\n",
      "Iteration 51, loss = 0.50154352\n",
      "Iteration 52, loss = 0.49380765\n",
      "Iteration 53, loss = 0.48663112\n",
      "Iteration 54, loss = 0.47985437\n",
      "Iteration 55, loss = 0.47349339\n",
      "Iteration 56, loss = 0.46745336\n",
      "Iteration 57, loss = 0.46184677\n",
      "Iteration 58, loss = 0.45676899\n",
      "Iteration 59, loss = 0.45202698\n",
      "Iteration 60, loss = 0.44753852\n",
      "Iteration 61, loss = 0.44355337\n",
      "Iteration 62, loss = 0.43974848\n",
      "Iteration 63, loss = 0.43632937\n",
      "Iteration 64, loss = 0.43315892\n",
      "Iteration 65, loss = 0.43026656\n",
      "Iteration 66, loss = 0.42764789\n",
      "Iteration 67, loss = 0.42527033\n",
      "Iteration 68, loss = 0.42301302\n",
      "Iteration 69, loss = 0.42100194\n",
      "Iteration 70, loss = 0.41914324\n",
      "Iteration 71, loss = 0.41750863\n",
      "Iteration 72, loss = 0.41594008\n",
      "Iteration 73, loss = 0.41468604\n",
      "Iteration 74, loss = 0.41317781\n",
      "Iteration 75, loss = 0.41204708\n",
      "Iteration 76, loss = 0.41087906\n",
      "Iteration 77, loss = 0.40990385\n",
      "Iteration 78, loss = 0.40896364\n",
      "Iteration 79, loss = 0.40808319\n",
      "Iteration 80, loss = 0.40738672\n",
      "Iteration 81, loss = 0.40674172\n",
      "Iteration 82, loss = 0.40597688\n",
      "Iteration 83, loss = 0.40536113\n",
      "Iteration 84, loss = 0.40483841\n",
      "Iteration 85, loss = 0.40422623\n",
      "Iteration 86, loss = 0.40379802\n",
      "Iteration 87, loss = 0.40331187\n",
      "Iteration 88, loss = 0.40292198\n",
      "Iteration 89, loss = 0.40260397\n",
      "Iteration 90, loss = 0.40228229\n",
      "Iteration 91, loss = 0.40199631\n",
      "Iteration 92, loss = 0.40154149\n",
      "Iteration 93, loss = 0.40121364\n",
      "Iteration 94, loss = 0.40094915\n",
      "Iteration 95, loss = 0.40085463\n",
      "Iteration 96, loss = 0.40049453\n",
      "Iteration 97, loss = 0.40034958\n",
      "Iteration 98, loss = 0.40006551\n",
      "Iteration 99, loss = 0.39998800\n",
      "Iteration 100, loss = 0.39968850\n",
      "Iteration 101, loss = 0.39952171\n",
      "Iteration 102, loss = 0.39944392\n",
      "Iteration 103, loss = 0.39908055\n",
      "Iteration 104, loss = 0.39920908\n",
      "Iteration 105, loss = 0.39894609\n",
      "Iteration 106, loss = 0.39879068\n",
      "Iteration 107, loss = 0.39866208\n",
      "Iteration 108, loss = 0.39852762\n",
      "Iteration 109, loss = 0.39852022\n",
      "Iteration 110, loss = 0.39841163\n",
      "Iteration 111, loss = 0.39820622\n",
      "Iteration 112, loss = 0.39808129\n",
      "Iteration 113, loss = 0.39806418\n",
      "Iteration 114, loss = 0.39795906\n",
      "Iteration 115, loss = 0.39787841\n",
      "Iteration 116, loss = 0.39785059\n",
      "Iteration 117, loss = 0.39771496\n",
      "Iteration 118, loss = 0.39764025\n",
      "Iteration 119, loss = 0.39763300\n",
      "Iteration 120, loss = 0.39747753\n",
      "Iteration 121, loss = 0.39750558\n",
      "Iteration 122, loss = 0.39765155\n",
      "Iteration 123, loss = 0.39730824\n",
      "Iteration 124, loss = 0.39719147\n",
      "Iteration 125, loss = 0.39716433\n",
      "Iteration 126, loss = 0.39702954\n",
      "Iteration 127, loss = 0.39707175\n",
      "Iteration 128, loss = 0.39687717\n",
      "Iteration 129, loss = 0.39697347\n",
      "Iteration 130, loss = 0.39677511\n",
      "Iteration 131, loss = 0.39693280\n",
      "Iteration 132, loss = 0.39677417\n",
      "Iteration 133, loss = 0.39668980\n",
      "Iteration 134, loss = 0.39690915\n",
      "Iteration 135, loss = 0.39658757\n",
      "Iteration 136, loss = 0.39653994\n",
      "Iteration 137, loss = 0.39646525\n",
      "Iteration 138, loss = 0.39643117\n",
      "Iteration 139, loss = 0.39633648\n",
      "Iteration 140, loss = 0.39643607\n",
      "Iteration 141, loss = 0.39626250\n",
      "Iteration 142, loss = 0.39620745\n",
      "Iteration 143, loss = 0.39617829\n",
      "Iteration 144, loss = 0.39612068\n",
      "Iteration 145, loss = 0.39611894\n",
      "Iteration 146, loss = 0.39611136\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69083583\n",
      "Iteration 2, loss = 0.67615494\n",
      "Iteration 3, loss = 0.63597013\n",
      "Iteration 4, loss = 0.56676101\n",
      "Iteration 5, loss = 0.49590892\n",
      "Iteration 6, loss = 0.44687662\n",
      "Iteration 7, loss = 0.41855541\n",
      "Iteration 8, loss = 0.40328922\n",
      "Iteration 9, loss = 0.39496554\n",
      "Iteration 10, loss = 0.38968025\n",
      "Iteration 11, loss = 0.38669813\n",
      "Iteration 12, loss = 0.38481841\n",
      "Iteration 13, loss = 0.38449937\n",
      "Iteration 14, loss = 0.38312786\n",
      "Iteration 15, loss = 0.38240689\n",
      "Iteration 16, loss = 0.38201760\n",
      "Iteration 17, loss = 0.38145081\n",
      "Iteration 18, loss = 0.38156347\n",
      "Iteration 19, loss = 0.38082919\n",
      "Iteration 20, loss = 0.38184244\n",
      "Iteration 21, loss = 0.38050473\n",
      "Iteration 22, loss = 0.37987529\n",
      "Iteration 23, loss = 0.38012104\n",
      "Iteration 24, loss = 0.38003614\n",
      "Iteration 25, loss = 0.37993241\n",
      "Iteration 26, loss = 0.37935033\n",
      "Iteration 27, loss = 0.37939955\n",
      "Iteration 28, loss = 0.37901984\n",
      "Iteration 29, loss = 0.37931438\n",
      "Iteration 30, loss = 0.37954816\n",
      "Iteration 31, loss = 0.37875195\n",
      "Iteration 32, loss = 0.37892918\n",
      "Iteration 33, loss = 0.37867535\n",
      "Iteration 34, loss = 0.37879189\n",
      "Iteration 35, loss = 0.37898438\n",
      "Iteration 36, loss = 0.37852425\n",
      "Iteration 37, loss = 0.37876537\n",
      "Iteration 38, loss = 0.37826733\n",
      "Iteration 39, loss = 0.37810957\n",
      "Iteration 40, loss = 0.37822227\n",
      "Iteration 41, loss = 0.37868595\n",
      "Iteration 42, loss = 0.37796784\n",
      "Iteration 43, loss = 0.37802012\n",
      "Iteration 44, loss = 0.37765562\n",
      "Iteration 45, loss = 0.37793600\n",
      "Iteration 46, loss = 0.37792507\n",
      "Iteration 47, loss = 0.37834689\n",
      "Iteration 48, loss = 0.37789152\n",
      "Iteration 49, loss = 0.37826726\n",
      "Iteration 50, loss = 0.37797614\n",
      "Iteration 51, loss = 0.37762660\n",
      "Iteration 52, loss = 0.37733013\n",
      "Iteration 53, loss = 0.37780735\n",
      "Iteration 54, loss = 0.37761496\n",
      "Iteration 55, loss = 0.37740807\n",
      "Iteration 56, loss = 0.37748837\n",
      "Iteration 57, loss = 0.37748307\n",
      "Iteration 58, loss = 0.37737693\n",
      "Iteration 59, loss = 0.37792676\n",
      "Iteration 60, loss = 0.37754908\n",
      "Iteration 61, loss = 0.37734380\n",
      "Iteration 62, loss = 0.37731295\n",
      "Iteration 63, loss = 0.37708934\n",
      "Iteration 64, loss = 0.37729193\n",
      "Iteration 65, loss = 0.37736158\n",
      "Iteration 66, loss = 0.37745695\n",
      "Iteration 67, loss = 0.37716741\n",
      "Iteration 68, loss = 0.37765164\n",
      "Iteration 69, loss = 0.37739599\n",
      "Iteration 70, loss = 0.37739991\n",
      "Iteration 71, loss = 0.37788021\n",
      "Iteration 72, loss = 0.37687832\n",
      "Iteration 73, loss = 0.37731304\n",
      "Iteration 74, loss = 0.37743395\n",
      "Iteration 75, loss = 0.37732828\n",
      "Iteration 76, loss = 0.37697671\n",
      "Iteration 77, loss = 0.37701316\n",
      "Iteration 78, loss = 0.37718455\n",
      "Iteration 79, loss = 0.37707391\n",
      "Iteration 80, loss = 0.37752097\n",
      "Iteration 81, loss = 0.37720965\n",
      "Iteration 82, loss = 0.37707752\n",
      "Iteration 83, loss = 0.37710152\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69073130\n",
      "Iteration 2, loss = 0.67522560\n",
      "Iteration 3, loss = 0.63386958\n",
      "Iteration 4, loss = 0.56510875\n",
      "Iteration 5, loss = 0.49845435\n",
      "Iteration 6, loss = 0.45331884\n",
      "Iteration 7, loss = 0.42662382\n",
      "Iteration 8, loss = 0.41181862\n",
      "Iteration 9, loss = 0.40322962\n",
      "Iteration 10, loss = 0.39779781\n",
      "Iteration 11, loss = 0.39516561\n",
      "Iteration 12, loss = 0.39291083\n",
      "Iteration 13, loss = 0.39203617\n",
      "Iteration 14, loss = 0.39153400\n",
      "Iteration 15, loss = 0.39077617\n",
      "Iteration 16, loss = 0.38981535\n",
      "Iteration 17, loss = 0.38943269\n",
      "Iteration 18, loss = 0.38931554\n",
      "Iteration 19, loss = 0.38921814\n",
      "Iteration 20, loss = 0.38908051\n",
      "Iteration 21, loss = 0.38879689\n",
      "Iteration 22, loss = 0.38838715\n",
      "Iteration 23, loss = 0.38797033\n",
      "Iteration 24, loss = 0.38815518\n",
      "Iteration 25, loss = 0.38767017\n",
      "Iteration 26, loss = 0.38812665\n",
      "Iteration 27, loss = 0.38691734\n",
      "Iteration 28, loss = 0.38704547\n",
      "Iteration 29, loss = 0.38742879\n",
      "Iteration 30, loss = 0.38798362\n",
      "Iteration 31, loss = 0.38689747\n",
      "Iteration 32, loss = 0.38673256\n",
      "Iteration 33, loss = 0.38698229\n",
      "Iteration 34, loss = 0.38642488\n",
      "Iteration 35, loss = 0.38618109\n",
      "Iteration 36, loss = 0.38642683\n",
      "Iteration 37, loss = 0.38642758\n",
      "Iteration 38, loss = 0.38640812\n",
      "Iteration 39, loss = 0.38670065\n",
      "Iteration 40, loss = 0.38590721\n",
      "Iteration 41, loss = 0.38671173\n",
      "Iteration 42, loss = 0.38598993\n",
      "Iteration 43, loss = 0.38581702\n",
      "Iteration 44, loss = 0.38590395\n",
      "Iteration 45, loss = 0.38596060\n",
      "Iteration 46, loss = 0.38555694\n",
      "Iteration 47, loss = 0.38603681\n",
      "Iteration 48, loss = 0.38570155\n",
      "Iteration 49, loss = 0.38562089\n",
      "Iteration 50, loss = 0.38550947\n",
      "Iteration 51, loss = 0.38529392\n",
      "Iteration 52, loss = 0.38568728\n",
      "Iteration 53, loss = 0.38527526\n",
      "Iteration 54, loss = 0.38576782\n",
      "Iteration 55, loss = 0.38513421\n",
      "Iteration 56, loss = 0.38526710\n",
      "Iteration 57, loss = 0.38556590\n",
      "Iteration 58, loss = 0.38515316\n",
      "Iteration 59, loss = 0.38522854\n",
      "Iteration 60, loss = 0.38573848\n",
      "Iteration 61, loss = 0.38516472\n",
      "Iteration 62, loss = 0.38513919\n",
      "Iteration 63, loss = 0.38503251\n",
      "Iteration 64, loss = 0.38520210\n",
      "Iteration 65, loss = 0.38523586\n",
      "Iteration 66, loss = 0.38577071\n",
      "Iteration 67, loss = 0.38510921\n",
      "Iteration 68, loss = 0.38501356\n",
      "Iteration 69, loss = 0.38533739\n",
      "Iteration 70, loss = 0.38511871\n",
      "Iteration 71, loss = 0.38533905\n",
      "Iteration 72, loss = 0.38488295\n",
      "Iteration 73, loss = 0.38495230\n",
      "Iteration 74, loss = 0.38511736\n",
      "Iteration 75, loss = 0.38484433\n",
      "Iteration 76, loss = 0.38586526\n",
      "Iteration 77, loss = 0.38622921\n",
      "Iteration 78, loss = 0.38563233\n",
      "Iteration 79, loss = 0.38506168\n",
      "Iteration 80, loss = 0.38534932\n",
      "Iteration 81, loss = 0.38497012\n",
      "Iteration 82, loss = 0.38484780\n",
      "Iteration 83, loss = 0.38502689\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69119549\n",
      "Iteration 2, loss = 0.67509385\n",
      "Iteration 3, loss = 0.63505052\n",
      "Iteration 4, loss = 0.57044555\n",
      "Iteration 5, loss = 0.50751497\n",
      "Iteration 6, loss = 0.46352826\n",
      "Iteration 7, loss = 0.43665983\n",
      "Iteration 8, loss = 0.42128479\n",
      "Iteration 9, loss = 0.41274178\n",
      "Iteration 10, loss = 0.40772953\n",
      "Iteration 11, loss = 0.40400156\n",
      "Iteration 12, loss = 0.40272172\n",
      "Iteration 13, loss = 0.40170730\n",
      "Iteration 14, loss = 0.40004732\n",
      "Iteration 15, loss = 0.39945030\n",
      "Iteration 16, loss = 0.39868567\n",
      "Iteration 17, loss = 0.39859946\n",
      "Iteration 18, loss = 0.39782699\n",
      "Iteration 19, loss = 0.39870650\n",
      "Iteration 20, loss = 0.39735340\n",
      "Iteration 21, loss = 0.39690011\n",
      "Iteration 22, loss = 0.39665957\n",
      "Iteration 23, loss = 0.39675527\n",
      "Iteration 24, loss = 0.39649067\n",
      "Iteration 25, loss = 0.39644853\n",
      "Iteration 26, loss = 0.39560111\n",
      "Iteration 27, loss = 0.39611487\n",
      "Iteration 28, loss = 0.39577058\n",
      "Iteration 29, loss = 0.39540520\n",
      "Iteration 30, loss = 0.39510158\n",
      "Iteration 31, loss = 0.39510579\n",
      "Iteration 32, loss = 0.39517475\n",
      "Iteration 33, loss = 0.39483888\n",
      "Iteration 34, loss = 0.39465342\n",
      "Iteration 35, loss = 0.39537889\n",
      "Iteration 36, loss = 0.39406357\n",
      "Iteration 37, loss = 0.39569167\n",
      "Iteration 38, loss = 0.39406126\n",
      "Iteration 39, loss = 0.39421454\n",
      "Iteration 40, loss = 0.39414680\n",
      "Iteration 41, loss = 0.39443290\n",
      "Iteration 42, loss = 0.39395152\n",
      "Iteration 43, loss = 0.39409254\n",
      "Iteration 44, loss = 0.39427548\n",
      "Iteration 45, loss = 0.39459180\n",
      "Iteration 46, loss = 0.39360893\n",
      "Iteration 47, loss = 0.39436130\n",
      "Iteration 48, loss = 0.39350491\n",
      "Iteration 49, loss = 0.39352349\n",
      "Iteration 50, loss = 0.39331223\n",
      "Iteration 51, loss = 0.39362895\n",
      "Iteration 52, loss = 0.39399625\n",
      "Iteration 53, loss = 0.39343902\n",
      "Iteration 54, loss = 0.39402098\n",
      "Iteration 55, loss = 0.39390812\n",
      "Iteration 56, loss = 0.39314466\n",
      "Iteration 57, loss = 0.39327009\n",
      "Iteration 58, loss = 0.39326518\n",
      "Iteration 59, loss = 0.39333992\n",
      "Iteration 60, loss = 0.39346162\n",
      "Iteration 61, loss = 0.39328462\n",
      "Iteration 62, loss = 0.39302229\n",
      "Iteration 63, loss = 0.39321700\n",
      "Iteration 64, loss = 0.39298393\n",
      "Iteration 65, loss = 0.39293452\n",
      "Iteration 66, loss = 0.39319912\n",
      "Iteration 67, loss = 0.39347891\n",
      "Iteration 68, loss = 0.39336937\n",
      "Iteration 69, loss = 0.39324282\n",
      "Iteration 70, loss = 0.39298351\n",
      "Iteration 71, loss = 0.39350394\n",
      "Iteration 72, loss = 0.39345703\n",
      "Iteration 73, loss = 0.39342644\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.67975992\n",
      "Iteration 2, loss = 0.65394503\n",
      "Iteration 3, loss = 0.64025174\n",
      "Iteration 4, loss = 0.62744581\n",
      "Iteration 5, loss = 0.61550372\n",
      "Iteration 6, loss = 0.60434211\n",
      "Iteration 7, loss = 0.59394855\n",
      "Iteration 8, loss = 0.58423548\n",
      "Iteration 9, loss = 0.57510189\n",
      "Iteration 10, loss = 0.56654467\n",
      "Iteration 11, loss = 0.55856361\n",
      "Iteration 12, loss = 0.55101141\n",
      "Iteration 13, loss = 0.54389842\n",
      "Iteration 14, loss = 0.53727232\n",
      "Iteration 15, loss = 0.53097686\n",
      "Iteration 16, loss = 0.52510123\n",
      "Iteration 17, loss = 0.51951399\n",
      "Iteration 18, loss = 0.51425465\n",
      "Iteration 19, loss = 0.50923013\n",
      "Iteration 20, loss = 0.50451989\n",
      "Iteration 21, loss = 0.50003258\n",
      "Iteration 22, loss = 0.49578168\n",
      "Iteration 23, loss = 0.49176602\n",
      "Iteration 24, loss = 0.48798216\n",
      "Iteration 25, loss = 0.48426453\n",
      "Iteration 26, loss = 0.48075867\n",
      "Iteration 27, loss = 0.47746204\n",
      "Iteration 28, loss = 0.47428984\n",
      "Iteration 29, loss = 0.47125927\n",
      "Iteration 30, loss = 0.46838446\n",
      "Iteration 31, loss = 0.46567031\n",
      "Iteration 32, loss = 0.46302711\n",
      "Iteration 33, loss = 0.46051512\n",
      "Iteration 34, loss = 0.45810512\n",
      "Iteration 35, loss = 0.45577369\n",
      "Iteration 36, loss = 0.45359453\n",
      "Iteration 37, loss = 0.45144778\n",
      "Iteration 38, loss = 0.44942404\n",
      "Iteration 39, loss = 0.44744858\n",
      "Iteration 40, loss = 0.44555896\n",
      "Iteration 41, loss = 0.44374348\n",
      "Iteration 42, loss = 0.44199862\n",
      "Iteration 43, loss = 0.44031514\n",
      "Iteration 44, loss = 0.43868818\n",
      "Iteration 45, loss = 0.43713297\n",
      "Iteration 46, loss = 0.43564446\n",
      "Iteration 47, loss = 0.43418430\n",
      "Iteration 48, loss = 0.43277229\n",
      "Iteration 49, loss = 0.43142580\n",
      "Iteration 50, loss = 0.43011476\n",
      "Iteration 51, loss = 0.42887781\n",
      "Iteration 52, loss = 0.42767442\n",
      "Iteration 53, loss = 0.42649751\n",
      "Iteration 54, loss = 0.42534053\n",
      "Iteration 55, loss = 0.42424997\n",
      "Iteration 56, loss = 0.42318692\n",
      "Iteration 57, loss = 0.42219489\n",
      "Iteration 58, loss = 0.42115542\n",
      "Iteration 59, loss = 0.42019172\n",
      "Iteration 60, loss = 0.41927148\n",
      "Iteration 61, loss = 0.41837301\n",
      "Iteration 62, loss = 0.41746161\n",
      "Iteration 63, loss = 0.41660835\n",
      "Iteration 64, loss = 0.41577336\n",
      "Iteration 65, loss = 0.41495857\n",
      "Iteration 66, loss = 0.41417224\n",
      "Iteration 67, loss = 0.41342853\n",
      "Iteration 68, loss = 0.41267280\n",
      "Iteration 69, loss = 0.41197754\n",
      "Iteration 70, loss = 0.41126790\n",
      "Iteration 71, loss = 0.41057622\n",
      "Iteration 72, loss = 0.40991620\n",
      "Iteration 73, loss = 0.40926147\n",
      "Iteration 74, loss = 0.40864785\n",
      "Iteration 75, loss = 0.40803176\n",
      "Iteration 76, loss = 0.40743332\n",
      "Iteration 77, loss = 0.40686185\n",
      "Iteration 78, loss = 0.40628510\n",
      "Iteration 79, loss = 0.40574867\n",
      "Iteration 80, loss = 0.40524613\n",
      "Iteration 81, loss = 0.40471775\n",
      "Iteration 82, loss = 0.40418212\n",
      "Iteration 83, loss = 0.40369929\n",
      "Iteration 84, loss = 0.40321324\n",
      "Iteration 85, loss = 0.40273819\n",
      "Iteration 86, loss = 0.40229465\n",
      "Iteration 87, loss = 0.40183837\n",
      "Iteration 88, loss = 0.40143566\n",
      "Iteration 89, loss = 0.40096474\n",
      "Iteration 90, loss = 0.40054311\n",
      "Iteration 91, loss = 0.40014653\n",
      "Iteration 92, loss = 0.39973768\n",
      "Iteration 93, loss = 0.39935729\n",
      "Iteration 94, loss = 0.39897832\n",
      "Iteration 95, loss = 0.39860988\n",
      "Iteration 96, loss = 0.39823472\n",
      "Iteration 97, loss = 0.39789983\n",
      "Iteration 98, loss = 0.39755490\n",
      "Iteration 99, loss = 0.39723846\n",
      "Iteration 100, loss = 0.39688395\n",
      "Iteration 101, loss = 0.39654767\n",
      "Iteration 102, loss = 0.39625585\n",
      "Iteration 103, loss = 0.39601071\n",
      "Iteration 104, loss = 0.39563180\n",
      "Iteration 105, loss = 0.39532177\n",
      "Iteration 106, loss = 0.39507569\n",
      "Iteration 107, loss = 0.39475189\n",
      "Iteration 108, loss = 0.39450141\n",
      "Iteration 109, loss = 0.39421241\n",
      "Iteration 110, loss = 0.39394840\n",
      "Iteration 111, loss = 0.39366603\n",
      "Iteration 112, loss = 0.39342268\n",
      "Iteration 113, loss = 0.39315615\n",
      "Iteration 114, loss = 0.39292325\n",
      "Iteration 115, loss = 0.39267577\n",
      "Iteration 116, loss = 0.39246617\n",
      "Iteration 117, loss = 0.39222105\n",
      "Iteration 118, loss = 0.39200429\n",
      "Iteration 119, loss = 0.39178613\n",
      "Iteration 120, loss = 0.39158150\n",
      "Iteration 121, loss = 0.39135617\n",
      "Iteration 122, loss = 0.39113210\n",
      "Iteration 123, loss = 0.39091701\n",
      "Iteration 124, loss = 0.39071917\n",
      "Iteration 125, loss = 0.39051080\n",
      "Iteration 126, loss = 0.39035415\n",
      "Iteration 127, loss = 0.39017766\n",
      "Iteration 128, loss = 0.38995232\n",
      "Iteration 129, loss = 0.38977825\n",
      "Iteration 130, loss = 0.38962438\n",
      "Iteration 131, loss = 0.38941100\n",
      "Iteration 132, loss = 0.38924658\n",
      "Iteration 133, loss = 0.38906564\n",
      "Iteration 134, loss = 0.38890162\n",
      "Iteration 135, loss = 0.38874713\n",
      "Iteration 136, loss = 0.38857102\n",
      "Iteration 137, loss = 0.38844559\n",
      "Iteration 138, loss = 0.38827135\n",
      "Iteration 139, loss = 0.38815425\n",
      "Iteration 140, loss = 0.38797416\n",
      "Iteration 141, loss = 0.38782343\n",
      "Iteration 142, loss = 0.38767555\n",
      "Iteration 143, loss = 0.38753662\n",
      "Iteration 144, loss = 0.38741485\n",
      "Iteration 145, loss = 0.38728399\n",
      "Iteration 146, loss = 0.38713573\n",
      "Iteration 147, loss = 0.38701585\n",
      "Iteration 148, loss = 0.38688215\n",
      "Iteration 149, loss = 0.38679029\n",
      "Iteration 150, loss = 0.38661751\n",
      "Iteration 151, loss = 0.38648722\n",
      "Iteration 152, loss = 0.38638827\n",
      "Iteration 153, loss = 0.38625768\n",
      "Iteration 154, loss = 0.38613265\n",
      "Iteration 155, loss = 0.38602040\n",
      "Iteration 156, loss = 0.38591937\n",
      "Iteration 157, loss = 0.38579558\n",
      "Iteration 158, loss = 0.38569763\n",
      "Iteration 159, loss = 0.38559092\n",
      "Iteration 160, loss = 0.38549775\n",
      "Iteration 161, loss = 0.38536754\n",
      "Iteration 162, loss = 0.38527754\n",
      "Iteration 163, loss = 0.38518563\n",
      "Iteration 164, loss = 0.38507237\n",
      "Iteration 165, loss = 0.38500443\n",
      "Iteration 166, loss = 0.38486681\n",
      "Iteration 167, loss = 0.38479484\n",
      "Iteration 168, loss = 0.38469886\n",
      "Iteration 169, loss = 0.38462863\n",
      "Iteration 170, loss = 0.38450799\n",
      "Iteration 171, loss = 0.38442680\n",
      "Iteration 172, loss = 0.38432120\n",
      "Iteration 173, loss = 0.38423057\n",
      "Iteration 174, loss = 0.38415343\n",
      "Iteration 175, loss = 0.38406531\n",
      "Iteration 176, loss = 0.38397813\n",
      "Iteration 177, loss = 0.38389846\n",
      "Iteration 178, loss = 0.38382336\n",
      "Iteration 179, loss = 0.38374538\n",
      "Iteration 180, loss = 0.38366613\n",
      "Iteration 181, loss = 0.38359614\n",
      "Iteration 182, loss = 0.38352467\n",
      "Iteration 183, loss = 0.38344031\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.68096745\n",
      "Iteration 2, loss = 0.65469753\n",
      "Iteration 3, loss = 0.64100819\n",
      "Iteration 4, loss = 0.62834740\n",
      "Iteration 5, loss = 0.61659484\n",
      "Iteration 6, loss = 0.60551340\n",
      "Iteration 7, loss = 0.59529300\n",
      "Iteration 8, loss = 0.58568962\n",
      "Iteration 9, loss = 0.57673775\n",
      "Iteration 10, loss = 0.56831809\n",
      "Iteration 11, loss = 0.56045394\n",
      "Iteration 12, loss = 0.55307844\n",
      "Iteration 13, loss = 0.54614795\n",
      "Iteration 14, loss = 0.53965212\n",
      "Iteration 15, loss = 0.53350272\n",
      "Iteration 16, loss = 0.52775118\n",
      "Iteration 17, loss = 0.52230853\n",
      "Iteration 18, loss = 0.51716000\n",
      "Iteration 19, loss = 0.51232664\n",
      "Iteration 20, loss = 0.50770218\n",
      "Iteration 21, loss = 0.50340349\n",
      "Iteration 22, loss = 0.49928523\n",
      "Iteration 23, loss = 0.49531844\n",
      "Iteration 24, loss = 0.49159389\n",
      "Iteration 25, loss = 0.48807535\n",
      "Iteration 26, loss = 0.48469710\n",
      "Iteration 27, loss = 0.48152682\n",
      "Iteration 28, loss = 0.47846663\n",
      "Iteration 29, loss = 0.47552134\n",
      "Iteration 30, loss = 0.47278048\n",
      "Iteration 31, loss = 0.47011168\n",
      "Iteration 32, loss = 0.46756721\n",
      "Iteration 33, loss = 0.46513418\n",
      "Iteration 34, loss = 0.46277505\n",
      "Iteration 35, loss = 0.46054747\n",
      "Iteration 36, loss = 0.45841713\n",
      "Iteration 37, loss = 0.45636615\n",
      "Iteration 38, loss = 0.45441651\n",
      "Iteration 39, loss = 0.45254788\n",
      "Iteration 40, loss = 0.45074251\n",
      "Iteration 41, loss = 0.44902242\n",
      "Iteration 42, loss = 0.44733150\n",
      "Iteration 43, loss = 0.44569676\n",
      "Iteration 44, loss = 0.44422517\n",
      "Iteration 45, loss = 0.44263920\n",
      "Iteration 46, loss = 0.44121583\n",
      "Iteration 47, loss = 0.43983401\n",
      "Iteration 48, loss = 0.43848258\n",
      "Iteration 49, loss = 0.43719954\n",
      "Iteration 50, loss = 0.43594846\n",
      "Iteration 51, loss = 0.43478730\n",
      "Iteration 52, loss = 0.43357975\n",
      "Iteration 53, loss = 0.43247088\n",
      "Iteration 54, loss = 0.43137450\n",
      "Iteration 55, loss = 0.43035887\n",
      "Iteration 56, loss = 0.42929738\n",
      "Iteration 57, loss = 0.42832837\n",
      "Iteration 58, loss = 0.42737085\n",
      "Iteration 59, loss = 0.42643860\n",
      "Iteration 60, loss = 0.42556361\n",
      "Iteration 61, loss = 0.42472270\n",
      "Iteration 62, loss = 0.42382953\n",
      "Iteration 63, loss = 0.42302247\n",
      "Iteration 64, loss = 0.42223520\n",
      "Iteration 65, loss = 0.42146022\n",
      "Iteration 66, loss = 0.42071577\n",
      "Iteration 67, loss = 0.42002390\n",
      "Iteration 68, loss = 0.41931070\n",
      "Iteration 69, loss = 0.41862675\n",
      "Iteration 70, loss = 0.41795749\n",
      "Iteration 71, loss = 0.41732392\n",
      "Iteration 72, loss = 0.41666434\n",
      "Iteration 73, loss = 0.41604113\n",
      "Iteration 74, loss = 0.41546091\n",
      "Iteration 75, loss = 0.41486290\n",
      "Iteration 76, loss = 0.41433573\n",
      "Iteration 77, loss = 0.41380042\n",
      "Iteration 78, loss = 0.41322637\n",
      "Iteration 79, loss = 0.41269842\n",
      "Iteration 80, loss = 0.41221821\n",
      "Iteration 81, loss = 0.41174334\n",
      "Iteration 82, loss = 0.41122828\n",
      "Iteration 83, loss = 0.41075244\n",
      "Iteration 84, loss = 0.41030182\n",
      "Iteration 85, loss = 0.40983739\n",
      "Iteration 86, loss = 0.40941621\n",
      "Iteration 87, loss = 0.40898637\n",
      "Iteration 88, loss = 0.40860814\n",
      "Iteration 89, loss = 0.40816173\n",
      "Iteration 90, loss = 0.40780168\n",
      "Iteration 91, loss = 0.40739669\n",
      "Iteration 92, loss = 0.40700270\n",
      "Iteration 93, loss = 0.40663664\n",
      "Iteration 94, loss = 0.40631053\n",
      "Iteration 95, loss = 0.40592954\n",
      "Iteration 96, loss = 0.40559791\n",
      "Iteration 97, loss = 0.40530185\n",
      "Iteration 98, loss = 0.40495105\n",
      "Iteration 99, loss = 0.40464713\n",
      "Iteration 100, loss = 0.40430916\n",
      "Iteration 101, loss = 0.40399217\n",
      "Iteration 102, loss = 0.40369241\n",
      "Iteration 103, loss = 0.40339014\n",
      "Iteration 104, loss = 0.40316298\n",
      "Iteration 105, loss = 0.40282448\n",
      "Iteration 106, loss = 0.40256740\n",
      "Iteration 107, loss = 0.40230118\n",
      "Iteration 108, loss = 0.40204782\n",
      "Iteration 109, loss = 0.40180161\n",
      "Iteration 110, loss = 0.40157117\n",
      "Iteration 111, loss = 0.40128480\n",
      "Iteration 112, loss = 0.40102933\n",
      "Iteration 113, loss = 0.40079935\n",
      "Iteration 114, loss = 0.40055583\n",
      "Iteration 115, loss = 0.40036770\n",
      "Iteration 116, loss = 0.40018141\n",
      "Iteration 117, loss = 0.39989102\n",
      "Iteration 118, loss = 0.39969692\n",
      "Iteration 119, loss = 0.39951670\n",
      "Iteration 120, loss = 0.39928459\n",
      "Iteration 121, loss = 0.39908037\n",
      "Iteration 122, loss = 0.39889245\n",
      "Iteration 123, loss = 0.39868796\n",
      "Iteration 124, loss = 0.39849273\n",
      "Iteration 125, loss = 0.39830957\n",
      "Iteration 126, loss = 0.39813553\n",
      "Iteration 127, loss = 0.39795802\n",
      "Iteration 128, loss = 0.39776601\n",
      "Iteration 129, loss = 0.39761456\n",
      "Iteration 130, loss = 0.39743617\n",
      "Iteration 131, loss = 0.39726661\n",
      "Iteration 132, loss = 0.39709438\n",
      "Iteration 133, loss = 0.39694628\n",
      "Iteration 134, loss = 0.39679043\n",
      "Iteration 135, loss = 0.39663167\n",
      "Iteration 136, loss = 0.39650601\n",
      "Iteration 137, loss = 0.39632842\n",
      "Iteration 138, loss = 0.39618630\n",
      "Iteration 139, loss = 0.39607467\n",
      "Iteration 140, loss = 0.39590802\n",
      "Iteration 141, loss = 0.39576972\n",
      "Iteration 142, loss = 0.39563962\n",
      "Iteration 143, loss = 0.39552820\n",
      "Iteration 144, loss = 0.39538837\n",
      "Iteration 145, loss = 0.39527847\n",
      "Iteration 146, loss = 0.39513736\n",
      "Iteration 147, loss = 0.39500512\n",
      "Iteration 148, loss = 0.39494679\n",
      "Iteration 149, loss = 0.39479699\n",
      "Iteration 150, loss = 0.39464379\n",
      "Iteration 151, loss = 0.39454978\n",
      "Iteration 152, loss = 0.39442062\n",
      "Iteration 153, loss = 0.39430720\n",
      "Iteration 154, loss = 0.39419306\n",
      "Iteration 155, loss = 0.39410386\n",
      "Iteration 156, loss = 0.39401995\n",
      "Iteration 157, loss = 0.39386948\n",
      "Iteration 158, loss = 0.39377297\n",
      "Iteration 159, loss = 0.39367799\n",
      "Iteration 160, loss = 0.39357428\n",
      "Iteration 161, loss = 0.39346787\n",
      "Iteration 162, loss = 0.39338984\n",
      "Iteration 163, loss = 0.39329983\n",
      "Iteration 164, loss = 0.39319816\n",
      "Iteration 165, loss = 0.39310170\n",
      "Iteration 166, loss = 0.39300786\n",
      "Iteration 167, loss = 0.39291308\n",
      "Iteration 168, loss = 0.39282606\n",
      "Iteration 169, loss = 0.39274477\n",
      "Iteration 170, loss = 0.39266182\n",
      "Iteration 171, loss = 0.39256056\n",
      "Iteration 172, loss = 0.39248871\n",
      "Iteration 173, loss = 0.39242259\n",
      "Iteration 174, loss = 0.39233004\n",
      "Iteration 175, loss = 0.39226319\n",
      "Iteration 176, loss = 0.39220196\n",
      "Iteration 177, loss = 0.39210147\n",
      "Iteration 178, loss = 0.39203286\n",
      "Iteration 179, loss = 0.39196195\n",
      "Iteration 180, loss = 0.39190970\n",
      "Iteration 181, loss = 0.39187196\n",
      "Iteration 182, loss = 0.39172833\n",
      "Iteration 183, loss = 0.39165790\n",
      "Iteration 184, loss = 0.39158682\n",
      "Iteration 185, loss = 0.39152429\n",
      "Iteration 186, loss = 0.39146731\n",
      "Iteration 187, loss = 0.39140157\n",
      "Iteration 188, loss = 0.39133372\n",
      "Iteration 189, loss = 0.39126538\n",
      "Iteration 190, loss = 0.39121821\n",
      "Iteration 191, loss = 0.39115364\n",
      "Iteration 192, loss = 0.39108148\n",
      "Iteration 193, loss = 0.39102564\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.68141899\n",
      "Iteration 2, loss = 0.65537792\n",
      "Iteration 3, loss = 0.64202212\n",
      "Iteration 4, loss = 0.62960945\n",
      "Iteration 5, loss = 0.61804165\n",
      "Iteration 6, loss = 0.60727575\n",
      "Iteration 7, loss = 0.59722430\n",
      "Iteration 8, loss = 0.58790051\n",
      "Iteration 9, loss = 0.57913998\n",
      "Iteration 10, loss = 0.57095438\n",
      "Iteration 11, loss = 0.56324110\n",
      "Iteration 12, loss = 0.55603865\n",
      "Iteration 13, loss = 0.54929016\n",
      "Iteration 14, loss = 0.54293651\n",
      "Iteration 15, loss = 0.53695108\n",
      "Iteration 16, loss = 0.53131137\n",
      "Iteration 17, loss = 0.52602478\n",
      "Iteration 18, loss = 0.52098861\n",
      "Iteration 19, loss = 0.51627790\n",
      "Iteration 20, loss = 0.51181850\n",
      "Iteration 21, loss = 0.50756766\n",
      "Iteration 22, loss = 0.50361994\n",
      "Iteration 23, loss = 0.49976005\n",
      "Iteration 24, loss = 0.49618870\n",
      "Iteration 25, loss = 0.49268475\n",
      "Iteration 26, loss = 0.48943315\n",
      "Iteration 27, loss = 0.48629896\n",
      "Iteration 28, loss = 0.48335458\n",
      "Iteration 29, loss = 0.48050529\n",
      "Iteration 30, loss = 0.47781597\n",
      "Iteration 31, loss = 0.47527078\n",
      "Iteration 32, loss = 0.47279937\n",
      "Iteration 33, loss = 0.47044763\n",
      "Iteration 34, loss = 0.46820538\n",
      "Iteration 35, loss = 0.46605292\n",
      "Iteration 36, loss = 0.46398667\n",
      "Iteration 37, loss = 0.46207913\n",
      "Iteration 38, loss = 0.46013646\n",
      "Iteration 39, loss = 0.45833888\n",
      "Iteration 40, loss = 0.45658145\n",
      "Iteration 41, loss = 0.45488699\n",
      "Iteration 42, loss = 0.45327073\n",
      "Iteration 43, loss = 0.45170949\n",
      "Iteration 44, loss = 0.45021077\n",
      "Iteration 45, loss = 0.44882629\n",
      "Iteration 46, loss = 0.44738121\n",
      "Iteration 47, loss = 0.44607631\n",
      "Iteration 48, loss = 0.44479248\n",
      "Iteration 49, loss = 0.44355038\n",
      "Iteration 50, loss = 0.44234209\n",
      "Iteration 51, loss = 0.44122230\n",
      "Iteration 52, loss = 0.44013568\n",
      "Iteration 53, loss = 0.43899096\n",
      "Iteration 54, loss = 0.43794244\n",
      "Iteration 55, loss = 0.43694516\n",
      "Iteration 56, loss = 0.43595125\n",
      "Iteration 57, loss = 0.43501159\n",
      "Iteration 58, loss = 0.43413256\n",
      "Iteration 59, loss = 0.43321330\n",
      "Iteration 60, loss = 0.43236328\n",
      "Iteration 61, loss = 0.43157280\n",
      "Iteration 62, loss = 0.43072204\n",
      "Iteration 63, loss = 0.42996186\n",
      "Iteration 64, loss = 0.42919477\n",
      "Iteration 65, loss = 0.42845302\n",
      "Iteration 66, loss = 0.42776148\n",
      "Iteration 67, loss = 0.42706374\n",
      "Iteration 68, loss = 0.42637863\n",
      "Iteration 69, loss = 0.42573738\n",
      "Iteration 70, loss = 0.42513581\n",
      "Iteration 71, loss = 0.42447435\n",
      "Iteration 72, loss = 0.42389743\n",
      "Iteration 73, loss = 0.42330507\n",
      "Iteration 74, loss = 0.42273083\n",
      "Iteration 75, loss = 0.42219151\n",
      "Iteration 76, loss = 0.42165031\n",
      "Iteration 77, loss = 0.42113017\n",
      "Iteration 78, loss = 0.42063949\n",
      "Iteration 79, loss = 0.42014178\n",
      "Iteration 80, loss = 0.41964797\n",
      "Iteration 81, loss = 0.41917497\n",
      "Iteration 82, loss = 0.41877700\n",
      "Iteration 83, loss = 0.41827640\n",
      "Iteration 84, loss = 0.41785854\n",
      "Iteration 85, loss = 0.41745097\n",
      "Iteration 86, loss = 0.41701234\n",
      "Iteration 87, loss = 0.41660787\n",
      "Iteration 88, loss = 0.41621742\n",
      "Iteration 89, loss = 0.41587930\n",
      "Iteration 90, loss = 0.41547331\n",
      "Iteration 91, loss = 0.41509757\n",
      "Iteration 92, loss = 0.41477417\n",
      "Iteration 93, loss = 0.41438582\n",
      "Iteration 94, loss = 0.41405983\n",
      "Iteration 95, loss = 0.41374631\n",
      "Iteration 96, loss = 0.41343404\n",
      "Iteration 97, loss = 0.41308488\n",
      "Iteration 98, loss = 0.41279663\n",
      "Iteration 99, loss = 0.41249017\n",
      "Iteration 100, loss = 0.41217436\n",
      "Iteration 101, loss = 0.41188377\n",
      "Iteration 102, loss = 0.41162659\n",
      "Iteration 103, loss = 0.41132345\n",
      "Iteration 104, loss = 0.41107293\n",
      "Iteration 105, loss = 0.41079431\n",
      "Iteration 106, loss = 0.41055862\n",
      "Iteration 107, loss = 0.41028782\n",
      "Iteration 108, loss = 0.41006800\n",
      "Iteration 109, loss = 0.40979002\n",
      "Iteration 110, loss = 0.40957157\n",
      "Iteration 111, loss = 0.40933085\n",
      "Iteration 112, loss = 0.40910751\n",
      "Iteration 113, loss = 0.40888348\n",
      "Iteration 114, loss = 0.40865985\n",
      "Iteration 115, loss = 0.40843719\n",
      "Iteration 116, loss = 0.40824072\n",
      "Iteration 117, loss = 0.40805422\n",
      "Iteration 118, loss = 0.40782545\n",
      "Iteration 119, loss = 0.40766045\n",
      "Iteration 120, loss = 0.40745634\n",
      "Iteration 121, loss = 0.40727225\n",
      "Iteration 122, loss = 0.40707183\n",
      "Iteration 123, loss = 0.40692628\n",
      "Iteration 124, loss = 0.40675584\n",
      "Iteration 125, loss = 0.40652470\n",
      "Iteration 126, loss = 0.40636559\n",
      "Iteration 127, loss = 0.40620485\n",
      "Iteration 128, loss = 0.40603637\n",
      "Iteration 129, loss = 0.40587451\n",
      "Iteration 130, loss = 0.40573880\n",
      "Iteration 131, loss = 0.40558485\n",
      "Iteration 132, loss = 0.40540655\n",
      "Iteration 133, loss = 0.40530906\n",
      "Iteration 134, loss = 0.40512687\n",
      "Iteration 135, loss = 0.40497159\n",
      "Iteration 136, loss = 0.40486785\n",
      "Iteration 137, loss = 0.40469542\n",
      "Iteration 138, loss = 0.40458837\n",
      "Iteration 139, loss = 0.40442025\n",
      "Iteration 140, loss = 0.40428210\n",
      "Iteration 141, loss = 0.40415669\n",
      "Iteration 142, loss = 0.40404056\n",
      "Iteration 143, loss = 0.40391945\n",
      "Iteration 144, loss = 0.40379621\n",
      "Iteration 145, loss = 0.40369198\n",
      "Iteration 146, loss = 0.40360261\n",
      "Iteration 147, loss = 0.40348551\n",
      "Iteration 148, loss = 0.40330754\n",
      "Iteration 149, loss = 0.40321345\n",
      "Iteration 150, loss = 0.40309030\n",
      "Iteration 151, loss = 0.40304496\n",
      "Iteration 152, loss = 0.40291144\n",
      "Iteration 153, loss = 0.40279232\n",
      "Iteration 154, loss = 0.40271245\n",
      "Iteration 155, loss = 0.40258354\n",
      "Iteration 156, loss = 0.40245046\n",
      "Iteration 157, loss = 0.40241660\n",
      "Iteration 158, loss = 0.40228584\n",
      "Iteration 159, loss = 0.40217786\n",
      "Iteration 160, loss = 0.40209015\n",
      "Iteration 161, loss = 0.40198921\n",
      "Iteration 162, loss = 0.40193369\n",
      "Iteration 163, loss = 0.40184108\n",
      "Iteration 164, loss = 0.40174933\n",
      "Iteration 165, loss = 0.40164994\n",
      "Iteration 166, loss = 0.40155141\n",
      "Iteration 167, loss = 0.40147067\n",
      "Iteration 168, loss = 0.40138875\n",
      "Iteration 169, loss = 0.40130921\n",
      "Iteration 170, loss = 0.40122008\n",
      "Iteration 171, loss = 0.40114122\n",
      "Iteration 172, loss = 0.40107690\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.67807457\n",
      "Iteration 2, loss = 0.64400744\n",
      "Iteration 3, loss = 0.61406341\n",
      "Iteration 4, loss = 0.58774510\n",
      "Iteration 5, loss = 0.56469772\n",
      "Iteration 6, loss = 0.54437827\n",
      "Iteration 7, loss = 0.52667325\n",
      "Iteration 8, loss = 0.51113013\n",
      "Iteration 9, loss = 0.49756639\n",
      "Iteration 10, loss = 0.48556221\n",
      "Iteration 11, loss = 0.47518821\n",
      "Iteration 12, loss = 0.46587686\n",
      "Iteration 13, loss = 0.45759234\n",
      "Iteration 14, loss = 0.45031285\n",
      "Iteration 15, loss = 0.44367817\n",
      "Iteration 16, loss = 0.43799484\n",
      "Iteration 17, loss = 0.43278609\n",
      "Iteration 18, loss = 0.42811299\n",
      "Iteration 19, loss = 0.42370272\n",
      "Iteration 20, loss = 0.41986931\n",
      "Iteration 21, loss = 0.41628781\n",
      "Iteration 22, loss = 0.41332541\n",
      "Iteration 23, loss = 0.41036882\n",
      "Iteration 24, loss = 0.40794541\n",
      "Iteration 25, loss = 0.40544150\n",
      "Iteration 26, loss = 0.40306694\n",
      "Iteration 27, loss = 0.40134183\n",
      "Iteration 28, loss = 0.39936827\n",
      "Iteration 29, loss = 0.39774737\n",
      "Iteration 30, loss = 0.39620493\n",
      "Iteration 31, loss = 0.39483038\n",
      "Iteration 32, loss = 0.39353154\n",
      "Iteration 33, loss = 0.39234303\n",
      "Iteration 34, loss = 0.39125778\n",
      "Iteration 35, loss = 0.39021676\n",
      "Iteration 36, loss = 0.38924239\n",
      "Iteration 37, loss = 0.38840829\n",
      "Iteration 38, loss = 0.38768012\n",
      "Iteration 39, loss = 0.38697751\n",
      "Iteration 40, loss = 0.38633819\n",
      "Iteration 41, loss = 0.38563294\n",
      "Iteration 42, loss = 0.38497561\n",
      "Iteration 43, loss = 0.38454556\n",
      "Iteration 44, loss = 0.38400243\n",
      "Iteration 45, loss = 0.38357637\n",
      "Iteration 46, loss = 0.38303972\n",
      "Iteration 47, loss = 0.38270372\n",
      "Iteration 48, loss = 0.38229749\n",
      "Iteration 49, loss = 0.38189062\n",
      "Iteration 50, loss = 0.38165360\n",
      "Iteration 51, loss = 0.38134230\n",
      "Iteration 52, loss = 0.38112411\n",
      "Iteration 53, loss = 0.38087168\n",
      "Iteration 54, loss = 0.38051023\n",
      "Iteration 55, loss = 0.38029303\n",
      "Iteration 56, loss = 0.38010704\n",
      "Iteration 57, loss = 0.38025077\n",
      "Iteration 58, loss = 0.37969452\n",
      "Iteration 59, loss = 0.37958606\n",
      "Iteration 60, loss = 0.37937637\n",
      "Iteration 61, loss = 0.37933653\n",
      "Iteration 62, loss = 0.37904805\n",
      "Iteration 63, loss = 0.37898092\n",
      "Iteration 64, loss = 0.37879118\n",
      "Iteration 65, loss = 0.37861754\n",
      "Iteration 66, loss = 0.37852689\n",
      "Iteration 67, loss = 0.37842127\n",
      "Iteration 68, loss = 0.37833537\n",
      "Iteration 69, loss = 0.37828527\n",
      "Iteration 70, loss = 0.37826140\n",
      "Iteration 71, loss = 0.37808661\n",
      "Iteration 72, loss = 0.37799867\n",
      "Iteration 73, loss = 0.37784685\n",
      "Iteration 74, loss = 0.37783545\n",
      "Iteration 75, loss = 0.37772873\n",
      "Iteration 76, loss = 0.37759063\n",
      "Iteration 77, loss = 0.37771393\n",
      "Iteration 78, loss = 0.37750678\n",
      "Iteration 79, loss = 0.37755491\n",
      "Iteration 80, loss = 0.37750775\n",
      "Iteration 81, loss = 0.37756435\n",
      "Iteration 82, loss = 0.37726079\n",
      "Iteration 83, loss = 0.37729369\n",
      "Iteration 84, loss = 0.37728180\n",
      "Iteration 85, loss = 0.37719325\n",
      "Iteration 86, loss = 0.37716571\n",
      "Iteration 87, loss = 0.37729019\n",
      "Iteration 88, loss = 0.37710237\n",
      "Iteration 89, loss = 0.37702601\n",
      "Iteration 90, loss = 0.37695806\n",
      "Iteration 91, loss = 0.37684368\n",
      "Iteration 92, loss = 0.37695305\n",
      "Iteration 93, loss = 0.37692174\n",
      "Iteration 94, loss = 0.37674693\n",
      "Iteration 95, loss = 0.37686086\n",
      "Iteration 96, loss = 0.37677293\n",
      "Iteration 97, loss = 0.37684342\n",
      "Iteration 98, loss = 0.37679048\n",
      "Iteration 99, loss = 0.37688432\n",
      "Iteration 100, loss = 0.37680401\n",
      "Iteration 101, loss = 0.37662587\n",
      "Iteration 102, loss = 0.37664231\n",
      "Iteration 103, loss = 0.37724151\n",
      "Iteration 104, loss = 0.37658097\n",
      "Iteration 105, loss = 0.37653200\n",
      "Iteration 106, loss = 0.37641568\n",
      "Iteration 107, loss = 0.37663919\n",
      "Iteration 108, loss = 0.37659403\n",
      "Iteration 109, loss = 0.37656416\n",
      "Iteration 110, loss = 0.37636102\n",
      "Iteration 111, loss = 0.37647009\n",
      "Iteration 112, loss = 0.37644802\n",
      "Iteration 113, loss = 0.37639419\n",
      "Iteration 114, loss = 0.37639010\n",
      "Iteration 115, loss = 0.37636705\n",
      "Iteration 116, loss = 0.37638774\n",
      "Iteration 117, loss = 0.37643378\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.68079110\n",
      "Iteration 2, loss = 0.64886648\n",
      "Iteration 3, loss = 0.62025700\n",
      "Iteration 4, loss = 0.59456385\n",
      "Iteration 5, loss = 0.57153411\n",
      "Iteration 6, loss = 0.55101340\n",
      "Iteration 7, loss = 0.53323600\n",
      "Iteration 8, loss = 0.51752472\n",
      "Iteration 9, loss = 0.50408670\n",
      "Iteration 10, loss = 0.49201328\n",
      "Iteration 11, loss = 0.48148173\n",
      "Iteration 12, loss = 0.47216394\n",
      "Iteration 13, loss = 0.46401532\n",
      "Iteration 14, loss = 0.45668856\n",
      "Iteration 15, loss = 0.45016583\n",
      "Iteration 16, loss = 0.44455048\n",
      "Iteration 17, loss = 0.43945470\n",
      "Iteration 18, loss = 0.43476812\n",
      "Iteration 19, loss = 0.43062741\n",
      "Iteration 20, loss = 0.42685995\n",
      "Iteration 21, loss = 0.42360773\n",
      "Iteration 22, loss = 0.42066468\n",
      "Iteration 23, loss = 0.41776444\n",
      "Iteration 24, loss = 0.41517724\n",
      "Iteration 25, loss = 0.41293290\n",
      "Iteration 26, loss = 0.41090960\n",
      "Iteration 27, loss = 0.40907230\n",
      "Iteration 28, loss = 0.40736176\n",
      "Iteration 29, loss = 0.40565292\n",
      "Iteration 30, loss = 0.40432061\n",
      "Iteration 31, loss = 0.40286849\n",
      "Iteration 32, loss = 0.40179114\n",
      "Iteration 33, loss = 0.40050933\n",
      "Iteration 34, loss = 0.39946918\n",
      "Iteration 35, loss = 0.39856653\n",
      "Iteration 36, loss = 0.39768050\n",
      "Iteration 37, loss = 0.39682209\n",
      "Iteration 38, loss = 0.39608962\n",
      "Iteration 39, loss = 0.39541502\n",
      "Iteration 40, loss = 0.39474470\n",
      "Iteration 41, loss = 0.39432655\n",
      "Iteration 42, loss = 0.39357517\n",
      "Iteration 43, loss = 0.39315242\n",
      "Iteration 44, loss = 0.39292430\n",
      "Iteration 45, loss = 0.39214076\n",
      "Iteration 46, loss = 0.39173812\n",
      "Iteration 47, loss = 0.39139450\n",
      "Iteration 48, loss = 0.39097310\n",
      "Iteration 49, loss = 0.39065984\n",
      "Iteration 50, loss = 0.39034745\n",
      "Iteration 51, loss = 0.39021275\n",
      "Iteration 52, loss = 0.38978118\n",
      "Iteration 53, loss = 0.38954283\n",
      "Iteration 54, loss = 0.38928077\n",
      "Iteration 55, loss = 0.38923245\n",
      "Iteration 56, loss = 0.38874895\n",
      "Iteration 57, loss = 0.38864057\n",
      "Iteration 58, loss = 0.38846868\n",
      "Iteration 59, loss = 0.38829711\n",
      "Iteration 60, loss = 0.38807913\n",
      "Iteration 61, loss = 0.38803962\n",
      "Iteration 62, loss = 0.38782672\n",
      "Iteration 63, loss = 0.38759449\n",
      "Iteration 64, loss = 0.38749273\n",
      "Iteration 65, loss = 0.38735149\n",
      "Iteration 66, loss = 0.38724186\n",
      "Iteration 67, loss = 0.38723244\n",
      "Iteration 68, loss = 0.38700069\n",
      "Iteration 69, loss = 0.38684823\n",
      "Iteration 70, loss = 0.38708626\n",
      "Iteration 71, loss = 0.38674504\n",
      "Iteration 72, loss = 0.38650689\n",
      "Iteration 73, loss = 0.38658982\n",
      "Iteration 74, loss = 0.38651615\n",
      "Iteration 75, loss = 0.38631059\n",
      "Iteration 76, loss = 0.38634127\n",
      "Iteration 77, loss = 0.38661436\n",
      "Iteration 78, loss = 0.38619108\n",
      "Iteration 79, loss = 0.38613281\n",
      "Iteration 80, loss = 0.38614142\n",
      "Iteration 81, loss = 0.38617317\n",
      "Iteration 82, loss = 0.38598187\n",
      "Iteration 83, loss = 0.38585702\n",
      "Iteration 84, loss = 0.38583391\n",
      "Iteration 85, loss = 0.38571772\n",
      "Iteration 86, loss = 0.38572335\n",
      "Iteration 87, loss = 0.38567682\n",
      "Iteration 88, loss = 0.38551981\n",
      "Iteration 89, loss = 0.38567635\n",
      "Iteration 90, loss = 0.38562767\n",
      "Iteration 91, loss = 0.38567658\n",
      "Iteration 92, loss = 0.38545850\n",
      "Iteration 93, loss = 0.38537939\n",
      "Iteration 94, loss = 0.38554306\n",
      "Iteration 95, loss = 0.38542791\n",
      "Iteration 96, loss = 0.38530168\n",
      "Iteration 97, loss = 0.38538690\n",
      "Iteration 98, loss = 0.38540239\n",
      "Iteration 99, loss = 0.38549597\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.68190202\n",
      "Iteration 2, loss = 0.65073910\n",
      "Iteration 3, loss = 0.62273679\n",
      "Iteration 4, loss = 0.59721567\n",
      "Iteration 5, loss = 0.57470598\n",
      "Iteration 6, loss = 0.55489499\n",
      "Iteration 7, loss = 0.53742411\n",
      "Iteration 8, loss = 0.52220870\n",
      "Iteration 9, loss = 0.50896895\n",
      "Iteration 10, loss = 0.49721758\n",
      "Iteration 11, loss = 0.48696661\n",
      "Iteration 12, loss = 0.47775340\n",
      "Iteration 13, loss = 0.46979228\n",
      "Iteration 14, loss = 0.46281380\n",
      "Iteration 15, loss = 0.45650198\n",
      "Iteration 16, loss = 0.45083504\n",
      "Iteration 17, loss = 0.44587572\n",
      "Iteration 18, loss = 0.44136086\n",
      "Iteration 19, loss = 0.43728018\n",
      "Iteration 20, loss = 0.43373995\n",
      "Iteration 21, loss = 0.43039373\n",
      "Iteration 22, loss = 0.42755351\n",
      "Iteration 23, loss = 0.42486642\n",
      "Iteration 24, loss = 0.42252297\n",
      "Iteration 25, loss = 0.42019045\n",
      "Iteration 26, loss = 0.41827598\n",
      "Iteration 27, loss = 0.41645715\n",
      "Iteration 28, loss = 0.41478036\n",
      "Iteration 29, loss = 0.41325958\n",
      "Iteration 30, loss = 0.41180866\n",
      "Iteration 31, loss = 0.41073431\n",
      "Iteration 32, loss = 0.40939104\n",
      "Iteration 33, loss = 0.40831178\n",
      "Iteration 34, loss = 0.40732349\n",
      "Iteration 35, loss = 0.40645662\n",
      "Iteration 36, loss = 0.40562145\n",
      "Iteration 37, loss = 0.40510223\n",
      "Iteration 38, loss = 0.40409920\n",
      "Iteration 39, loss = 0.40366960\n",
      "Iteration 40, loss = 0.40303133\n",
      "Iteration 41, loss = 0.40235032\n",
      "Iteration 42, loss = 0.40178923\n",
      "Iteration 43, loss = 0.40121581\n",
      "Iteration 44, loss = 0.40084987\n",
      "Iteration 45, loss = 0.40060551\n",
      "Iteration 46, loss = 0.40000569\n",
      "Iteration 47, loss = 0.39964032\n",
      "Iteration 48, loss = 0.39927547\n",
      "Iteration 49, loss = 0.39899084\n",
      "Iteration 50, loss = 0.39864126\n",
      "Iteration 51, loss = 0.39839355\n",
      "Iteration 52, loss = 0.39829484\n",
      "Iteration 53, loss = 0.39791683\n",
      "Iteration 54, loss = 0.39758068\n",
      "Iteration 55, loss = 0.39742602\n",
      "Iteration 56, loss = 0.39718465\n",
      "Iteration 57, loss = 0.39697820\n",
      "Iteration 58, loss = 0.39690362\n",
      "Iteration 59, loss = 0.39659051\n",
      "Iteration 60, loss = 0.39647923\n",
      "Iteration 61, loss = 0.39641210\n",
      "Iteration 62, loss = 0.39622671\n",
      "Iteration 63, loss = 0.39607690\n",
      "Iteration 64, loss = 0.39594985\n",
      "Iteration 65, loss = 0.39572059\n",
      "Iteration 66, loss = 0.39565845\n",
      "Iteration 67, loss = 0.39557552\n",
      "Iteration 68, loss = 0.39545524\n",
      "Iteration 69, loss = 0.39535731\n",
      "Iteration 70, loss = 0.39520980\n",
      "Iteration 71, loss = 0.39527767\n",
      "Iteration 72, loss = 0.39509732\n",
      "Iteration 73, loss = 0.39488286\n",
      "Iteration 74, loss = 0.39485619\n",
      "Iteration 75, loss = 0.39485989\n",
      "Iteration 76, loss = 0.39472211\n",
      "Iteration 77, loss = 0.39464351\n",
      "Iteration 78, loss = 0.39463017\n",
      "Iteration 79, loss = 0.39466736\n",
      "Iteration 80, loss = 0.39443944\n",
      "Iteration 81, loss = 0.39436435\n",
      "Iteration 82, loss = 0.39453444\n",
      "Iteration 83, loss = 0.39439529\n",
      "Iteration 84, loss = 0.39415814\n",
      "Iteration 85, loss = 0.39415935\n",
      "Iteration 86, loss = 0.39415090\n",
      "Iteration 87, loss = 0.39404338\n",
      "Iteration 88, loss = 0.39395586\n",
      "Iteration 89, loss = 0.39411265\n",
      "Iteration 90, loss = 0.39391704\n",
      "Iteration 91, loss = 0.39387511\n",
      "Iteration 92, loss = 0.39394940\n",
      "Iteration 93, loss = 0.39386962\n",
      "Iteration 94, loss = 0.39367868\n",
      "Iteration 95, loss = 0.39383694\n",
      "Iteration 96, loss = 0.39378562\n",
      "Iteration 97, loss = 0.39356794\n",
      "Iteration 98, loss = 0.39370987\n",
      "Iteration 99, loss = 0.39357397\n",
      "Iteration 100, loss = 0.39353293\n",
      "Iteration 101, loss = 0.39343560\n",
      "Iteration 102, loss = 0.39354556\n",
      "Iteration 103, loss = 0.39329954\n",
      "Iteration 104, loss = 0.39343984\n",
      "Iteration 105, loss = 0.39334460\n",
      "Iteration 106, loss = 0.39344437\n",
      "Iteration 107, loss = 0.39324978\n",
      "Iteration 108, loss = 0.39329903\n",
      "Iteration 109, loss = 0.39330371\n",
      "Iteration 110, loss = 0.39325996\n",
      "Iteration 111, loss = 0.39322771\n",
      "Iteration 112, loss = 0.39316355\n",
      "Iteration 113, loss = 0.39323610\n",
      "Iteration 114, loss = 0.39307995\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.68841564\n",
      "Iteration 2, loss = 0.68216130\n",
      "Iteration 3, loss = 0.67642741\n",
      "Iteration 4, loss = 0.66764171\n",
      "Iteration 5, loss = 0.65387053\n",
      "Iteration 6, loss = 0.63337154\n",
      "Iteration 7, loss = 0.60517715\n",
      "Iteration 8, loss = 0.57009812\n",
      "Iteration 9, loss = 0.53211884\n",
      "Iteration 10, loss = 0.49643321\n",
      "Iteration 11, loss = 0.46645458\n",
      "Iteration 12, loss = 0.44309323\n",
      "Iteration 13, loss = 0.42582648\n",
      "Iteration 14, loss = 0.41314236\n",
      "Iteration 15, loss = 0.40427883\n",
      "Iteration 16, loss = 0.39788267\n",
      "Iteration 17, loss = 0.39320954\n",
      "Iteration 18, loss = 0.39002061\n",
      "Iteration 19, loss = 0.38758639\n",
      "Iteration 20, loss = 0.38572679\n",
      "Iteration 21, loss = 0.38432201\n",
      "Iteration 22, loss = 0.38318995\n",
      "Iteration 23, loss = 0.38260901\n",
      "Iteration 24, loss = 0.38214105\n",
      "Iteration 25, loss = 0.38140704\n",
      "Iteration 26, loss = 0.38137826\n",
      "Iteration 27, loss = 0.38116249\n",
      "Iteration 28, loss = 0.38074983\n",
      "Iteration 29, loss = 0.38060506\n",
      "Iteration 30, loss = 0.38021371\n",
      "Iteration 31, loss = 0.38017849\n",
      "Iteration 32, loss = 0.38028634\n",
      "Iteration 33, loss = 0.38012322\n",
      "Iteration 34, loss = 0.37987506\n",
      "Iteration 35, loss = 0.37991734\n",
      "Iteration 36, loss = 0.37980816\n",
      "Iteration 37, loss = 0.37966612\n",
      "Iteration 38, loss = 0.37961817\n",
      "Iteration 39, loss = 0.37942700\n",
      "Iteration 40, loss = 0.37955147\n",
      "Iteration 41, loss = 0.37940958\n",
      "Iteration 42, loss = 0.37936356\n",
      "Iteration 43, loss = 0.37919790\n",
      "Iteration 44, loss = 0.37920192\n",
      "Iteration 45, loss = 0.37915217\n",
      "Iteration 46, loss = 0.37906865\n",
      "Iteration 47, loss = 0.37922858\n",
      "Iteration 48, loss = 0.37895734\n",
      "Iteration 49, loss = 0.37901374\n",
      "Iteration 50, loss = 0.37914627\n",
      "Iteration 51, loss = 0.37882621\n",
      "Iteration 52, loss = 0.37920932\n",
      "Iteration 53, loss = 0.37910679\n",
      "Iteration 54, loss = 0.37879544\n",
      "Iteration 55, loss = 0.37856595\n",
      "Iteration 56, loss = 0.37882615\n",
      "Iteration 57, loss = 0.37875886\n",
      "Iteration 58, loss = 0.37869765\n",
      "Iteration 59, loss = 0.37866815\n",
      "Iteration 60, loss = 0.37857029\n",
      "Iteration 61, loss = 0.37847577\n",
      "Iteration 62, loss = 0.37865863\n",
      "Iteration 63, loss = 0.37846125\n",
      "Iteration 64, loss = 0.37856356\n",
      "Iteration 65, loss = 0.37841575\n",
      "Iteration 66, loss = 0.37845668\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.68822958\n",
      "Iteration 2, loss = 0.68241533\n",
      "Iteration 3, loss = 0.67681684\n",
      "Iteration 4, loss = 0.66839671\n",
      "Iteration 5, loss = 0.65518348\n",
      "Iteration 6, loss = 0.63546536\n",
      "Iteration 7, loss = 0.60828081\n",
      "Iteration 8, loss = 0.57439183\n",
      "Iteration 9, loss = 0.53759866\n",
      "Iteration 10, loss = 0.50295747\n",
      "Iteration 11, loss = 0.47359824\n",
      "Iteration 12, loss = 0.45052327\n",
      "Iteration 13, loss = 0.43344702\n",
      "Iteration 14, loss = 0.42096707\n",
      "Iteration 15, loss = 0.41207872\n",
      "Iteration 16, loss = 0.40564408\n",
      "Iteration 17, loss = 0.40100147\n",
      "Iteration 18, loss = 0.39791801\n",
      "Iteration 19, loss = 0.39533509\n",
      "Iteration 20, loss = 0.39377329\n",
      "Iteration 21, loss = 0.39244970\n",
      "Iteration 22, loss = 0.39110773\n",
      "Iteration 23, loss = 0.39059384\n",
      "Iteration 24, loss = 0.39013459\n",
      "Iteration 25, loss = 0.38948080\n",
      "Iteration 26, loss = 0.38949312\n",
      "Iteration 27, loss = 0.38884630\n",
      "Iteration 28, loss = 0.38843872\n",
      "Iteration 29, loss = 0.38849782\n",
      "Iteration 30, loss = 0.38819534\n",
      "Iteration 31, loss = 0.38814165\n",
      "Iteration 32, loss = 0.38803061\n",
      "Iteration 33, loss = 0.38795644\n",
      "Iteration 34, loss = 0.38778975\n",
      "Iteration 35, loss = 0.38782776\n",
      "Iteration 36, loss = 0.38762762\n",
      "Iteration 37, loss = 0.38766239\n",
      "Iteration 38, loss = 0.38747162\n",
      "Iteration 39, loss = 0.38734822\n",
      "Iteration 40, loss = 0.38731361\n",
      "Iteration 41, loss = 0.38737696\n",
      "Iteration 42, loss = 0.38754042\n",
      "Iteration 43, loss = 0.38758268\n",
      "Iteration 44, loss = 0.38698611\n",
      "Iteration 45, loss = 0.38698920\n",
      "Iteration 46, loss = 0.38688694\n",
      "Iteration 47, loss = 0.38716671\n",
      "Iteration 48, loss = 0.38670458\n",
      "Iteration 49, loss = 0.38677806\n",
      "Iteration 50, loss = 0.38686869\n",
      "Iteration 51, loss = 0.38677408\n",
      "Iteration 52, loss = 0.38666142\n",
      "Iteration 53, loss = 0.38648925\n",
      "Iteration 54, loss = 0.38649243\n",
      "Iteration 55, loss = 0.38668190\n",
      "Iteration 56, loss = 0.38637701\n",
      "Iteration 57, loss = 0.38626065\n",
      "Iteration 58, loss = 0.38666596\n",
      "Iteration 59, loss = 0.38630333\n",
      "Iteration 60, loss = 0.38616824\n",
      "Iteration 61, loss = 0.38608873\n",
      "Iteration 62, loss = 0.38625144\n",
      "Iteration 63, loss = 0.38610635\n",
      "Iteration 64, loss = 0.38608457\n",
      "Iteration 65, loss = 0.38613361\n",
      "Iteration 66, loss = 0.38634273\n",
      "Iteration 67, loss = 0.38615062\n",
      "Iteration 68, loss = 0.38635992\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.68810620\n",
      "Iteration 2, loss = 0.68258327\n",
      "Iteration 3, loss = 0.67722208\n",
      "Iteration 4, loss = 0.66908643\n",
      "Iteration 5, loss = 0.65651651\n",
      "Iteration 6, loss = 0.63784860\n",
      "Iteration 7, loss = 0.61169489\n",
      "Iteration 8, loss = 0.57937503\n",
      "Iteration 9, loss = 0.54423145\n",
      "Iteration 10, loss = 0.51035367\n",
      "Iteration 11, loss = 0.48171439\n",
      "Iteration 12, loss = 0.45874383\n",
      "Iteration 13, loss = 0.44184229\n",
      "Iteration 14, loss = 0.42943227\n",
      "Iteration 15, loss = 0.42070313\n",
      "Iteration 16, loss = 0.41436497\n",
      "Iteration 17, loss = 0.40987720\n",
      "Iteration 18, loss = 0.40680035\n",
      "Iteration 19, loss = 0.40412053\n",
      "Iteration 20, loss = 0.40254173\n",
      "Iteration 21, loss = 0.40121536\n",
      "Iteration 22, loss = 0.40050828\n",
      "Iteration 23, loss = 0.39945779\n",
      "Iteration 24, loss = 0.39927996\n",
      "Iteration 25, loss = 0.39878746\n",
      "Iteration 26, loss = 0.39813176\n",
      "Iteration 27, loss = 0.39784088\n",
      "Iteration 28, loss = 0.39729856\n",
      "Iteration 29, loss = 0.39719117\n",
      "Iteration 30, loss = 0.39718050\n",
      "Iteration 31, loss = 0.39685827\n",
      "Iteration 32, loss = 0.39674880\n",
      "Iteration 33, loss = 0.39655400\n",
      "Iteration 34, loss = 0.39632546\n",
      "Iteration 35, loss = 0.39606382\n",
      "Iteration 36, loss = 0.39644994\n",
      "Iteration 37, loss = 0.39599617\n",
      "Iteration 38, loss = 0.39591218\n",
      "Iteration 39, loss = 0.39583251\n",
      "Iteration 40, loss = 0.39581098\n",
      "Iteration 41, loss = 0.39550899\n",
      "Iteration 42, loss = 0.39571545\n",
      "Iteration 43, loss = 0.39553918\n",
      "Iteration 44, loss = 0.39514761\n",
      "Iteration 45, loss = 0.39561974\n",
      "Iteration 46, loss = 0.39535259\n",
      "Iteration 47, loss = 0.39503604\n",
      "Iteration 48, loss = 0.39516770\n",
      "Iteration 49, loss = 0.39501111\n",
      "Iteration 50, loss = 0.39513352\n",
      "Iteration 51, loss = 0.39551336\n",
      "Iteration 52, loss = 0.39469920\n",
      "Iteration 53, loss = 0.39470720\n",
      "Iteration 54, loss = 0.39487901\n",
      "Iteration 55, loss = 0.39460878\n",
      "Iteration 56, loss = 0.39460283\n",
      "Iteration 57, loss = 0.39464723\n",
      "Iteration 58, loss = 0.39431230\n",
      "Iteration 59, loss = 0.39460884\n",
      "Iteration 60, loss = 0.39475830\n",
      "Iteration 61, loss = 0.39467492\n",
      "Iteration 62, loss = 0.39447209\n",
      "Iteration 63, loss = 0.39441660\n",
      "Iteration 64, loss = 0.39423076\n",
      "Iteration 65, loss = 0.39420997\n",
      "Iteration 66, loss = 0.39427916\n",
      "Iteration 67, loss = 0.39439110\n",
      "Iteration 68, loss = 0.39427338\n",
      "Iteration 69, loss = 0.39425746\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.68100845\n",
      "Iteration 2, loss = 0.63404398\n",
      "Iteration 3, loss = 0.54656853\n",
      "Iteration 4, loss = 0.47060203\n",
      "Iteration 5, loss = 0.42744121\n",
      "Iteration 6, loss = 0.40492276\n",
      "Iteration 7, loss = 0.39397476\n",
      "Iteration 8, loss = 0.38755798\n",
      "Iteration 9, loss = 0.38461604\n",
      "Iteration 10, loss = 0.38294338\n",
      "Iteration 11, loss = 0.38156421\n",
      "Iteration 12, loss = 0.38115757\n",
      "Iteration 13, loss = 0.38117256\n",
      "Iteration 14, loss = 0.38073828\n",
      "Iteration 15, loss = 0.38075169\n",
      "Iteration 16, loss = 0.38041988\n",
      "Iteration 17, loss = 0.38011482\n",
      "Iteration 18, loss = 0.38000388\n",
      "Iteration 19, loss = 0.38003587\n",
      "Iteration 20, loss = 0.37998434\n",
      "Iteration 21, loss = 0.37965517\n",
      "Iteration 22, loss = 0.37926391\n",
      "Iteration 23, loss = 0.37935486\n",
      "Iteration 24, loss = 0.37976207\n",
      "Iteration 25, loss = 0.37918694\n",
      "Iteration 26, loss = 0.37964726\n",
      "Iteration 27, loss = 0.37970423\n",
      "Iteration 28, loss = 0.37922693\n",
      "Iteration 29, loss = 0.37923918\n",
      "Iteration 30, loss = 0.37898761\n",
      "Iteration 31, loss = 0.37903398\n",
      "Iteration 32, loss = 0.37945897\n",
      "Iteration 33, loss = 0.37927947\n",
      "Iteration 34, loss = 0.37906538\n",
      "Iteration 35, loss = 0.37919149\n",
      "Iteration 36, loss = 0.37908098\n",
      "Iteration 37, loss = 0.37925771\n",
      "Iteration 38, loss = 0.37919610\n",
      "Iteration 39, loss = 0.37879078\n",
      "Iteration 40, loss = 0.37914560\n",
      "Iteration 41, loss = 0.37889468\n",
      "Iteration 42, loss = 0.37901907\n",
      "Iteration 43, loss = 0.37868486\n",
      "Iteration 44, loss = 0.37887142\n",
      "Iteration 45, loss = 0.37882051\n",
      "Iteration 46, loss = 0.37865559\n",
      "Iteration 47, loss = 0.37898141\n",
      "Iteration 48, loss = 0.37867335\n",
      "Iteration 49, loss = 0.37871757\n",
      "Iteration 50, loss = 0.37900025\n",
      "Iteration 51, loss = 0.37866224\n",
      "Iteration 52, loss = 0.37945180\n",
      "Iteration 53, loss = 0.37930319\n",
      "Iteration 54, loss = 0.37871604\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.68033486\n",
      "Iteration 2, loss = 0.63387365\n",
      "Iteration 3, loss = 0.54950380\n",
      "Iteration 4, loss = 0.47590192\n",
      "Iteration 5, loss = 0.43326891\n",
      "Iteration 6, loss = 0.41166984\n",
      "Iteration 7, loss = 0.40082365\n",
      "Iteration 8, loss = 0.39524435\n",
      "Iteration 9, loss = 0.39279125\n",
      "Iteration 10, loss = 0.39083435\n",
      "Iteration 11, loss = 0.38978805\n",
      "Iteration 12, loss = 0.38891635\n",
      "Iteration 13, loss = 0.38895412\n",
      "Iteration 14, loss = 0.38878608\n",
      "Iteration 15, loss = 0.38860984\n",
      "Iteration 16, loss = 0.38792482\n",
      "Iteration 17, loss = 0.38778220\n",
      "Iteration 18, loss = 0.38807708\n",
      "Iteration 19, loss = 0.38761750\n",
      "Iteration 20, loss = 0.38773946\n",
      "Iteration 21, loss = 0.38769878\n",
      "Iteration 22, loss = 0.38702279\n",
      "Iteration 23, loss = 0.38722920\n",
      "Iteration 24, loss = 0.38757421\n",
      "Iteration 25, loss = 0.38704401\n",
      "Iteration 26, loss = 0.38782164\n",
      "Iteration 27, loss = 0.38687286\n",
      "Iteration 28, loss = 0.38658655\n",
      "Iteration 29, loss = 0.38685623\n",
      "Iteration 30, loss = 0.38677682\n",
      "Iteration 31, loss = 0.38698226\n",
      "Iteration 32, loss = 0.38686326\n",
      "Iteration 33, loss = 0.38695027\n",
      "Iteration 34, loss = 0.38655483\n",
      "Iteration 35, loss = 0.38719549\n",
      "Iteration 36, loss = 0.38679212\n",
      "Iteration 37, loss = 0.38692574\n",
      "Iteration 38, loss = 0.38680252\n",
      "Iteration 39, loss = 0.38678019\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.67908540\n",
      "Iteration 2, loss = 0.62912395\n",
      "Iteration 3, loss = 0.54579481\n",
      "Iteration 4, loss = 0.47665570\n",
      "Iteration 5, loss = 0.43766041\n",
      "Iteration 6, loss = 0.41813662\n",
      "Iteration 7, loss = 0.40860991\n",
      "Iteration 8, loss = 0.40420585\n",
      "Iteration 9, loss = 0.40079671\n",
      "Iteration 10, loss = 0.39934140\n",
      "Iteration 11, loss = 0.39923400\n",
      "Iteration 12, loss = 0.39776752\n",
      "Iteration 13, loss = 0.39741831\n",
      "Iteration 14, loss = 0.39694773\n",
      "Iteration 15, loss = 0.39670301\n",
      "Iteration 16, loss = 0.39650766\n",
      "Iteration 17, loss = 0.39645722\n",
      "Iteration 18, loss = 0.39637052\n",
      "Iteration 19, loss = 0.39589409\n",
      "Iteration 20, loss = 0.39620103\n",
      "Iteration 21, loss = 0.39564871\n",
      "Iteration 22, loss = 0.39609347\n",
      "Iteration 23, loss = 0.39552783\n",
      "Iteration 24, loss = 0.39589629\n",
      "Iteration 25, loss = 0.39592520\n",
      "Iteration 26, loss = 0.39534362\n",
      "Iteration 27, loss = 0.39536578\n",
      "Iteration 28, loss = 0.39476565\n",
      "Iteration 29, loss = 0.39487736\n",
      "Iteration 30, loss = 0.39514829\n",
      "Iteration 31, loss = 0.39490214\n",
      "Iteration 32, loss = 0.39493573\n",
      "Iteration 33, loss = 0.39474439\n",
      "Iteration 34, loss = 0.39468573\n",
      "Iteration 35, loss = 0.39436259\n",
      "Iteration 36, loss = 0.39525946\n",
      "Iteration 37, loss = 0.39462254\n",
      "Iteration 38, loss = 0.39456881\n",
      "Iteration 39, loss = 0.39457046\n",
      "Iteration 40, loss = 0.39452641\n",
      "Iteration 41, loss = 0.39426729\n",
      "Iteration 42, loss = 0.39476832\n",
      "Iteration 43, loss = 0.39451190\n",
      "Iteration 44, loss = 0.39409198\n",
      "Iteration 45, loss = 0.39489582\n",
      "Iteration 46, loss = 0.39468414\n",
      "Iteration 47, loss = 0.39416527\n",
      "Iteration 48, loss = 0.39444068\n",
      "Iteration 49, loss = 0.39452884\n",
      "Iteration 50, loss = 0.39451869\n",
      "Iteration 51, loss = 0.39544322\n",
      "Iteration 52, loss = 0.39410116\n",
      "Iteration 53, loss = 0.39412124\n",
      "Iteration 54, loss = 0.39443362\n",
      "Iteration 55, loss = 0.39401993\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.81393224\n",
      "Iteration 2, loss = 0.68291616\n",
      "Iteration 3, loss = 0.66065678\n",
      "Iteration 4, loss = 0.63163222\n",
      "Iteration 5, loss = 0.59652308\n",
      "Iteration 6, loss = 0.55782735\n",
      "Iteration 7, loss = 0.51959095\n",
      "Iteration 8, loss = 0.48542473\n",
      "Iteration 9, loss = 0.45862110\n",
      "Iteration 10, loss = 0.43789738\n",
      "Iteration 11, loss = 0.42260035\n",
      "Iteration 12, loss = 0.41170920\n",
      "Iteration 13, loss = 0.40389746\n",
      "Iteration 14, loss = 0.39804304\n",
      "Iteration 15, loss = 0.39444430\n",
      "Iteration 16, loss = 0.39114615\n",
      "Iteration 17, loss = 0.38941596\n",
      "Iteration 18, loss = 0.38742726\n",
      "Iteration 19, loss = 0.38618193\n",
      "Iteration 20, loss = 0.38555146\n",
      "Iteration 21, loss = 0.38448406\n",
      "Iteration 22, loss = 0.38387496\n",
      "Iteration 23, loss = 0.38367312\n",
      "Iteration 24, loss = 0.38324198\n",
      "Iteration 25, loss = 0.38250135\n",
      "Iteration 26, loss = 0.38236469\n",
      "Iteration 27, loss = 0.38238166\n",
      "Iteration 28, loss = 0.38188777\n",
      "Iteration 29, loss = 0.38169749\n",
      "Iteration 30, loss = 0.38135669\n",
      "Iteration 31, loss = 0.38144738\n",
      "Iteration 32, loss = 0.38093322\n",
      "Iteration 33, loss = 0.38106571\n",
      "Iteration 34, loss = 0.38087641\n",
      "Iteration 35, loss = 0.38072899\n",
      "Iteration 36, loss = 0.38062313\n",
      "Iteration 37, loss = 0.38037984\n",
      "Iteration 38, loss = 0.38101477\n",
      "Iteration 39, loss = 0.38022240\n",
      "Iteration 40, loss = 0.38054442\n",
      "Iteration 41, loss = 0.37978789\n",
      "Iteration 42, loss = 0.37988494\n",
      "Iteration 43, loss = 0.37981821\n",
      "Iteration 44, loss = 0.37984602\n",
      "Iteration 45, loss = 0.37977889\n",
      "Iteration 46, loss = 0.37922309\n",
      "Iteration 47, loss = 0.38017797\n",
      "Iteration 48, loss = 0.37927093\n",
      "Iteration 49, loss = 0.37952452\n",
      "Iteration 50, loss = 0.37916060\n",
      "Iteration 51, loss = 0.37906021\n",
      "Iteration 52, loss = 0.37940113\n",
      "Iteration 53, loss = 0.37901223\n",
      "Iteration 54, loss = 0.37902823\n",
      "Iteration 55, loss = 0.37905996\n",
      "Iteration 56, loss = 0.37914240\n",
      "Iteration 57, loss = 0.37900703\n",
      "Iteration 58, loss = 0.37892781\n",
      "Iteration 59, loss = 0.37837416\n",
      "Iteration 60, loss = 0.37882098\n",
      "Iteration 61, loss = 0.37855172\n",
      "Iteration 62, loss = 0.37848620\n",
      "Iteration 63, loss = 0.37867579\n",
      "Iteration 64, loss = 0.37856904\n",
      "Iteration 65, loss = 0.37866913\n",
      "Iteration 66, loss = 0.37857339\n",
      "Iteration 67, loss = 0.37872711\n",
      "Iteration 68, loss = 0.37849162\n",
      "Iteration 69, loss = 0.37829345\n",
      "Iteration 70, loss = 0.37822949\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.81649602\n",
      "Iteration 2, loss = 0.68264469\n",
      "Iteration 3, loss = 0.66027556\n",
      "Iteration 4, loss = 0.63161007\n",
      "Iteration 5, loss = 0.59711275\n",
      "Iteration 6, loss = 0.55950099\n",
      "Iteration 7, loss = 0.52241174\n",
      "Iteration 8, loss = 0.48992847\n",
      "Iteration 9, loss = 0.46385944\n",
      "Iteration 10, loss = 0.44407866\n",
      "Iteration 11, loss = 0.42937659\n",
      "Iteration 12, loss = 0.41885073\n",
      "Iteration 13, loss = 0.41157617\n",
      "Iteration 14, loss = 0.40601988\n",
      "Iteration 15, loss = 0.40208761\n",
      "Iteration 16, loss = 0.39918586\n",
      "Iteration 17, loss = 0.39704635\n",
      "Iteration 18, loss = 0.39543978\n",
      "Iteration 19, loss = 0.39421725\n",
      "Iteration 20, loss = 0.39338506\n",
      "Iteration 21, loss = 0.39256806\n",
      "Iteration 22, loss = 0.39193179\n",
      "Iteration 23, loss = 0.39169728\n",
      "Iteration 24, loss = 0.39129080\n",
      "Iteration 25, loss = 0.39082295\n",
      "Iteration 26, loss = 0.39038677\n",
      "Iteration 27, loss = 0.39040497\n",
      "Iteration 28, loss = 0.38992258\n",
      "Iteration 29, loss = 0.38952902\n",
      "Iteration 30, loss = 0.38942674\n",
      "Iteration 31, loss = 0.38922540\n",
      "Iteration 32, loss = 0.38935962\n",
      "Iteration 33, loss = 0.38924875\n",
      "Iteration 34, loss = 0.38912216\n",
      "Iteration 35, loss = 0.38877816\n",
      "Iteration 36, loss = 0.38890279\n",
      "Iteration 37, loss = 0.38835675\n",
      "Iteration 38, loss = 0.38827923\n",
      "Iteration 39, loss = 0.38817788\n",
      "Iteration 40, loss = 0.38817379\n",
      "Iteration 41, loss = 0.38806835\n",
      "Iteration 42, loss = 0.38793303\n",
      "Iteration 43, loss = 0.38736577\n",
      "Iteration 44, loss = 0.38770994\n",
      "Iteration 45, loss = 0.38751001\n",
      "Iteration 46, loss = 0.38736192\n",
      "Iteration 47, loss = 0.38729310\n",
      "Iteration 48, loss = 0.38698958\n",
      "Iteration 49, loss = 0.38711714\n",
      "Iteration 50, loss = 0.38696388\n",
      "Iteration 51, loss = 0.38691304\n",
      "Iteration 52, loss = 0.38705076\n",
      "Iteration 53, loss = 0.38659156\n",
      "Iteration 54, loss = 0.38670731\n",
      "Iteration 55, loss = 0.38694954\n",
      "Iteration 56, loss = 0.38662691\n",
      "Iteration 57, loss = 0.38681941\n",
      "Iteration 58, loss = 0.38728734\n",
      "Iteration 59, loss = 0.38660483\n",
      "Iteration 60, loss = 0.38630216\n",
      "Iteration 61, loss = 0.38624379\n",
      "Iteration 62, loss = 0.38676438\n",
      "Iteration 63, loss = 0.38633782\n",
      "Iteration 64, loss = 0.38643028\n",
      "Iteration 65, loss = 0.38625139\n",
      "Iteration 66, loss = 0.38632528\n",
      "Iteration 67, loss = 0.38631218\n",
      "Iteration 68, loss = 0.38593879\n",
      "Iteration 69, loss = 0.38606361\n",
      "Iteration 70, loss = 0.38601607\n",
      "Iteration 71, loss = 0.38605211\n",
      "Iteration 72, loss = 0.38621419\n",
      "Iteration 73, loss = 0.38633775\n",
      "Iteration 74, loss = 0.38590478\n",
      "Iteration 75, loss = 0.38582754\n",
      "Iteration 76, loss = 0.38603619\n",
      "Iteration 77, loss = 0.38591884\n",
      "Iteration 78, loss = 0.38588573\n",
      "Iteration 79, loss = 0.38600060\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.81582413\n",
      "Iteration 2, loss = 0.68267140\n",
      "Iteration 3, loss = 0.66095953\n",
      "Iteration 4, loss = 0.63304303\n",
      "Iteration 5, loss = 0.59951115\n",
      "Iteration 6, loss = 0.56311014\n",
      "Iteration 7, loss = 0.52739456\n",
      "Iteration 8, loss = 0.49604161\n",
      "Iteration 9, loss = 0.47073095\n",
      "Iteration 10, loss = 0.45158513\n",
      "Iteration 11, loss = 0.43759342\n",
      "Iteration 12, loss = 0.42732912\n",
      "Iteration 13, loss = 0.42010757\n",
      "Iteration 14, loss = 0.41481109\n",
      "Iteration 15, loss = 0.41131395\n",
      "Iteration 16, loss = 0.40867990\n",
      "Iteration 17, loss = 0.40643013\n",
      "Iteration 18, loss = 0.40477530\n",
      "Iteration 19, loss = 0.40346064\n",
      "Iteration 20, loss = 0.40283244\n",
      "Iteration 21, loss = 0.40196069\n",
      "Iteration 22, loss = 0.40124950\n",
      "Iteration 23, loss = 0.40077821\n",
      "Iteration 24, loss = 0.40033917\n",
      "Iteration 25, loss = 0.40005706\n",
      "Iteration 26, loss = 0.39955362\n",
      "Iteration 27, loss = 0.39950482\n",
      "Iteration 28, loss = 0.39931297\n",
      "Iteration 29, loss = 0.39883359\n",
      "Iteration 30, loss = 0.39845710\n",
      "Iteration 31, loss = 0.39859817\n",
      "Iteration 32, loss = 0.39850638\n",
      "Iteration 33, loss = 0.39773696\n",
      "Iteration 34, loss = 0.39762968\n",
      "Iteration 35, loss = 0.39740240\n",
      "Iteration 36, loss = 0.39733231\n",
      "Iteration 37, loss = 0.39716912\n",
      "Iteration 38, loss = 0.39728635\n",
      "Iteration 39, loss = 0.39685035\n",
      "Iteration 40, loss = 0.39677782\n",
      "Iteration 41, loss = 0.39649982\n",
      "Iteration 42, loss = 0.39636209\n",
      "Iteration 43, loss = 0.39616274\n",
      "Iteration 44, loss = 0.39612278\n",
      "Iteration 45, loss = 0.39585932\n",
      "Iteration 46, loss = 0.39575288\n",
      "Iteration 47, loss = 0.39572506\n",
      "Iteration 48, loss = 0.39551609\n",
      "Iteration 49, loss = 0.39577244\n",
      "Iteration 50, loss = 0.39593242\n",
      "Iteration 51, loss = 0.39551798\n",
      "Iteration 52, loss = 0.39527664\n",
      "Iteration 53, loss = 0.39502816\n",
      "Iteration 54, loss = 0.39514946\n",
      "Iteration 55, loss = 0.39534862\n",
      "Iteration 56, loss = 0.39495873\n",
      "Iteration 57, loss = 0.39507041\n",
      "Iteration 58, loss = 0.39505044\n",
      "Iteration 59, loss = 0.39486393\n",
      "Iteration 60, loss = 0.39462154\n",
      "Iteration 61, loss = 0.39482801\n",
      "Iteration 62, loss = 0.39482413\n",
      "Iteration 63, loss = 0.39472323\n",
      "Iteration 64, loss = 0.39475067\n",
      "Iteration 65, loss = 0.39440696\n",
      "Iteration 66, loss = 0.39461028\n",
      "Iteration 67, loss = 0.39463540\n",
      "Iteration 68, loss = 0.39425744\n",
      "Iteration 69, loss = 0.39447634\n",
      "Iteration 70, loss = 0.39428495\n",
      "Iteration 71, loss = 0.39406607\n",
      "Iteration 72, loss = 0.39421536\n",
      "Iteration 73, loss = 0.39410261\n",
      "Iteration 74, loss = 0.39433852\n",
      "Iteration 75, loss = 0.39430590\n",
      "Iteration 76, loss = 0.39398814\n",
      "Iteration 77, loss = 0.39420573\n",
      "Iteration 78, loss = 0.39441893\n",
      "Iteration 79, loss = 0.39407867\n",
      "Iteration 80, loss = 0.39380459\n",
      "Iteration 81, loss = 0.39465005\n",
      "Iteration 82, loss = 0.39378818\n",
      "Iteration 83, loss = 0.39375444\n",
      "Iteration 84, loss = 0.39423803\n",
      "Iteration 85, loss = 0.39385448\n",
      "Iteration 86, loss = 0.39389482\n",
      "Iteration 87, loss = 0.39363653\n",
      "Iteration 88, loss = 0.39371411\n",
      "Iteration 89, loss = 0.39386307\n",
      "Iteration 90, loss = 0.39375266\n",
      "Iteration 91, loss = 0.39383726\n",
      "Iteration 92, loss = 0.39382651\n",
      "Iteration 93, loss = 0.39370821\n",
      "Iteration 94, loss = 0.39366128\n",
      "Iteration 95, loss = 0.39370421\n",
      "Iteration 96, loss = 0.39383419\n",
      "Iteration 97, loss = 0.39358374\n",
      "Iteration 98, loss = 0.39350392\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.86318516\n",
      "Iteration 2, loss = 0.71215523\n",
      "Iteration 3, loss = 0.68317699\n",
      "Iteration 4, loss = 0.64535739\n",
      "Iteration 5, loss = 0.59177210\n",
      "Iteration 6, loss = 0.52648547\n",
      "Iteration 7, loss = 0.46874157\n",
      "Iteration 8, loss = 0.43013373\n",
      "Iteration 9, loss = 0.40827151\n",
      "Iteration 10, loss = 0.39635996\n",
      "Iteration 11, loss = 0.38961005\n",
      "Iteration 12, loss = 0.38590479\n",
      "Iteration 13, loss = 0.38358266\n",
      "Iteration 14, loss = 0.38249539\n",
      "Iteration 15, loss = 0.38204323\n",
      "Iteration 16, loss = 0.38052237\n",
      "Iteration 17, loss = 0.38110754\n",
      "Iteration 18, loss = 0.38058240\n",
      "Iteration 19, loss = 0.38002394\n",
      "Iteration 20, loss = 0.38047998\n",
      "Iteration 21, loss = 0.37956241\n",
      "Iteration 22, loss = 0.37945245\n",
      "Iteration 23, loss = 0.37983682\n",
      "Iteration 24, loss = 0.37990452\n",
      "Iteration 25, loss = 0.37887819\n",
      "Iteration 26, loss = 0.37878457\n",
      "Iteration 27, loss = 0.37927831\n",
      "Iteration 28, loss = 0.37885279\n",
      "Iteration 29, loss = 0.37883807\n",
      "Iteration 30, loss = 0.37869159\n",
      "Iteration 31, loss = 0.37917966\n",
      "Iteration 32, loss = 0.37805934\n",
      "Iteration 33, loss = 0.37912736\n",
      "Iteration 34, loss = 0.37835857\n",
      "Iteration 35, loss = 0.37864821\n",
      "Iteration 36, loss = 0.37844458\n",
      "Iteration 37, loss = 0.37826849\n",
      "Iteration 38, loss = 0.37954443\n",
      "Iteration 39, loss = 0.37845126\n",
      "Iteration 40, loss = 0.37937272\n",
      "Iteration 41, loss = 0.37817413\n",
      "Iteration 42, loss = 0.37828063\n",
      "Iteration 43, loss = 0.37816836\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.86748964\n",
      "Iteration 2, loss = 0.71210667\n",
      "Iteration 3, loss = 0.68328258\n",
      "Iteration 4, loss = 0.64687819\n",
      "Iteration 5, loss = 0.59598713\n",
      "Iteration 6, loss = 0.53391483\n",
      "Iteration 7, loss = 0.47760402\n",
      "Iteration 8, loss = 0.43956135\n",
      "Iteration 9, loss = 0.41757636\n",
      "Iteration 10, loss = 0.40507428\n",
      "Iteration 11, loss = 0.39798254\n",
      "Iteration 12, loss = 0.39409958\n",
      "Iteration 13, loss = 0.39232756\n",
      "Iteration 14, loss = 0.39097085\n",
      "Iteration 15, loss = 0.38991275\n",
      "Iteration 16, loss = 0.38893332\n",
      "Iteration 17, loss = 0.38858498\n",
      "Iteration 18, loss = 0.38851671\n",
      "Iteration 19, loss = 0.38805475\n",
      "Iteration 20, loss = 0.38789375\n",
      "Iteration 21, loss = 0.38742442\n",
      "Iteration 22, loss = 0.38743154\n",
      "Iteration 23, loss = 0.38750714\n",
      "Iteration 24, loss = 0.38771909\n",
      "Iteration 25, loss = 0.38706059\n",
      "Iteration 26, loss = 0.38653081\n",
      "Iteration 27, loss = 0.38706161\n",
      "Iteration 28, loss = 0.38645574\n",
      "Iteration 29, loss = 0.38633218\n",
      "Iteration 30, loss = 0.38645688\n",
      "Iteration 31, loss = 0.38627696\n",
      "Iteration 32, loss = 0.38703062\n",
      "Iteration 33, loss = 0.38697016\n",
      "Iteration 34, loss = 0.38691884\n",
      "Iteration 35, loss = 0.38600574\n",
      "Iteration 36, loss = 0.38740932\n",
      "Iteration 37, loss = 0.38606929\n",
      "Iteration 38, loss = 0.38600106\n",
      "Iteration 39, loss = 0.38613820\n",
      "Iteration 40, loss = 0.38715210\n",
      "Iteration 41, loss = 0.38649750\n",
      "Iteration 42, loss = 0.38681213\n",
      "Iteration 43, loss = 0.38533240\n",
      "Iteration 44, loss = 0.38597173\n",
      "Iteration 45, loss = 0.38578092\n",
      "Iteration 46, loss = 0.38591739\n",
      "Iteration 47, loss = 0.38591760\n",
      "Iteration 48, loss = 0.38534146\n",
      "Iteration 49, loss = 0.38554821\n",
      "Iteration 50, loss = 0.38561154\n",
      "Iteration 51, loss = 0.38572338\n",
      "Iteration 52, loss = 0.38588780\n",
      "Iteration 53, loss = 0.38531968\n",
      "Iteration 54, loss = 0.38553581\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.86412788\n",
      "Iteration 2, loss = 0.71091428\n",
      "Iteration 3, loss = 0.68315936\n",
      "Iteration 4, loss = 0.64712755\n",
      "Iteration 5, loss = 0.59637233\n",
      "Iteration 6, loss = 0.53552345\n",
      "Iteration 7, loss = 0.48135540\n",
      "Iteration 8, loss = 0.44499767\n",
      "Iteration 9, loss = 0.42394893\n",
      "Iteration 10, loss = 0.41258568\n",
      "Iteration 11, loss = 0.40623315\n",
      "Iteration 12, loss = 0.40256929\n",
      "Iteration 13, loss = 0.40058072\n",
      "Iteration 14, loss = 0.39903433\n",
      "Iteration 15, loss = 0.39854467\n",
      "Iteration 16, loss = 0.39829752\n",
      "Iteration 17, loss = 0.39769320\n",
      "Iteration 18, loss = 0.39697558\n",
      "Iteration 19, loss = 0.39637511\n",
      "Iteration 20, loss = 0.39644713\n",
      "Iteration 21, loss = 0.39621266\n",
      "Iteration 22, loss = 0.39565680\n",
      "Iteration 23, loss = 0.39544657\n",
      "Iteration 24, loss = 0.39542122\n",
      "Iteration 25, loss = 0.39539991\n",
      "Iteration 26, loss = 0.39486144\n",
      "Iteration 27, loss = 0.39517904\n",
      "Iteration 28, loss = 0.39524915\n",
      "Iteration 29, loss = 0.39487025\n",
      "Iteration 30, loss = 0.39469810\n",
      "Iteration 31, loss = 0.39501436\n",
      "Iteration 32, loss = 0.39581934\n",
      "Iteration 33, loss = 0.39399638\n",
      "Iteration 34, loss = 0.39421345\n",
      "Iteration 35, loss = 0.39401838\n",
      "Iteration 36, loss = 0.39397770\n",
      "Iteration 37, loss = 0.39406671\n",
      "Iteration 38, loss = 0.39536067\n",
      "Iteration 39, loss = 0.39401521\n",
      "Iteration 40, loss = 0.39431348\n",
      "Iteration 41, loss = 0.39386017\n",
      "Iteration 42, loss = 0.39374926\n",
      "Iteration 43, loss = 0.39357901\n",
      "Iteration 44, loss = 0.39377906\n",
      "Iteration 45, loss = 0.39356874\n",
      "Iteration 46, loss = 0.39347101\n",
      "Iteration 47, loss = 0.39367576\n",
      "Iteration 48, loss = 0.39346031\n",
      "Iteration 49, loss = 0.39439556\n",
      "Iteration 50, loss = 0.39484861\n",
      "Iteration 51, loss = 0.39411934\n",
      "Iteration 52, loss = 0.39370588\n",
      "Iteration 53, loss = 0.39343200\n",
      "Iteration 54, loss = 0.39344754\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.71059927\n",
      "Iteration 2, loss = 0.69312922\n",
      "Iteration 3, loss = 0.69065335\n",
      "Iteration 4, loss = 0.68855951\n",
      "Iteration 5, loss = 0.68591995\n",
      "Iteration 6, loss = 0.68233369\n",
      "Iteration 7, loss = 0.67720069\n",
      "Iteration 8, loss = 0.66914719\n",
      "Iteration 9, loss = 0.65729459\n",
      "Iteration 10, loss = 0.63959615\n",
      "Iteration 11, loss = 0.61439404\n",
      "Iteration 12, loss = 0.58142893\n",
      "Iteration 13, loss = 0.54353887\n",
      "Iteration 14, loss = 0.50567663\n",
      "Iteration 15, loss = 0.47245497\n",
      "Iteration 16, loss = 0.44668385\n",
      "Iteration 17, loss = 0.42752590\n",
      "Iteration 18, loss = 0.41417387\n",
      "Iteration 19, loss = 0.40429632\n",
      "Iteration 20, loss = 0.39834051\n",
      "Iteration 21, loss = 0.39322229\n",
      "Iteration 22, loss = 0.38964398\n",
      "Iteration 23, loss = 0.38760981\n",
      "Iteration 24, loss = 0.38587809\n",
      "Iteration 25, loss = 0.38465555\n",
      "Iteration 26, loss = 0.38336966\n",
      "Iteration 27, loss = 0.38284781\n",
      "Iteration 28, loss = 0.38206853\n",
      "Iteration 29, loss = 0.38193262\n",
      "Iteration 30, loss = 0.38168021\n",
      "Iteration 31, loss = 0.38101994\n",
      "Iteration 32, loss = 0.38091637\n",
      "Iteration 33, loss = 0.38055920\n",
      "Iteration 34, loss = 0.38040478\n",
      "Iteration 35, loss = 0.38044035\n",
      "Iteration 36, loss = 0.38004675\n",
      "Iteration 37, loss = 0.38007214\n",
      "Iteration 38, loss = 0.37968736\n",
      "Iteration 39, loss = 0.37947521\n",
      "Iteration 40, loss = 0.37937554\n",
      "Iteration 41, loss = 0.37965538\n",
      "Iteration 42, loss = 0.37914631\n",
      "Iteration 43, loss = 0.37904329\n",
      "Iteration 44, loss = 0.37869189\n",
      "Iteration 45, loss = 0.37885215\n",
      "Iteration 46, loss = 0.37876264\n",
      "Iteration 47, loss = 0.37893154\n",
      "Iteration 48, loss = 0.37865803\n",
      "Iteration 49, loss = 0.37880214\n",
      "Iteration 50, loss = 0.37856112\n",
      "Iteration 51, loss = 0.37834515\n",
      "Iteration 52, loss = 0.37800620\n",
      "Iteration 53, loss = 0.37832742\n",
      "Iteration 54, loss = 0.37815315\n",
      "Iteration 55, loss = 0.37799507\n",
      "Iteration 56, loss = 0.37799096\n",
      "Iteration 57, loss = 0.37783194\n",
      "Iteration 58, loss = 0.37778377\n",
      "Iteration 59, loss = 0.37812439\n",
      "Iteration 60, loss = 0.37783901\n",
      "Iteration 61, loss = 0.37769778\n",
      "Iteration 62, loss = 0.37763487\n",
      "Iteration 63, loss = 0.37741501\n",
      "Iteration 64, loss = 0.37756021\n",
      "Iteration 65, loss = 0.37763373\n",
      "Iteration 66, loss = 0.37755565\n",
      "Iteration 67, loss = 0.37727596\n",
      "Iteration 68, loss = 0.37766064\n",
      "Iteration 69, loss = 0.37750760\n",
      "Iteration 70, loss = 0.37742191\n",
      "Iteration 71, loss = 0.37779267\n",
      "Iteration 72, loss = 0.37704750\n",
      "Iteration 73, loss = 0.37727863\n",
      "Iteration 74, loss = 0.37734444\n",
      "Iteration 75, loss = 0.37730427\n",
      "Iteration 76, loss = 0.37704872\n",
      "Iteration 77, loss = 0.37705676\n",
      "Iteration 78, loss = 0.37716962\n",
      "Iteration 79, loss = 0.37709253\n",
      "Iteration 80, loss = 0.37731025\n",
      "Iteration 81, loss = 0.37713370\n",
      "Iteration 82, loss = 0.37705699\n",
      "Iteration 83, loss = 0.37702296\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.71200736\n",
      "Iteration 2, loss = 0.69286609\n",
      "Iteration 3, loss = 0.69048138\n",
      "Iteration 4, loss = 0.68831843\n",
      "Iteration 5, loss = 0.68577437\n",
      "Iteration 6, loss = 0.68216567\n",
      "Iteration 7, loss = 0.67692443\n",
      "Iteration 8, loss = 0.66882531\n",
      "Iteration 9, loss = 0.65698027\n",
      "Iteration 10, loss = 0.63924108\n",
      "Iteration 11, loss = 0.61428842\n",
      "Iteration 12, loss = 0.58168435\n",
      "Iteration 13, loss = 0.54472046\n",
      "Iteration 14, loss = 0.50859229\n",
      "Iteration 15, loss = 0.47679199\n",
      "Iteration 16, loss = 0.45197500\n",
      "Iteration 17, loss = 0.43376095\n",
      "Iteration 18, loss = 0.42099276\n",
      "Iteration 19, loss = 0.41210066\n",
      "Iteration 20, loss = 0.40578736\n",
      "Iteration 21, loss = 0.40139111\n",
      "Iteration 22, loss = 0.39797150\n",
      "Iteration 23, loss = 0.39571652\n",
      "Iteration 24, loss = 0.39429424\n",
      "Iteration 25, loss = 0.39293631\n",
      "Iteration 26, loss = 0.39231274\n",
      "Iteration 27, loss = 0.39085878\n",
      "Iteration 28, loss = 0.39052684\n",
      "Iteration 29, loss = 0.39040174\n",
      "Iteration 30, loss = 0.39029322\n",
      "Iteration 31, loss = 0.38953056\n",
      "Iteration 32, loss = 0.38918903\n",
      "Iteration 33, loss = 0.38915062\n",
      "Iteration 34, loss = 0.38868770\n",
      "Iteration 35, loss = 0.38833145\n",
      "Iteration 36, loss = 0.38835941\n",
      "Iteration 37, loss = 0.38825860\n",
      "Iteration 38, loss = 0.38805755\n",
      "Iteration 39, loss = 0.38824875\n",
      "Iteration 40, loss = 0.38761017\n",
      "Iteration 41, loss = 0.38804912\n",
      "Iteration 42, loss = 0.38750554\n",
      "Iteration 43, loss = 0.38724212\n",
      "Iteration 44, loss = 0.38713266\n",
      "Iteration 45, loss = 0.38709400\n",
      "Iteration 46, loss = 0.38682293\n",
      "Iteration 47, loss = 0.38707927\n",
      "Iteration 48, loss = 0.38679388\n",
      "Iteration 49, loss = 0.38664057\n",
      "Iteration 50, loss = 0.38656350\n",
      "Iteration 51, loss = 0.38629273\n",
      "Iteration 52, loss = 0.38653107\n",
      "Iteration 53, loss = 0.38617088\n",
      "Iteration 54, loss = 0.38639918\n",
      "Iteration 55, loss = 0.38595072\n",
      "Iteration 56, loss = 0.38598027\n",
      "Iteration 57, loss = 0.38611021\n",
      "Iteration 58, loss = 0.38583159\n",
      "Iteration 59, loss = 0.38584354\n",
      "Iteration 60, loss = 0.38613606\n",
      "Iteration 61, loss = 0.38566098\n",
      "Iteration 62, loss = 0.38569066\n",
      "Iteration 63, loss = 0.38544247\n",
      "Iteration 64, loss = 0.38563526\n",
      "Iteration 65, loss = 0.38560278\n",
      "Iteration 66, loss = 0.38586551\n",
      "Iteration 67, loss = 0.38547217\n",
      "Iteration 68, loss = 0.38540453\n",
      "Iteration 69, loss = 0.38553644\n",
      "Iteration 70, loss = 0.38537603\n",
      "Iteration 71, loss = 0.38559429\n",
      "Iteration 72, loss = 0.38505839\n",
      "Iteration 73, loss = 0.38518483\n",
      "Iteration 74, loss = 0.38526331\n",
      "Iteration 75, loss = 0.38497383\n",
      "Iteration 76, loss = 0.38584805\n",
      "Iteration 77, loss = 0.38579479\n",
      "Iteration 78, loss = 0.38540738\n",
      "Iteration 79, loss = 0.38513448\n",
      "Iteration 80, loss = 0.38536752\n",
      "Iteration 81, loss = 0.38498075\n",
      "Iteration 82, loss = 0.38497893\n",
      "Iteration 83, loss = 0.38502927\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.71305555\n",
      "Iteration 2, loss = 0.69282810\n",
      "Iteration 3, loss = 0.69052575\n",
      "Iteration 4, loss = 0.68840609\n",
      "Iteration 5, loss = 0.68586973\n",
      "Iteration 6, loss = 0.68233648\n",
      "Iteration 7, loss = 0.67719193\n",
      "Iteration 8, loss = 0.66949309\n",
      "Iteration 9, loss = 0.65797618\n",
      "Iteration 10, loss = 0.64102059\n",
      "Iteration 11, loss = 0.61678688\n",
      "Iteration 12, loss = 0.58582724\n",
      "Iteration 13, loss = 0.55048828\n",
      "Iteration 14, loss = 0.51513775\n",
      "Iteration 15, loss = 0.48437130\n",
      "Iteration 16, loss = 0.46003529\n",
      "Iteration 17, loss = 0.44237796\n",
      "Iteration 18, loss = 0.42953529\n",
      "Iteration 19, loss = 0.42113099\n",
      "Iteration 20, loss = 0.41458807\n",
      "Iteration 21, loss = 0.41023371\n",
      "Iteration 22, loss = 0.40719833\n",
      "Iteration 23, loss = 0.40518465\n",
      "Iteration 24, loss = 0.40348554\n",
      "Iteration 25, loss = 0.40243715\n",
      "Iteration 26, loss = 0.40106143\n",
      "Iteration 27, loss = 0.40071957\n",
      "Iteration 28, loss = 0.40002453\n",
      "Iteration 29, loss = 0.39942549\n",
      "Iteration 30, loss = 0.39895564\n",
      "Iteration 31, loss = 0.39866561\n",
      "Iteration 32, loss = 0.39848507\n",
      "Iteration 33, loss = 0.39798868\n",
      "Iteration 34, loss = 0.39778404\n",
      "Iteration 35, loss = 0.39792257\n",
      "Iteration 36, loss = 0.39684025\n",
      "Iteration 37, loss = 0.39784533\n",
      "Iteration 38, loss = 0.39668975\n",
      "Iteration 39, loss = 0.39667020\n",
      "Iteration 40, loss = 0.39652793\n",
      "Iteration 41, loss = 0.39643909\n",
      "Iteration 42, loss = 0.39608436\n",
      "Iteration 43, loss = 0.39612645\n",
      "Iteration 44, loss = 0.39604393\n",
      "Iteration 45, loss = 0.39612932\n",
      "Iteration 46, loss = 0.39534712\n",
      "Iteration 47, loss = 0.39587842\n",
      "Iteration 48, loss = 0.39522619\n",
      "Iteration 49, loss = 0.39511846\n",
      "Iteration 50, loss = 0.39486302\n",
      "Iteration 51, loss = 0.39504011\n",
      "Iteration 52, loss = 0.39512432\n",
      "Iteration 53, loss = 0.39473463\n",
      "Iteration 54, loss = 0.39504831\n",
      "Iteration 55, loss = 0.39488961\n",
      "Iteration 56, loss = 0.39434049\n",
      "Iteration 57, loss = 0.39432018\n",
      "Iteration 58, loss = 0.39422635\n",
      "Iteration 59, loss = 0.39425291\n",
      "Iteration 60, loss = 0.39424822\n",
      "Iteration 61, loss = 0.39409739\n",
      "Iteration 62, loss = 0.39384881\n",
      "Iteration 63, loss = 0.39402588\n",
      "Iteration 64, loss = 0.39375292\n",
      "Iteration 65, loss = 0.39362212\n",
      "Iteration 66, loss = 0.39387868\n",
      "Iteration 67, loss = 0.39394351\n",
      "Iteration 68, loss = 0.39379590\n",
      "Iteration 69, loss = 0.39370185\n",
      "Iteration 70, loss = 0.39350129\n",
      "Iteration 71, loss = 0.39376456\n",
      "Iteration 72, loss = 0.39366508\n",
      "Iteration 73, loss = 0.39375308\n",
      "Iteration 74, loss = 0.39374491\n",
      "Iteration 75, loss = 0.39328332\n",
      "Iteration 76, loss = 0.39331239\n",
      "Iteration 77, loss = 0.39343506\n",
      "Iteration 78, loss = 0.39330104\n",
      "Iteration 79, loss = 0.39319956\n",
      "Iteration 80, loss = 0.39325884\n",
      "Iteration 81, loss = 0.39346433\n",
      "Iteration 82, loss = 0.39326144\n",
      "Iteration 83, loss = 0.39311608\n",
      "Iteration 84, loss = 0.39341843\n",
      "Iteration 85, loss = 0.39304233\n",
      "Iteration 86, loss = 0.39305374\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70246550\n",
      "Iteration 2, loss = 0.65878566\n",
      "Iteration 3, loss = 0.53494236\n",
      "Iteration 4, loss = 0.42712696\n",
      "Iteration 5, loss = 0.39326677\n",
      "Iteration 6, loss = 0.38500403\n",
      "Iteration 7, loss = 0.38209977\n",
      "Iteration 8, loss = 0.38155536\n",
      "Iteration 9, loss = 0.38077747\n",
      "Iteration 10, loss = 0.37984702\n",
      "Iteration 11, loss = 0.37956773\n",
      "Iteration 12, loss = 0.37893958\n",
      "Iteration 13, loss = 0.38028131\n",
      "Iteration 14, loss = 0.37897172\n",
      "Iteration 15, loss = 0.37909931\n",
      "Iteration 16, loss = 0.37882644\n",
      "Iteration 17, loss = 0.37807860\n",
      "Iteration 18, loss = 0.37807996\n",
      "Iteration 19, loss = 0.37777750\n",
      "Iteration 20, loss = 0.37998376\n",
      "Iteration 21, loss = 0.37775513\n",
      "Iteration 22, loss = 0.37775015\n",
      "Iteration 23, loss = 0.37792078\n",
      "Iteration 24, loss = 0.37789558\n",
      "Iteration 25, loss = 0.37761219\n",
      "Iteration 26, loss = 0.37720921\n",
      "Iteration 27, loss = 0.37737520\n",
      "Iteration 28, loss = 0.37718779\n",
      "Iteration 29, loss = 0.37801291\n",
      "Iteration 30, loss = 0.37783760\n",
      "Iteration 31, loss = 0.37737563\n",
      "Iteration 32, loss = 0.37782098\n",
      "Iteration 33, loss = 0.37731374\n",
      "Iteration 34, loss = 0.37768783\n",
      "Iteration 35, loss = 0.37734776\n",
      "Iteration 36, loss = 0.37759455\n",
      "Iteration 37, loss = 0.37779885\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70469981\n",
      "Iteration 2, loss = 0.65667503\n",
      "Iteration 3, loss = 0.53516571\n",
      "Iteration 4, loss = 0.43490729\n",
      "Iteration 5, loss = 0.40278971\n",
      "Iteration 6, loss = 0.39510383\n",
      "Iteration 7, loss = 0.39106730\n",
      "Iteration 8, loss = 0.39018365\n",
      "Iteration 9, loss = 0.38927349\n",
      "Iteration 10, loss = 0.38806415\n",
      "Iteration 11, loss = 0.38818461\n",
      "Iteration 12, loss = 0.38719864\n",
      "Iteration 13, loss = 0.38778526\n",
      "Iteration 14, loss = 0.38740264\n",
      "Iteration 15, loss = 0.38743055\n",
      "Iteration 16, loss = 0.38666787\n",
      "Iteration 17, loss = 0.38648107\n",
      "Iteration 18, loss = 0.38593677\n",
      "Iteration 19, loss = 0.38642489\n",
      "Iteration 20, loss = 0.38654724\n",
      "Iteration 21, loss = 0.38593526\n",
      "Iteration 22, loss = 0.38691806\n",
      "Iteration 23, loss = 0.38587924\n",
      "Iteration 24, loss = 0.38644673\n",
      "Iteration 25, loss = 0.38558406\n",
      "Iteration 26, loss = 0.38618003\n",
      "Iteration 27, loss = 0.38521179\n",
      "Iteration 28, loss = 0.38497583\n",
      "Iteration 29, loss = 0.38564813\n",
      "Iteration 30, loss = 0.38609616\n",
      "Iteration 31, loss = 0.38640388\n",
      "Iteration 32, loss = 0.38547903\n",
      "Iteration 33, loss = 0.38570486\n",
      "Iteration 34, loss = 0.38535771\n",
      "Iteration 35, loss = 0.38495035\n",
      "Iteration 36, loss = 0.38569851\n",
      "Iteration 37, loss = 0.38556084\n",
      "Iteration 38, loss = 0.38570114\n",
      "Iteration 39, loss = 0.38684483\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70639155\n",
      "Iteration 2, loss = 0.65809456\n",
      "Iteration 3, loss = 0.54394901\n",
      "Iteration 4, loss = 0.44775148\n",
      "Iteration 5, loss = 0.41450657\n",
      "Iteration 6, loss = 0.40454312\n",
      "Iteration 7, loss = 0.40043260\n",
      "Iteration 8, loss = 0.39892674\n",
      "Iteration 9, loss = 0.39842301\n",
      "Iteration 10, loss = 0.39804850\n",
      "Iteration 11, loss = 0.39640619\n",
      "Iteration 12, loss = 0.39704635\n",
      "Iteration 13, loss = 0.39728972\n",
      "Iteration 14, loss = 0.39557464\n",
      "Iteration 15, loss = 0.39559058\n",
      "Iteration 16, loss = 0.39468705\n",
      "Iteration 17, loss = 0.39501523\n",
      "Iteration 18, loss = 0.39433822\n",
      "Iteration 19, loss = 0.39551220\n",
      "Iteration 20, loss = 0.39451956\n",
      "Iteration 21, loss = 0.39408575\n",
      "Iteration 22, loss = 0.39395000\n",
      "Iteration 23, loss = 0.39426399\n",
      "Iteration 24, loss = 0.39372061\n",
      "Iteration 25, loss = 0.39500235\n",
      "Iteration 26, loss = 0.39323874\n",
      "Iteration 27, loss = 0.39387308\n",
      "Iteration 28, loss = 0.39318953\n",
      "Iteration 29, loss = 0.39339912\n",
      "Iteration 30, loss = 0.39311677\n",
      "Iteration 31, loss = 0.39337842\n",
      "Iteration 32, loss = 0.39367085\n",
      "Iteration 33, loss = 0.39301589\n",
      "Iteration 34, loss = 0.39343033\n",
      "Iteration 35, loss = 0.39345228\n",
      "Iteration 36, loss = 0.39377944\n",
      "Iteration 37, loss = 0.39414341\n",
      "Iteration 38, loss = 0.39325507\n",
      "Iteration 39, loss = 0.39320505\n",
      "Iteration 40, loss = 0.39320804\n",
      "Iteration 41, loss = 0.39376629\n",
      "Iteration 42, loss = 0.39296456\n",
      "Iteration 43, loss = 0.39366061\n",
      "Iteration 44, loss = 0.39319660\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.67975992\n",
      "Iteration 2, loss = 0.65394503\n",
      "Iteration 3, loss = 0.64025174\n",
      "Iteration 4, loss = 0.62744581\n",
      "Iteration 5, loss = 0.61550372\n",
      "Iteration 6, loss = 0.60434211\n",
      "Iteration 7, loss = 0.59394855\n",
      "Iteration 8, loss = 0.58423548\n",
      "Iteration 9, loss = 0.57510189\n",
      "Iteration 10, loss = 0.56654467\n",
      "Iteration 11, loss = 0.55856361\n",
      "Iteration 12, loss = 0.55101141\n",
      "Iteration 13, loss = 0.54389842\n",
      "Iteration 14, loss = 0.53727232\n",
      "Iteration 15, loss = 0.53097686\n",
      "Iteration 16, loss = 0.52510123\n",
      "Iteration 17, loss = 0.51951399\n",
      "Iteration 18, loss = 0.51425465\n",
      "Iteration 19, loss = 0.50923013\n",
      "Iteration 20, loss = 0.50451989\n",
      "Iteration 21, loss = 0.50003258\n",
      "Iteration 22, loss = 0.49578168\n",
      "Iteration 23, loss = 0.49176602\n",
      "Iteration 24, loss = 0.48798216\n",
      "Iteration 25, loss = 0.48426453\n",
      "Iteration 26, loss = 0.48075867\n",
      "Iteration 27, loss = 0.47746204\n",
      "Iteration 28, loss = 0.47428984\n",
      "Iteration 29, loss = 0.47125927\n",
      "Iteration 30, loss = 0.46838446\n",
      "Iteration 31, loss = 0.46567031\n",
      "Iteration 32, loss = 0.46302711\n",
      "Iteration 33, loss = 0.46051512\n",
      "Iteration 34, loss = 0.45810512\n",
      "Iteration 35, loss = 0.45577369\n",
      "Iteration 36, loss = 0.45359453\n",
      "Iteration 37, loss = 0.45144778\n",
      "Iteration 38, loss = 0.44942404\n",
      "Iteration 39, loss = 0.44744858\n",
      "Iteration 40, loss = 0.44555896\n",
      "Iteration 41, loss = 0.44374348\n",
      "Iteration 42, loss = 0.44199862\n",
      "Iteration 43, loss = 0.44031514\n",
      "Iteration 44, loss = 0.43868818\n",
      "Iteration 45, loss = 0.43713297\n",
      "Iteration 46, loss = 0.43564446\n",
      "Iteration 47, loss = 0.43418430\n",
      "Iteration 48, loss = 0.43277229\n",
      "Iteration 49, loss = 0.43142580\n",
      "Iteration 50, loss = 0.43011476\n",
      "Iteration 51, loss = 0.42887781\n",
      "Iteration 52, loss = 0.42767442\n",
      "Iteration 53, loss = 0.42649751\n",
      "Iteration 54, loss = 0.42534053\n",
      "Iteration 55, loss = 0.42424997\n",
      "Iteration 56, loss = 0.42318692\n",
      "Iteration 57, loss = 0.42219489\n",
      "Iteration 58, loss = 0.42115542\n",
      "Iteration 59, loss = 0.42019172\n",
      "Iteration 60, loss = 0.41927148\n",
      "Iteration 61, loss = 0.41837301\n",
      "Iteration 62, loss = 0.41746161\n",
      "Iteration 63, loss = 0.41660835\n",
      "Iteration 64, loss = 0.41577336\n",
      "Iteration 65, loss = 0.41495857\n",
      "Iteration 66, loss = 0.41417224\n",
      "Iteration 67, loss = 0.41342853\n",
      "Iteration 68, loss = 0.41267280\n",
      "Iteration 69, loss = 0.41197754\n",
      "Iteration 70, loss = 0.41126790\n",
      "Iteration 71, loss = 0.41057622\n",
      "Iteration 72, loss = 0.40991620\n",
      "Iteration 73, loss = 0.40926147\n",
      "Iteration 74, loss = 0.40864785\n",
      "Iteration 75, loss = 0.40803176\n",
      "Iteration 76, loss = 0.40743332\n",
      "Iteration 77, loss = 0.40686185\n",
      "Iteration 78, loss = 0.40628510\n",
      "Iteration 79, loss = 0.40574867\n",
      "Iteration 80, loss = 0.40524613\n",
      "Iteration 81, loss = 0.40471775\n",
      "Iteration 82, loss = 0.40418212\n",
      "Iteration 83, loss = 0.40369929\n",
      "Iteration 84, loss = 0.40321324\n",
      "Iteration 85, loss = 0.40273819\n",
      "Iteration 86, loss = 0.40229465\n",
      "Iteration 87, loss = 0.40183837\n",
      "Iteration 88, loss = 0.40143566\n",
      "Iteration 89, loss = 0.40096474\n",
      "Iteration 90, loss = 0.40054311\n",
      "Iteration 91, loss = 0.40014653\n",
      "Iteration 92, loss = 0.39973768\n",
      "Iteration 93, loss = 0.39935729\n",
      "Iteration 94, loss = 0.39897832\n",
      "Iteration 95, loss = 0.39860988\n",
      "Iteration 96, loss = 0.39823472\n",
      "Iteration 97, loss = 0.39789983\n",
      "Iteration 98, loss = 0.39755490\n",
      "Iteration 99, loss = 0.39723846\n",
      "Iteration 100, loss = 0.39688395\n",
      "Iteration 101, loss = 0.39654767\n",
      "Iteration 102, loss = 0.39625585\n",
      "Iteration 103, loss = 0.39601071\n",
      "Iteration 104, loss = 0.39563180\n",
      "Iteration 105, loss = 0.39532177\n",
      "Iteration 106, loss = 0.39507569\n",
      "Iteration 107, loss = 0.39475189\n",
      "Iteration 108, loss = 0.39450141\n",
      "Iteration 109, loss = 0.39421241\n",
      "Iteration 110, loss = 0.39394840\n",
      "Iteration 111, loss = 0.39366603\n",
      "Iteration 112, loss = 0.39342268\n",
      "Iteration 113, loss = 0.39315615\n",
      "Iteration 114, loss = 0.39292325\n",
      "Iteration 115, loss = 0.39267577\n",
      "Iteration 116, loss = 0.39246617\n",
      "Iteration 117, loss = 0.39222105\n",
      "Iteration 118, loss = 0.39200429\n",
      "Iteration 119, loss = 0.39178613\n",
      "Iteration 120, loss = 0.39158150\n",
      "Iteration 121, loss = 0.39135617\n",
      "Iteration 122, loss = 0.39113210\n",
      "Iteration 123, loss = 0.39091701\n",
      "Iteration 124, loss = 0.39071917\n",
      "Iteration 125, loss = 0.39051080\n",
      "Iteration 126, loss = 0.39035415\n",
      "Iteration 127, loss = 0.39017766\n",
      "Iteration 128, loss = 0.38995232\n",
      "Iteration 129, loss = 0.38977825\n",
      "Iteration 130, loss = 0.38962438\n",
      "Iteration 131, loss = 0.38941100\n",
      "Iteration 132, loss = 0.38924658\n",
      "Iteration 133, loss = 0.38906564\n",
      "Iteration 134, loss = 0.38890162\n",
      "Iteration 135, loss = 0.38874713\n",
      "Iteration 136, loss = 0.38857102\n",
      "Iteration 137, loss = 0.38844559\n",
      "Iteration 138, loss = 0.38827135\n",
      "Iteration 139, loss = 0.38815425\n",
      "Iteration 140, loss = 0.38797416\n",
      "Iteration 141, loss = 0.38782343\n",
      "Iteration 142, loss = 0.38767555\n",
      "Iteration 143, loss = 0.38753662\n",
      "Iteration 144, loss = 0.38741485\n",
      "Iteration 145, loss = 0.38728399\n",
      "Iteration 146, loss = 0.38713573\n",
      "Iteration 147, loss = 0.38701585\n",
      "Iteration 148, loss = 0.38688215\n",
      "Iteration 149, loss = 0.38679029\n",
      "Iteration 150, loss = 0.38661751\n",
      "Iteration 151, loss = 0.38648722\n",
      "Iteration 152, loss = 0.38638827\n",
      "Iteration 153, loss = 0.38625768\n",
      "Iteration 154, loss = 0.38613265\n",
      "Iteration 155, loss = 0.38602040\n",
      "Iteration 156, loss = 0.38591937\n",
      "Iteration 157, loss = 0.38579558\n",
      "Iteration 158, loss = 0.38569763\n",
      "Iteration 159, loss = 0.38559092\n",
      "Iteration 160, loss = 0.38549775\n",
      "Iteration 161, loss = 0.38536754\n",
      "Iteration 162, loss = 0.38527754\n",
      "Iteration 163, loss = 0.38518563\n",
      "Iteration 164, loss = 0.38507237\n",
      "Iteration 165, loss = 0.38500443\n",
      "Iteration 166, loss = 0.38486681\n",
      "Iteration 167, loss = 0.38479484\n",
      "Iteration 168, loss = 0.38469886\n",
      "Iteration 169, loss = 0.38462863\n",
      "Iteration 170, loss = 0.38450799\n",
      "Iteration 171, loss = 0.38442680\n",
      "Iteration 172, loss = 0.38432120\n",
      "Iteration 173, loss = 0.38423057\n",
      "Iteration 174, loss = 0.38415343\n",
      "Iteration 175, loss = 0.38406531\n",
      "Iteration 176, loss = 0.38397813\n",
      "Iteration 177, loss = 0.38389846\n",
      "Iteration 178, loss = 0.38382336\n",
      "Iteration 179, loss = 0.38374538\n",
      "Iteration 180, loss = 0.38366613\n",
      "Iteration 181, loss = 0.38359614\n",
      "Iteration 182, loss = 0.38352467\n",
      "Iteration 183, loss = 0.38344031\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.68096745\n",
      "Iteration 2, loss = 0.65469753\n",
      "Iteration 3, loss = 0.64100819\n",
      "Iteration 4, loss = 0.62834740\n",
      "Iteration 5, loss = 0.61659484\n",
      "Iteration 6, loss = 0.60551340\n",
      "Iteration 7, loss = 0.59529300\n",
      "Iteration 8, loss = 0.58568962\n",
      "Iteration 9, loss = 0.57673775\n",
      "Iteration 10, loss = 0.56831809\n",
      "Iteration 11, loss = 0.56045394\n",
      "Iteration 12, loss = 0.55307844\n",
      "Iteration 13, loss = 0.54614795\n",
      "Iteration 14, loss = 0.53965212\n",
      "Iteration 15, loss = 0.53350272\n",
      "Iteration 16, loss = 0.52775118\n",
      "Iteration 17, loss = 0.52230853\n",
      "Iteration 18, loss = 0.51716000\n",
      "Iteration 19, loss = 0.51232664\n",
      "Iteration 20, loss = 0.50770218\n",
      "Iteration 21, loss = 0.50340349\n",
      "Iteration 22, loss = 0.49928523\n",
      "Iteration 23, loss = 0.49531844\n",
      "Iteration 24, loss = 0.49159389\n",
      "Iteration 25, loss = 0.48807535\n",
      "Iteration 26, loss = 0.48469710\n",
      "Iteration 27, loss = 0.48152682\n",
      "Iteration 28, loss = 0.47846663\n",
      "Iteration 29, loss = 0.47552134\n",
      "Iteration 30, loss = 0.47278048\n",
      "Iteration 31, loss = 0.47011168\n",
      "Iteration 32, loss = 0.46756721\n",
      "Iteration 33, loss = 0.46513418\n",
      "Iteration 34, loss = 0.46277505\n",
      "Iteration 35, loss = 0.46054747\n",
      "Iteration 36, loss = 0.45841713\n",
      "Iteration 37, loss = 0.45636615\n",
      "Iteration 38, loss = 0.45441651\n",
      "Iteration 39, loss = 0.45254788\n",
      "Iteration 40, loss = 0.45074251\n",
      "Iteration 41, loss = 0.44902242\n",
      "Iteration 42, loss = 0.44733150\n",
      "Iteration 43, loss = 0.44569676\n",
      "Iteration 44, loss = 0.44422517\n",
      "Iteration 45, loss = 0.44263920\n",
      "Iteration 46, loss = 0.44121583\n",
      "Iteration 47, loss = 0.43983401\n",
      "Iteration 48, loss = 0.43848258\n",
      "Iteration 49, loss = 0.43719954\n",
      "Iteration 50, loss = 0.43594846\n",
      "Iteration 51, loss = 0.43478730\n",
      "Iteration 52, loss = 0.43357975\n",
      "Iteration 53, loss = 0.43247088\n",
      "Iteration 54, loss = 0.43137450\n",
      "Iteration 55, loss = 0.43035887\n",
      "Iteration 56, loss = 0.42929738\n",
      "Iteration 57, loss = 0.42832837\n",
      "Iteration 58, loss = 0.42737085\n",
      "Iteration 59, loss = 0.42643860\n",
      "Iteration 60, loss = 0.42556361\n",
      "Iteration 61, loss = 0.42472270\n",
      "Iteration 62, loss = 0.42382953\n",
      "Iteration 63, loss = 0.42302247\n",
      "Iteration 64, loss = 0.42223520\n",
      "Iteration 65, loss = 0.42146022\n",
      "Iteration 66, loss = 0.42071577\n",
      "Iteration 67, loss = 0.42002390\n",
      "Iteration 68, loss = 0.41931070\n",
      "Iteration 69, loss = 0.41862675\n",
      "Iteration 70, loss = 0.41795749\n",
      "Iteration 71, loss = 0.41732392\n",
      "Iteration 72, loss = 0.41666434\n",
      "Iteration 73, loss = 0.41604113\n",
      "Iteration 74, loss = 0.41546091\n",
      "Iteration 75, loss = 0.41486290\n",
      "Iteration 76, loss = 0.41433573\n",
      "Iteration 77, loss = 0.41380042\n",
      "Iteration 78, loss = 0.41322637\n",
      "Iteration 79, loss = 0.41269842\n",
      "Iteration 80, loss = 0.41221821\n",
      "Iteration 81, loss = 0.41174334\n",
      "Iteration 82, loss = 0.41122828\n",
      "Iteration 83, loss = 0.41075244\n",
      "Iteration 84, loss = 0.41030182\n",
      "Iteration 85, loss = 0.40983739\n",
      "Iteration 86, loss = 0.40941621\n",
      "Iteration 87, loss = 0.40898637\n",
      "Iteration 88, loss = 0.40860814\n",
      "Iteration 89, loss = 0.40816173\n",
      "Iteration 90, loss = 0.40780168\n",
      "Iteration 91, loss = 0.40739669\n",
      "Iteration 92, loss = 0.40700270\n",
      "Iteration 93, loss = 0.40663664\n",
      "Iteration 94, loss = 0.40631053\n",
      "Iteration 95, loss = 0.40592954\n",
      "Iteration 96, loss = 0.40559791\n",
      "Iteration 97, loss = 0.40530185\n",
      "Iteration 98, loss = 0.40495105\n",
      "Iteration 99, loss = 0.40464713\n",
      "Iteration 100, loss = 0.40430916\n",
      "Iteration 101, loss = 0.40399217\n",
      "Iteration 102, loss = 0.40369241\n",
      "Iteration 103, loss = 0.40339014\n",
      "Iteration 104, loss = 0.40316298\n",
      "Iteration 105, loss = 0.40282448\n",
      "Iteration 106, loss = 0.40256740\n",
      "Iteration 107, loss = 0.40230118\n",
      "Iteration 108, loss = 0.40204782\n",
      "Iteration 109, loss = 0.40180161\n",
      "Iteration 110, loss = 0.40157117\n",
      "Iteration 111, loss = 0.40128480\n",
      "Iteration 112, loss = 0.40102933\n",
      "Iteration 113, loss = 0.40079935\n",
      "Iteration 114, loss = 0.40055583\n",
      "Iteration 115, loss = 0.40036770\n",
      "Iteration 116, loss = 0.40018141\n",
      "Iteration 117, loss = 0.39989102\n",
      "Iteration 118, loss = 0.39969692\n",
      "Iteration 119, loss = 0.39951670\n",
      "Iteration 120, loss = 0.39928459\n",
      "Iteration 121, loss = 0.39908037\n",
      "Iteration 122, loss = 0.39889245\n",
      "Iteration 123, loss = 0.39868796\n",
      "Iteration 124, loss = 0.39849273\n",
      "Iteration 125, loss = 0.39830957\n",
      "Iteration 126, loss = 0.39813553\n",
      "Iteration 127, loss = 0.39795802\n",
      "Iteration 128, loss = 0.39776601\n",
      "Iteration 129, loss = 0.39761456\n",
      "Iteration 130, loss = 0.39743617\n",
      "Iteration 131, loss = 0.39726661\n",
      "Iteration 132, loss = 0.39709438\n",
      "Iteration 133, loss = 0.39694628\n",
      "Iteration 134, loss = 0.39679043\n",
      "Iteration 135, loss = 0.39663167\n",
      "Iteration 136, loss = 0.39650601\n",
      "Iteration 137, loss = 0.39632842\n",
      "Iteration 138, loss = 0.39618630\n",
      "Iteration 139, loss = 0.39607467\n",
      "Iteration 140, loss = 0.39590802\n",
      "Iteration 141, loss = 0.39576972\n",
      "Iteration 142, loss = 0.39563962\n",
      "Iteration 143, loss = 0.39552820\n",
      "Iteration 144, loss = 0.39538837\n",
      "Iteration 145, loss = 0.39527847\n",
      "Iteration 146, loss = 0.39513736\n",
      "Iteration 147, loss = 0.39500512\n",
      "Iteration 148, loss = 0.39494679\n",
      "Iteration 149, loss = 0.39479699\n",
      "Iteration 150, loss = 0.39464379\n",
      "Iteration 151, loss = 0.39454978\n",
      "Iteration 152, loss = 0.39442062\n",
      "Iteration 153, loss = 0.39430720\n",
      "Iteration 154, loss = 0.39419306\n",
      "Iteration 155, loss = 0.39410386\n",
      "Iteration 156, loss = 0.39401995\n",
      "Iteration 157, loss = 0.39386948\n",
      "Iteration 158, loss = 0.39377297\n",
      "Iteration 159, loss = 0.39367799\n",
      "Iteration 160, loss = 0.39357428\n",
      "Iteration 161, loss = 0.39346787\n",
      "Iteration 162, loss = 0.39338984\n",
      "Iteration 163, loss = 0.39329983\n",
      "Iteration 164, loss = 0.39319816\n",
      "Iteration 165, loss = 0.39310170\n",
      "Iteration 166, loss = 0.39300786\n",
      "Iteration 167, loss = 0.39291308\n",
      "Iteration 168, loss = 0.39282606\n",
      "Iteration 169, loss = 0.39274477\n",
      "Iteration 170, loss = 0.39266182\n",
      "Iteration 171, loss = 0.39256056\n",
      "Iteration 172, loss = 0.39248871\n",
      "Iteration 173, loss = 0.39242259\n",
      "Iteration 174, loss = 0.39233004\n",
      "Iteration 175, loss = 0.39226319\n",
      "Iteration 176, loss = 0.39220196\n",
      "Iteration 177, loss = 0.39210147\n",
      "Iteration 178, loss = 0.39203286\n",
      "Iteration 179, loss = 0.39196195\n",
      "Iteration 180, loss = 0.39190970\n",
      "Iteration 181, loss = 0.39187196\n",
      "Iteration 182, loss = 0.39172833\n",
      "Iteration 183, loss = 0.39165790\n",
      "Iteration 184, loss = 0.39158682\n",
      "Iteration 185, loss = 0.39152429\n",
      "Iteration 186, loss = 0.39146731\n",
      "Iteration 187, loss = 0.39140157\n",
      "Iteration 188, loss = 0.39133372\n",
      "Iteration 189, loss = 0.39126538\n",
      "Iteration 190, loss = 0.39121821\n",
      "Iteration 191, loss = 0.39115364\n",
      "Iteration 192, loss = 0.39108148\n",
      "Iteration 193, loss = 0.39102564\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.68141899\n",
      "Iteration 2, loss = 0.65537792\n",
      "Iteration 3, loss = 0.64202212\n",
      "Iteration 4, loss = 0.62960945\n",
      "Iteration 5, loss = 0.61804165\n",
      "Iteration 6, loss = 0.60727575\n",
      "Iteration 7, loss = 0.59722430\n",
      "Iteration 8, loss = 0.58790051\n",
      "Iteration 9, loss = 0.57913998\n",
      "Iteration 10, loss = 0.57095438\n",
      "Iteration 11, loss = 0.56324110\n",
      "Iteration 12, loss = 0.55603865\n",
      "Iteration 13, loss = 0.54929016\n",
      "Iteration 14, loss = 0.54293651\n",
      "Iteration 15, loss = 0.53695108\n",
      "Iteration 16, loss = 0.53131137\n",
      "Iteration 17, loss = 0.52602478\n",
      "Iteration 18, loss = 0.52098861\n",
      "Iteration 19, loss = 0.51627790\n",
      "Iteration 20, loss = 0.51181850\n",
      "Iteration 21, loss = 0.50756766\n",
      "Iteration 22, loss = 0.50361994\n",
      "Iteration 23, loss = 0.49976005\n",
      "Iteration 24, loss = 0.49618870\n",
      "Iteration 25, loss = 0.49268475\n",
      "Iteration 26, loss = 0.48943315\n",
      "Iteration 27, loss = 0.48629896\n",
      "Iteration 28, loss = 0.48335458\n",
      "Iteration 29, loss = 0.48050529\n",
      "Iteration 30, loss = 0.47781597\n",
      "Iteration 31, loss = 0.47527078\n",
      "Iteration 32, loss = 0.47279937\n",
      "Iteration 33, loss = 0.47044763\n",
      "Iteration 34, loss = 0.46820538\n",
      "Iteration 35, loss = 0.46605292\n",
      "Iteration 36, loss = 0.46398667\n",
      "Iteration 37, loss = 0.46207913\n",
      "Iteration 38, loss = 0.46013646\n",
      "Iteration 39, loss = 0.45833888\n",
      "Iteration 40, loss = 0.45658145\n",
      "Iteration 41, loss = 0.45488699\n",
      "Iteration 42, loss = 0.45327073\n",
      "Iteration 43, loss = 0.45170949\n",
      "Iteration 44, loss = 0.45021077\n",
      "Iteration 45, loss = 0.44882629\n",
      "Iteration 46, loss = 0.44738121\n",
      "Iteration 47, loss = 0.44607631\n",
      "Iteration 48, loss = 0.44479248\n",
      "Iteration 49, loss = 0.44355038\n",
      "Iteration 50, loss = 0.44234209\n",
      "Iteration 51, loss = 0.44122230\n",
      "Iteration 52, loss = 0.44013568\n",
      "Iteration 53, loss = 0.43899096\n",
      "Iteration 54, loss = 0.43794244\n",
      "Iteration 55, loss = 0.43694516\n",
      "Iteration 56, loss = 0.43595125\n",
      "Iteration 57, loss = 0.43501159\n",
      "Iteration 58, loss = 0.43413256\n",
      "Iteration 59, loss = 0.43321330\n",
      "Iteration 60, loss = 0.43236328\n",
      "Iteration 61, loss = 0.43157280\n",
      "Iteration 62, loss = 0.43072204\n",
      "Iteration 63, loss = 0.42996186\n",
      "Iteration 64, loss = 0.42919477\n",
      "Iteration 65, loss = 0.42845302\n",
      "Iteration 66, loss = 0.42776148\n",
      "Iteration 67, loss = 0.42706374\n",
      "Iteration 68, loss = 0.42637863\n",
      "Iteration 69, loss = 0.42573738\n",
      "Iteration 70, loss = 0.42513581\n",
      "Iteration 71, loss = 0.42447435\n",
      "Iteration 72, loss = 0.42389743\n",
      "Iteration 73, loss = 0.42330507\n",
      "Iteration 74, loss = 0.42273083\n",
      "Iteration 75, loss = 0.42219151\n",
      "Iteration 76, loss = 0.42165031\n",
      "Iteration 77, loss = 0.42113017\n",
      "Iteration 78, loss = 0.42063949\n",
      "Iteration 79, loss = 0.42014178\n",
      "Iteration 80, loss = 0.41964797\n",
      "Iteration 81, loss = 0.41917497\n",
      "Iteration 82, loss = 0.41877700\n",
      "Iteration 83, loss = 0.41827640\n",
      "Iteration 84, loss = 0.41785854\n",
      "Iteration 85, loss = 0.41745097\n",
      "Iteration 86, loss = 0.41701234\n",
      "Iteration 87, loss = 0.41660787\n",
      "Iteration 88, loss = 0.41621742\n",
      "Iteration 89, loss = 0.41587930\n",
      "Iteration 90, loss = 0.41547331\n",
      "Iteration 91, loss = 0.41509757\n",
      "Iteration 92, loss = 0.41477417\n",
      "Iteration 93, loss = 0.41438582\n",
      "Iteration 94, loss = 0.41405983\n",
      "Iteration 95, loss = 0.41374631\n",
      "Iteration 96, loss = 0.41343404\n",
      "Iteration 97, loss = 0.41308488\n",
      "Iteration 98, loss = 0.41279663\n",
      "Iteration 99, loss = 0.41249017\n",
      "Iteration 100, loss = 0.41217436\n",
      "Iteration 101, loss = 0.41188377\n",
      "Iteration 102, loss = 0.41162659\n",
      "Iteration 103, loss = 0.41132345\n",
      "Iteration 104, loss = 0.41107293\n",
      "Iteration 105, loss = 0.41079431\n",
      "Iteration 106, loss = 0.41055862\n",
      "Iteration 107, loss = 0.41028782\n",
      "Iteration 108, loss = 0.41006800\n",
      "Iteration 109, loss = 0.40979002\n",
      "Iteration 110, loss = 0.40957157\n",
      "Iteration 111, loss = 0.40933085\n",
      "Iteration 112, loss = 0.40910751\n",
      "Iteration 113, loss = 0.40888348\n",
      "Iteration 114, loss = 0.40865985\n",
      "Iteration 115, loss = 0.40843719\n",
      "Iteration 116, loss = 0.40824072\n",
      "Iteration 117, loss = 0.40805422\n",
      "Iteration 118, loss = 0.40782545\n",
      "Iteration 119, loss = 0.40766045\n",
      "Iteration 120, loss = 0.40745634\n",
      "Iteration 121, loss = 0.40727225\n",
      "Iteration 122, loss = 0.40707183\n",
      "Iteration 123, loss = 0.40692628\n",
      "Iteration 124, loss = 0.40675584\n",
      "Iteration 125, loss = 0.40652470\n",
      "Iteration 126, loss = 0.40636559\n",
      "Iteration 127, loss = 0.40620485\n",
      "Iteration 128, loss = 0.40603637\n",
      "Iteration 129, loss = 0.40587451\n",
      "Iteration 130, loss = 0.40573880\n",
      "Iteration 131, loss = 0.40558485\n",
      "Iteration 132, loss = 0.40540655\n",
      "Iteration 133, loss = 0.40530906\n",
      "Iteration 134, loss = 0.40512687\n",
      "Iteration 135, loss = 0.40497159\n",
      "Iteration 136, loss = 0.40486785\n",
      "Iteration 137, loss = 0.40469542\n",
      "Iteration 138, loss = 0.40458837\n",
      "Iteration 139, loss = 0.40442025\n",
      "Iteration 140, loss = 0.40428210\n",
      "Iteration 141, loss = 0.40415669\n",
      "Iteration 142, loss = 0.40404056\n",
      "Iteration 143, loss = 0.40391945\n",
      "Iteration 144, loss = 0.40379621\n",
      "Iteration 145, loss = 0.40369198\n",
      "Iteration 146, loss = 0.40360261\n",
      "Iteration 147, loss = 0.40348551\n",
      "Iteration 148, loss = 0.40330754\n",
      "Iteration 149, loss = 0.40321345\n",
      "Iteration 150, loss = 0.40309030\n",
      "Iteration 151, loss = 0.40304496\n",
      "Iteration 152, loss = 0.40291144\n",
      "Iteration 153, loss = 0.40279232\n",
      "Iteration 154, loss = 0.40271245\n",
      "Iteration 155, loss = 0.40258354\n",
      "Iteration 156, loss = 0.40245046\n",
      "Iteration 157, loss = 0.40241660\n",
      "Iteration 158, loss = 0.40228584\n",
      "Iteration 159, loss = 0.40217786\n",
      "Iteration 160, loss = 0.40209015\n",
      "Iteration 161, loss = 0.40198921\n",
      "Iteration 162, loss = 0.40193369\n",
      "Iteration 163, loss = 0.40184108\n",
      "Iteration 164, loss = 0.40174933\n",
      "Iteration 165, loss = 0.40164994\n",
      "Iteration 166, loss = 0.40155141\n",
      "Iteration 167, loss = 0.40147067\n",
      "Iteration 168, loss = 0.40138875\n",
      "Iteration 169, loss = 0.40130921\n",
      "Iteration 170, loss = 0.40122008\n",
      "Iteration 171, loss = 0.40114122\n",
      "Iteration 172, loss = 0.40107690\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.67807457\n",
      "Iteration 2, loss = 0.64400744\n",
      "Iteration 3, loss = 0.61406341\n",
      "Iteration 4, loss = 0.58774510\n",
      "Iteration 5, loss = 0.56469772\n",
      "Iteration 6, loss = 0.54437827\n",
      "Iteration 7, loss = 0.52667325\n",
      "Iteration 8, loss = 0.51113013\n",
      "Iteration 9, loss = 0.49756639\n",
      "Iteration 10, loss = 0.48556221\n",
      "Iteration 11, loss = 0.47518821\n",
      "Iteration 12, loss = 0.46587686\n",
      "Iteration 13, loss = 0.45759234\n",
      "Iteration 14, loss = 0.45031285\n",
      "Iteration 15, loss = 0.44367817\n",
      "Iteration 16, loss = 0.43799484\n",
      "Iteration 17, loss = 0.43278609\n",
      "Iteration 18, loss = 0.42811299\n",
      "Iteration 19, loss = 0.42370272\n",
      "Iteration 20, loss = 0.41986931\n",
      "Iteration 21, loss = 0.41628781\n",
      "Iteration 22, loss = 0.41332541\n",
      "Iteration 23, loss = 0.41036882\n",
      "Iteration 24, loss = 0.40794541\n",
      "Iteration 25, loss = 0.40544150\n",
      "Iteration 26, loss = 0.40306694\n",
      "Iteration 27, loss = 0.40134183\n",
      "Iteration 28, loss = 0.39936827\n",
      "Iteration 29, loss = 0.39774737\n",
      "Iteration 30, loss = 0.39620493\n",
      "Iteration 31, loss = 0.39483038\n",
      "Iteration 32, loss = 0.39353154\n",
      "Iteration 33, loss = 0.39234303\n",
      "Iteration 34, loss = 0.39125778\n",
      "Iteration 35, loss = 0.39021676\n",
      "Iteration 36, loss = 0.38924239\n",
      "Iteration 37, loss = 0.38840829\n",
      "Iteration 38, loss = 0.38768012\n",
      "Iteration 39, loss = 0.38697751\n",
      "Iteration 40, loss = 0.38633819\n",
      "Iteration 41, loss = 0.38563294\n",
      "Iteration 42, loss = 0.38497561\n",
      "Iteration 43, loss = 0.38454556\n",
      "Iteration 44, loss = 0.38400243\n",
      "Iteration 45, loss = 0.38357637\n",
      "Iteration 46, loss = 0.38303972\n",
      "Iteration 47, loss = 0.38270372\n",
      "Iteration 48, loss = 0.38229749\n",
      "Iteration 49, loss = 0.38189062\n",
      "Iteration 50, loss = 0.38165360\n",
      "Iteration 51, loss = 0.38134230\n",
      "Iteration 52, loss = 0.38112411\n",
      "Iteration 53, loss = 0.38087168\n",
      "Iteration 54, loss = 0.38051023\n",
      "Iteration 55, loss = 0.38029303\n",
      "Iteration 56, loss = 0.38010704\n",
      "Iteration 57, loss = 0.38025077\n",
      "Iteration 58, loss = 0.37969452\n",
      "Iteration 59, loss = 0.37958606\n",
      "Iteration 60, loss = 0.37937637\n",
      "Iteration 61, loss = 0.37933653\n",
      "Iteration 62, loss = 0.37904805\n",
      "Iteration 63, loss = 0.37898092\n",
      "Iteration 64, loss = 0.37879118\n",
      "Iteration 65, loss = 0.37861754\n",
      "Iteration 66, loss = 0.37852689\n",
      "Iteration 67, loss = 0.37842127\n",
      "Iteration 68, loss = 0.37833537\n",
      "Iteration 69, loss = 0.37828527\n",
      "Iteration 70, loss = 0.37826140\n",
      "Iteration 71, loss = 0.37808661\n",
      "Iteration 72, loss = 0.37799867\n",
      "Iteration 73, loss = 0.37784685\n",
      "Iteration 74, loss = 0.37783545\n",
      "Iteration 75, loss = 0.37772873\n",
      "Iteration 76, loss = 0.37759063\n",
      "Iteration 77, loss = 0.37771393\n",
      "Iteration 78, loss = 0.37750678\n",
      "Iteration 79, loss = 0.37755491\n",
      "Iteration 80, loss = 0.37750775\n",
      "Iteration 81, loss = 0.37756435\n",
      "Iteration 82, loss = 0.37726079\n",
      "Iteration 83, loss = 0.37729369\n",
      "Iteration 84, loss = 0.37728180\n",
      "Iteration 85, loss = 0.37719325\n",
      "Iteration 86, loss = 0.37716571\n",
      "Iteration 87, loss = 0.37729019\n",
      "Iteration 88, loss = 0.37710237\n",
      "Iteration 89, loss = 0.37702601\n",
      "Iteration 90, loss = 0.37695806\n",
      "Iteration 91, loss = 0.37684368\n",
      "Iteration 92, loss = 0.37695305\n",
      "Iteration 93, loss = 0.37692174\n",
      "Iteration 94, loss = 0.37674693\n",
      "Iteration 95, loss = 0.37686086\n",
      "Iteration 96, loss = 0.37677293\n",
      "Iteration 97, loss = 0.37684342\n",
      "Iteration 98, loss = 0.37679048\n",
      "Iteration 99, loss = 0.37688432\n",
      "Iteration 100, loss = 0.37680401\n",
      "Iteration 101, loss = 0.37662587\n",
      "Iteration 102, loss = 0.37664231\n",
      "Iteration 103, loss = 0.37724151\n",
      "Iteration 104, loss = 0.37658097\n",
      "Iteration 105, loss = 0.37653200\n",
      "Iteration 106, loss = 0.37641568\n",
      "Iteration 107, loss = 0.37663919\n",
      "Iteration 108, loss = 0.37659403\n",
      "Iteration 109, loss = 0.37656416\n",
      "Iteration 110, loss = 0.37636102\n",
      "Iteration 111, loss = 0.37647009\n",
      "Iteration 112, loss = 0.37644802\n",
      "Iteration 113, loss = 0.37639419\n",
      "Iteration 114, loss = 0.37639010\n",
      "Iteration 115, loss = 0.37636705\n",
      "Iteration 116, loss = 0.37638774\n",
      "Iteration 117, loss = 0.37643378\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.68079110\n",
      "Iteration 2, loss = 0.64886648\n",
      "Iteration 3, loss = 0.62025700\n",
      "Iteration 4, loss = 0.59456385\n",
      "Iteration 5, loss = 0.57153411\n",
      "Iteration 6, loss = 0.55101340\n",
      "Iteration 7, loss = 0.53323600\n",
      "Iteration 8, loss = 0.51752472\n",
      "Iteration 9, loss = 0.50408670\n",
      "Iteration 10, loss = 0.49201328\n",
      "Iteration 11, loss = 0.48148173\n",
      "Iteration 12, loss = 0.47216394\n",
      "Iteration 13, loss = 0.46401532\n",
      "Iteration 14, loss = 0.45668856\n",
      "Iteration 15, loss = 0.45016583\n",
      "Iteration 16, loss = 0.44455048\n",
      "Iteration 17, loss = 0.43945470\n",
      "Iteration 18, loss = 0.43476812\n",
      "Iteration 19, loss = 0.43062741\n",
      "Iteration 20, loss = 0.42685995\n",
      "Iteration 21, loss = 0.42360773\n",
      "Iteration 22, loss = 0.42066468\n",
      "Iteration 23, loss = 0.41776444\n",
      "Iteration 24, loss = 0.41517724\n",
      "Iteration 25, loss = 0.41293290\n",
      "Iteration 26, loss = 0.41090960\n",
      "Iteration 27, loss = 0.40907230\n",
      "Iteration 28, loss = 0.40736176\n",
      "Iteration 29, loss = 0.40565292\n",
      "Iteration 30, loss = 0.40432061\n",
      "Iteration 31, loss = 0.40286849\n",
      "Iteration 32, loss = 0.40179114\n",
      "Iteration 33, loss = 0.40050933\n",
      "Iteration 34, loss = 0.39946918\n",
      "Iteration 35, loss = 0.39856653\n",
      "Iteration 36, loss = 0.39768050\n",
      "Iteration 37, loss = 0.39682209\n",
      "Iteration 38, loss = 0.39608962\n",
      "Iteration 39, loss = 0.39541502\n",
      "Iteration 40, loss = 0.39474470\n",
      "Iteration 41, loss = 0.39432655\n",
      "Iteration 42, loss = 0.39357517\n",
      "Iteration 43, loss = 0.39315242\n",
      "Iteration 44, loss = 0.39292430\n",
      "Iteration 45, loss = 0.39214076\n",
      "Iteration 46, loss = 0.39173812\n",
      "Iteration 47, loss = 0.39139450\n",
      "Iteration 48, loss = 0.39097310\n",
      "Iteration 49, loss = 0.39065984\n",
      "Iteration 50, loss = 0.39034745\n",
      "Iteration 51, loss = 0.39021275\n",
      "Iteration 52, loss = 0.38978118\n",
      "Iteration 53, loss = 0.38954283\n",
      "Iteration 54, loss = 0.38928077\n",
      "Iteration 55, loss = 0.38923245\n",
      "Iteration 56, loss = 0.38874895\n",
      "Iteration 57, loss = 0.38864057\n",
      "Iteration 58, loss = 0.38846868\n",
      "Iteration 59, loss = 0.38829711\n",
      "Iteration 60, loss = 0.38807913\n",
      "Iteration 61, loss = 0.38803962\n",
      "Iteration 62, loss = 0.38782672\n",
      "Iteration 63, loss = 0.38759449\n",
      "Iteration 64, loss = 0.38749273\n",
      "Iteration 65, loss = 0.38735149\n",
      "Iteration 66, loss = 0.38724186\n",
      "Iteration 67, loss = 0.38723244\n",
      "Iteration 68, loss = 0.38700069\n",
      "Iteration 69, loss = 0.38684823\n",
      "Iteration 70, loss = 0.38708626\n",
      "Iteration 71, loss = 0.38674504\n",
      "Iteration 72, loss = 0.38650689\n",
      "Iteration 73, loss = 0.38658982\n",
      "Iteration 74, loss = 0.38651615\n",
      "Iteration 75, loss = 0.38631059\n",
      "Iteration 76, loss = 0.38634127\n",
      "Iteration 77, loss = 0.38661436\n",
      "Iteration 78, loss = 0.38619108\n",
      "Iteration 79, loss = 0.38613281\n",
      "Iteration 80, loss = 0.38614142\n",
      "Iteration 81, loss = 0.38617317\n",
      "Iteration 82, loss = 0.38598187\n",
      "Iteration 83, loss = 0.38585702\n",
      "Iteration 84, loss = 0.38583391\n",
      "Iteration 85, loss = 0.38571772\n",
      "Iteration 86, loss = 0.38572335\n",
      "Iteration 87, loss = 0.38567682\n",
      "Iteration 88, loss = 0.38551981\n",
      "Iteration 89, loss = 0.38567635\n",
      "Iteration 90, loss = 0.38562767\n",
      "Iteration 91, loss = 0.38567658\n",
      "Iteration 92, loss = 0.38545850\n",
      "Iteration 93, loss = 0.38537939\n",
      "Iteration 94, loss = 0.38554306\n",
      "Iteration 95, loss = 0.38542791\n",
      "Iteration 96, loss = 0.38530168\n",
      "Iteration 97, loss = 0.38538690\n",
      "Iteration 98, loss = 0.38540239\n",
      "Iteration 99, loss = 0.38549597\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.68190202\n",
      "Iteration 2, loss = 0.65073910\n",
      "Iteration 3, loss = 0.62273679\n",
      "Iteration 4, loss = 0.59721567\n",
      "Iteration 5, loss = 0.57470598\n",
      "Iteration 6, loss = 0.55489499\n",
      "Iteration 7, loss = 0.53742411\n",
      "Iteration 8, loss = 0.52220870\n",
      "Iteration 9, loss = 0.50896895\n",
      "Iteration 10, loss = 0.49721758\n",
      "Iteration 11, loss = 0.48696661\n",
      "Iteration 12, loss = 0.47775340\n",
      "Iteration 13, loss = 0.46979228\n",
      "Iteration 14, loss = 0.46281380\n",
      "Iteration 15, loss = 0.45650198\n",
      "Iteration 16, loss = 0.45083504\n",
      "Iteration 17, loss = 0.44587572\n",
      "Iteration 18, loss = 0.44136086\n",
      "Iteration 19, loss = 0.43728018\n",
      "Iteration 20, loss = 0.43373995\n",
      "Iteration 21, loss = 0.43039373\n",
      "Iteration 22, loss = 0.42755351\n",
      "Iteration 23, loss = 0.42486642\n",
      "Iteration 24, loss = 0.42252297\n",
      "Iteration 25, loss = 0.42019045\n",
      "Iteration 26, loss = 0.41827598\n",
      "Iteration 27, loss = 0.41645715\n",
      "Iteration 28, loss = 0.41478036\n",
      "Iteration 29, loss = 0.41325958\n",
      "Iteration 30, loss = 0.41180866\n",
      "Iteration 31, loss = 0.41073431\n",
      "Iteration 32, loss = 0.40939104\n",
      "Iteration 33, loss = 0.40831178\n",
      "Iteration 34, loss = 0.40732349\n",
      "Iteration 35, loss = 0.40645662\n",
      "Iteration 36, loss = 0.40562145\n",
      "Iteration 37, loss = 0.40510223\n",
      "Iteration 38, loss = 0.40409920\n",
      "Iteration 39, loss = 0.40366960\n",
      "Iteration 40, loss = 0.40303133\n",
      "Iteration 41, loss = 0.40235032\n",
      "Iteration 42, loss = 0.40178923\n",
      "Iteration 43, loss = 0.40121581\n",
      "Iteration 44, loss = 0.40084987\n",
      "Iteration 45, loss = 0.40060551\n",
      "Iteration 46, loss = 0.40000569\n",
      "Iteration 47, loss = 0.39964032\n",
      "Iteration 48, loss = 0.39927547\n",
      "Iteration 49, loss = 0.39899084\n",
      "Iteration 50, loss = 0.39864126\n",
      "Iteration 51, loss = 0.39839355\n",
      "Iteration 52, loss = 0.39829484\n",
      "Iteration 53, loss = 0.39791683\n",
      "Iteration 54, loss = 0.39758068\n",
      "Iteration 55, loss = 0.39742602\n",
      "Iteration 56, loss = 0.39718465\n",
      "Iteration 57, loss = 0.39697820\n",
      "Iteration 58, loss = 0.39690362\n",
      "Iteration 59, loss = 0.39659051\n",
      "Iteration 60, loss = 0.39647923\n",
      "Iteration 61, loss = 0.39641210\n",
      "Iteration 62, loss = 0.39622671\n",
      "Iteration 63, loss = 0.39607690\n",
      "Iteration 64, loss = 0.39594985\n",
      "Iteration 65, loss = 0.39572059\n",
      "Iteration 66, loss = 0.39565845\n",
      "Iteration 67, loss = 0.39557552\n",
      "Iteration 68, loss = 0.39545524\n",
      "Iteration 69, loss = 0.39535731\n",
      "Iteration 70, loss = 0.39520980\n",
      "Iteration 71, loss = 0.39527767\n",
      "Iteration 72, loss = 0.39509732\n",
      "Iteration 73, loss = 0.39488286\n",
      "Iteration 74, loss = 0.39485619\n",
      "Iteration 75, loss = 0.39485989\n",
      "Iteration 76, loss = 0.39472211\n",
      "Iteration 77, loss = 0.39464351\n",
      "Iteration 78, loss = 0.39463017\n",
      "Iteration 79, loss = 0.39466736\n",
      "Iteration 80, loss = 0.39443944\n",
      "Iteration 81, loss = 0.39436435\n",
      "Iteration 82, loss = 0.39453444\n",
      "Iteration 83, loss = 0.39439529\n",
      "Iteration 84, loss = 0.39415814\n",
      "Iteration 85, loss = 0.39415935\n",
      "Iteration 86, loss = 0.39415090\n",
      "Iteration 87, loss = 0.39404338\n",
      "Iteration 88, loss = 0.39395586\n",
      "Iteration 89, loss = 0.39411265\n",
      "Iteration 90, loss = 0.39391704\n",
      "Iteration 91, loss = 0.39387511\n",
      "Iteration 92, loss = 0.39394940\n",
      "Iteration 93, loss = 0.39386962\n",
      "Iteration 94, loss = 0.39367868\n",
      "Iteration 95, loss = 0.39383694\n",
      "Iteration 96, loss = 0.39378562\n",
      "Iteration 97, loss = 0.39356794\n",
      "Iteration 98, loss = 0.39370987\n",
      "Iteration 99, loss = 0.39357397\n",
      "Iteration 100, loss = 0.39353293\n",
      "Iteration 101, loss = 0.39343560\n",
      "Iteration 102, loss = 0.39354556\n",
      "Iteration 103, loss = 0.39329954\n",
      "Iteration 104, loss = 0.39343984\n",
      "Iteration 105, loss = 0.39334460\n",
      "Iteration 106, loss = 0.39344437\n",
      "Iteration 107, loss = 0.39324978\n",
      "Iteration 108, loss = 0.39329903\n",
      "Iteration 109, loss = 0.39330371\n",
      "Iteration 110, loss = 0.39325996\n",
      "Iteration 111, loss = 0.39322771\n",
      "Iteration 112, loss = 0.39316355\n",
      "Iteration 113, loss = 0.39323610\n",
      "Iteration 114, loss = 0.39307995\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.68374381\n",
      "Iteration 2, loss = 0.67236952\n",
      "Iteration 3, loss = 0.65989110\n",
      "Iteration 4, loss = 0.64246563\n",
      "Iteration 5, loss = 0.61880374\n",
      "Iteration 6, loss = 0.58876208\n",
      "Iteration 7, loss = 0.55400633\n",
      "Iteration 8, loss = 0.51826098\n",
      "Iteration 9, loss = 0.48662117\n",
      "Iteration 10, loss = 0.46162926\n",
      "Iteration 11, loss = 0.44252299\n",
      "Iteration 12, loss = 0.42809610\n",
      "Iteration 13, loss = 0.41746993\n",
      "Iteration 14, loss = 0.40925054\n",
      "Iteration 15, loss = 0.40336820\n",
      "Iteration 16, loss = 0.39878812\n",
      "Iteration 17, loss = 0.39526027\n",
      "Iteration 18, loss = 0.39267797\n",
      "Iteration 19, loss = 0.39058737\n",
      "Iteration 20, loss = 0.38878664\n",
      "Iteration 21, loss = 0.38732822\n",
      "Iteration 22, loss = 0.38619561\n",
      "Iteration 23, loss = 0.38545470\n",
      "Iteration 24, loss = 0.38480571\n",
      "Iteration 25, loss = 0.38393558\n",
      "Iteration 26, loss = 0.38372589\n",
      "Iteration 27, loss = 0.38322902\n",
      "Iteration 28, loss = 0.38272749\n",
      "Iteration 29, loss = 0.38244080\n",
      "Iteration 30, loss = 0.38190819\n",
      "Iteration 31, loss = 0.38171714\n",
      "Iteration 32, loss = 0.38166845\n",
      "Iteration 33, loss = 0.38144928\n",
      "Iteration 34, loss = 0.38103070\n",
      "Iteration 35, loss = 0.38093734\n",
      "Iteration 36, loss = 0.38071752\n",
      "Iteration 37, loss = 0.38050045\n",
      "Iteration 38, loss = 0.38032630\n",
      "Iteration 39, loss = 0.38009105\n",
      "Iteration 40, loss = 0.38007801\n",
      "Iteration 41, loss = 0.37994554\n",
      "Iteration 42, loss = 0.37977925\n",
      "Iteration 43, loss = 0.37955644\n",
      "Iteration 44, loss = 0.37955053\n",
      "Iteration 45, loss = 0.37944555\n",
      "Iteration 46, loss = 0.37926430\n",
      "Iteration 47, loss = 0.37929538\n",
      "Iteration 48, loss = 0.37902361\n",
      "Iteration 49, loss = 0.37904068\n",
      "Iteration 50, loss = 0.37902792\n",
      "Iteration 51, loss = 0.37867908\n",
      "Iteration 52, loss = 0.37908250\n",
      "Iteration 53, loss = 0.37884023\n",
      "Iteration 54, loss = 0.37851414\n",
      "Iteration 55, loss = 0.37827947\n",
      "Iteration 56, loss = 0.37847571\n",
      "Iteration 57, loss = 0.37839231\n",
      "Iteration 58, loss = 0.37823981\n",
      "Iteration 59, loss = 0.37819645\n",
      "Iteration 60, loss = 0.37809086\n",
      "Iteration 61, loss = 0.37801441\n",
      "Iteration 62, loss = 0.37803602\n",
      "Iteration 63, loss = 0.37788856\n",
      "Iteration 64, loss = 0.37794227\n",
      "Iteration 65, loss = 0.37777477\n",
      "Iteration 66, loss = 0.37779153\n",
      "Iteration 67, loss = 0.37765522\n",
      "Iteration 68, loss = 0.37779337\n",
      "Iteration 69, loss = 0.37749499\n",
      "Iteration 70, loss = 0.37759149\n",
      "Iteration 71, loss = 0.37772449\n",
      "Iteration 72, loss = 0.37769623\n",
      "Iteration 73, loss = 0.37748101\n",
      "Iteration 74, loss = 0.37732428\n",
      "Iteration 75, loss = 0.37733919\n",
      "Iteration 76, loss = 0.37716841\n",
      "Iteration 77, loss = 0.37737155\n",
      "Iteration 78, loss = 0.37725005\n",
      "Iteration 79, loss = 0.37722260\n",
      "Iteration 80, loss = 0.37736609\n",
      "Iteration 81, loss = 0.37691965\n",
      "Iteration 82, loss = 0.37700897\n",
      "Iteration 83, loss = 0.37704554\n",
      "Iteration 84, loss = 0.37703656\n",
      "Iteration 85, loss = 0.37731476\n",
      "Iteration 86, loss = 0.37702631\n",
      "Iteration 87, loss = 0.37711008\n",
      "Iteration 88, loss = 0.37704217\n",
      "Iteration 89, loss = 0.37686284\n",
      "Iteration 90, loss = 0.37712818\n",
      "Iteration 91, loss = 0.37714649\n",
      "Iteration 92, loss = 0.37697635\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.68354580\n",
      "Iteration 2, loss = 0.67281980\n",
      "Iteration 3, loss = 0.66058127\n",
      "Iteration 4, loss = 0.64358787\n",
      "Iteration 5, loss = 0.62063214\n",
      "Iteration 6, loss = 0.59146522\n",
      "Iteration 7, loss = 0.55773126\n",
      "Iteration 8, loss = 0.52309629\n",
      "Iteration 9, loss = 0.49244666\n",
      "Iteration 10, loss = 0.46814727\n",
      "Iteration 11, loss = 0.44951933\n",
      "Iteration 12, loss = 0.43542064\n",
      "Iteration 13, loss = 0.42507978\n",
      "Iteration 14, loss = 0.41714153\n",
      "Iteration 15, loss = 0.41128369\n",
      "Iteration 16, loss = 0.40676143\n",
      "Iteration 17, loss = 0.40331225\n",
      "Iteration 18, loss = 0.40087087\n",
      "Iteration 19, loss = 0.39854577\n",
      "Iteration 20, loss = 0.39718217\n",
      "Iteration 21, loss = 0.39581266\n",
      "Iteration 22, loss = 0.39450512\n",
      "Iteration 23, loss = 0.39380028\n",
      "Iteration 24, loss = 0.39323588\n",
      "Iteration 25, loss = 0.39241967\n",
      "Iteration 26, loss = 0.39218375\n",
      "Iteration 27, loss = 0.39143821\n",
      "Iteration 28, loss = 0.39091278\n",
      "Iteration 29, loss = 0.39076486\n",
      "Iteration 30, loss = 0.39034876\n",
      "Iteration 31, loss = 0.39012461\n",
      "Iteration 32, loss = 0.38988552\n",
      "Iteration 33, loss = 0.38967256\n",
      "Iteration 34, loss = 0.38932323\n",
      "Iteration 35, loss = 0.38925842\n",
      "Iteration 36, loss = 0.38895332\n",
      "Iteration 37, loss = 0.38895807\n",
      "Iteration 38, loss = 0.38868209\n",
      "Iteration 39, loss = 0.38843012\n",
      "Iteration 40, loss = 0.38826422\n",
      "Iteration 41, loss = 0.38829873\n",
      "Iteration 42, loss = 0.38830895\n",
      "Iteration 43, loss = 0.38825884\n",
      "Iteration 44, loss = 0.38776320\n",
      "Iteration 45, loss = 0.38757954\n",
      "Iteration 46, loss = 0.38745151\n",
      "Iteration 47, loss = 0.38763614\n",
      "Iteration 48, loss = 0.38717769\n",
      "Iteration 49, loss = 0.38729790\n",
      "Iteration 50, loss = 0.38717709\n",
      "Iteration 51, loss = 0.38707203\n",
      "Iteration 52, loss = 0.38686414\n",
      "Iteration 53, loss = 0.38666910\n",
      "Iteration 54, loss = 0.38666491\n",
      "Iteration 55, loss = 0.38668947\n",
      "Iteration 56, loss = 0.38652018\n",
      "Iteration 57, loss = 0.38629644\n",
      "Iteration 58, loss = 0.38668820\n",
      "Iteration 59, loss = 0.38635329\n",
      "Iteration 60, loss = 0.38614726\n",
      "Iteration 61, loss = 0.38608181\n",
      "Iteration 62, loss = 0.38618702\n",
      "Iteration 63, loss = 0.38595532\n",
      "Iteration 64, loss = 0.38592994\n",
      "Iteration 65, loss = 0.38593443\n",
      "Iteration 66, loss = 0.38606131\n",
      "Iteration 67, loss = 0.38590337\n",
      "Iteration 68, loss = 0.38594641\n",
      "Iteration 69, loss = 0.38566693\n",
      "Iteration 70, loss = 0.38556398\n",
      "Iteration 71, loss = 0.38592003\n",
      "Iteration 72, loss = 0.38552814\n",
      "Iteration 73, loss = 0.38552168\n",
      "Iteration 74, loss = 0.38612726\n",
      "Iteration 75, loss = 0.38566300\n",
      "Iteration 76, loss = 0.38546945\n",
      "Iteration 77, loss = 0.38567983\n",
      "Iteration 78, loss = 0.38554056\n",
      "Iteration 79, loss = 0.38536730\n",
      "Iteration 80, loss = 0.38562240\n",
      "Iteration 81, loss = 0.38518505\n",
      "Iteration 82, loss = 0.38521573\n",
      "Iteration 83, loss = 0.38526516\n",
      "Iteration 84, loss = 0.38556825\n",
      "Iteration 85, loss = 0.38544923\n",
      "Iteration 86, loss = 0.38532401\n",
      "Iteration 87, loss = 0.38523443\n",
      "Iteration 88, loss = 0.38506182\n",
      "Iteration 89, loss = 0.38491359\n",
      "Iteration 90, loss = 0.38548707\n",
      "Iteration 91, loss = 0.38519648\n",
      "Iteration 92, loss = 0.38497163\n",
      "Iteration 93, loss = 0.38511933\n",
      "Iteration 94, loss = 0.38482684\n",
      "Iteration 95, loss = 0.38547790\n",
      "Iteration 96, loss = 0.38481890\n",
      "Iteration 97, loss = 0.38514634\n",
      "Iteration 98, loss = 0.38494925\n",
      "Iteration 99, loss = 0.38520950\n",
      "Iteration 100, loss = 0.38511466\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.68344007\n",
      "Iteration 2, loss = 0.67320693\n",
      "Iteration 3, loss = 0.66138743\n",
      "Iteration 4, loss = 0.64495215\n",
      "Iteration 5, loss = 0.62293879\n",
      "Iteration 6, loss = 0.59507864\n",
      "Iteration 7, loss = 0.56237398\n",
      "Iteration 8, loss = 0.52906899\n",
      "Iteration 9, loss = 0.49941304\n",
      "Iteration 10, loss = 0.47529641\n",
      "Iteration 11, loss = 0.45708544\n",
      "Iteration 12, loss = 0.44312397\n",
      "Iteration 13, loss = 0.43297971\n",
      "Iteration 14, loss = 0.42527880\n",
      "Iteration 15, loss = 0.41966788\n",
      "Iteration 16, loss = 0.41531686\n",
      "Iteration 17, loss = 0.41196992\n",
      "Iteration 18, loss = 0.40964297\n",
      "Iteration 19, loss = 0.40735364\n",
      "Iteration 20, loss = 0.40592614\n",
      "Iteration 21, loss = 0.40459378\n",
      "Iteration 22, loss = 0.40372429\n",
      "Iteration 23, loss = 0.40260311\n",
      "Iteration 24, loss = 0.40228698\n",
      "Iteration 25, loss = 0.40160355\n",
      "Iteration 26, loss = 0.40078383\n",
      "Iteration 27, loss = 0.40028438\n",
      "Iteration 28, loss = 0.39967191\n",
      "Iteration 29, loss = 0.39936938\n",
      "Iteration 30, loss = 0.39922002\n",
      "Iteration 31, loss = 0.39873112\n",
      "Iteration 32, loss = 0.39848512\n",
      "Iteration 33, loss = 0.39814491\n",
      "Iteration 34, loss = 0.39785156\n",
      "Iteration 35, loss = 0.39742673\n",
      "Iteration 36, loss = 0.39774244\n",
      "Iteration 37, loss = 0.39718893\n",
      "Iteration 38, loss = 0.39706790\n",
      "Iteration 39, loss = 0.39683251\n",
      "Iteration 40, loss = 0.39677715\n",
      "Iteration 41, loss = 0.39639761\n",
      "Iteration 42, loss = 0.39642276\n",
      "Iteration 43, loss = 0.39627177\n",
      "Iteration 44, loss = 0.39590569\n",
      "Iteration 45, loss = 0.39614760\n",
      "Iteration 46, loss = 0.39584750\n",
      "Iteration 47, loss = 0.39554175\n",
      "Iteration 48, loss = 0.39550262\n",
      "Iteration 49, loss = 0.39540376\n",
      "Iteration 50, loss = 0.39541830\n",
      "Iteration 51, loss = 0.39558693\n",
      "Iteration 52, loss = 0.39497998\n",
      "Iteration 53, loss = 0.39494111\n",
      "Iteration 54, loss = 0.39497410\n",
      "Iteration 55, loss = 0.39469602\n",
      "Iteration 56, loss = 0.39466645\n",
      "Iteration 57, loss = 0.39465811\n",
      "Iteration 58, loss = 0.39438798\n",
      "Iteration 59, loss = 0.39455395\n",
      "Iteration 60, loss = 0.39472426\n",
      "Iteration 61, loss = 0.39453550\n",
      "Iteration 62, loss = 0.39437636\n",
      "Iteration 63, loss = 0.39425851\n",
      "Iteration 64, loss = 0.39407157\n",
      "Iteration 65, loss = 0.39406519\n",
      "Iteration 66, loss = 0.39408830\n",
      "Iteration 67, loss = 0.39416231\n",
      "Iteration 68, loss = 0.39403325\n",
      "Iteration 69, loss = 0.39396260\n",
      "Iteration 70, loss = 0.39384635\n",
      "Iteration 71, loss = 0.39389780\n",
      "Iteration 72, loss = 0.39381655\n",
      "Iteration 73, loss = 0.39392258\n",
      "Iteration 74, loss = 0.39367648\n",
      "Iteration 75, loss = 0.39391112\n",
      "Iteration 76, loss = 0.39363345\n",
      "Iteration 77, loss = 0.39357973\n",
      "Iteration 78, loss = 0.39348836\n",
      "Iteration 79, loss = 0.39340041\n",
      "Iteration 80, loss = 0.39342454\n",
      "Iteration 81, loss = 0.39356136\n",
      "Iteration 82, loss = 0.39351745\n",
      "Iteration 83, loss = 0.39335281\n",
      "Iteration 84, loss = 0.39344801\n",
      "Iteration 85, loss = 0.39390790\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.67461439\n",
      "Iteration 2, loss = 0.62299706\n",
      "Iteration 3, loss = 0.54010865\n",
      "Iteration 4, loss = 0.46876868\n",
      "Iteration 5, loss = 0.42919560\n",
      "Iteration 6, loss = 0.40823254\n",
      "Iteration 7, loss = 0.39766925\n",
      "Iteration 8, loss = 0.39118938\n",
      "Iteration 9, loss = 0.38780537\n",
      "Iteration 10, loss = 0.38578576\n",
      "Iteration 11, loss = 0.38393994\n",
      "Iteration 12, loss = 0.38303565\n",
      "Iteration 13, loss = 0.38259097\n",
      "Iteration 14, loss = 0.38172697\n",
      "Iteration 15, loss = 0.38143696\n",
      "Iteration 16, loss = 0.38086551\n",
      "Iteration 17, loss = 0.38041637\n",
      "Iteration 18, loss = 0.38014614\n",
      "Iteration 19, loss = 0.38003095\n",
      "Iteration 20, loss = 0.37967293\n",
      "Iteration 21, loss = 0.37918226\n",
      "Iteration 22, loss = 0.37879779\n",
      "Iteration 23, loss = 0.37875771\n",
      "Iteration 24, loss = 0.37899573\n",
      "Iteration 25, loss = 0.37827086\n",
      "Iteration 26, loss = 0.37869744\n",
      "Iteration 27, loss = 0.37856617\n",
      "Iteration 28, loss = 0.37805129\n",
      "Iteration 29, loss = 0.37805152\n",
      "Iteration 30, loss = 0.37768992\n",
      "Iteration 31, loss = 0.37763222\n",
      "Iteration 32, loss = 0.37805250\n",
      "Iteration 33, loss = 0.37779710\n",
      "Iteration 34, loss = 0.37757411\n",
      "Iteration 35, loss = 0.37759462\n",
      "Iteration 36, loss = 0.37737481\n",
      "Iteration 37, loss = 0.37752566\n",
      "Iteration 38, loss = 0.37734744\n",
      "Iteration 39, loss = 0.37702861\n",
      "Iteration 40, loss = 0.37725273\n",
      "Iteration 41, loss = 0.37708553\n",
      "Iteration 42, loss = 0.37719265\n",
      "Iteration 43, loss = 0.37680659\n",
      "Iteration 44, loss = 0.37700177\n",
      "Iteration 45, loss = 0.37699524\n",
      "Iteration 46, loss = 0.37671970\n",
      "Iteration 47, loss = 0.37694770\n",
      "Iteration 48, loss = 0.37669602\n",
      "Iteration 49, loss = 0.37673755\n",
      "Iteration 50, loss = 0.37696026\n",
      "Iteration 51, loss = 0.37661725\n",
      "Iteration 52, loss = 0.37750169\n",
      "Iteration 53, loss = 0.37720488\n",
      "Iteration 54, loss = 0.37662530\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.67363922\n",
      "Iteration 2, loss = 0.62349023\n",
      "Iteration 3, loss = 0.54396019\n",
      "Iteration 4, loss = 0.47465215\n",
      "Iteration 5, loss = 0.43521866\n",
      "Iteration 6, loss = 0.41509323\n",
      "Iteration 7, loss = 0.40477541\n",
      "Iteration 8, loss = 0.39912284\n",
      "Iteration 9, loss = 0.39621708\n",
      "Iteration 10, loss = 0.39386065\n",
      "Iteration 11, loss = 0.39244370\n",
      "Iteration 12, loss = 0.39122414\n",
      "Iteration 13, loss = 0.39103157\n",
      "Iteration 14, loss = 0.39037254\n",
      "Iteration 15, loss = 0.38989716\n",
      "Iteration 16, loss = 0.38902722\n",
      "Iteration 17, loss = 0.38888350\n",
      "Iteration 18, loss = 0.38889508\n",
      "Iteration 19, loss = 0.38808842\n",
      "Iteration 20, loss = 0.38811754\n",
      "Iteration 21, loss = 0.38794159\n",
      "Iteration 22, loss = 0.38720448\n",
      "Iteration 23, loss = 0.38723425\n",
      "Iteration 24, loss = 0.38742815\n",
      "Iteration 25, loss = 0.38684078\n",
      "Iteration 26, loss = 0.38733333\n",
      "Iteration 27, loss = 0.38654705\n",
      "Iteration 28, loss = 0.38615949\n",
      "Iteration 29, loss = 0.38637018\n",
      "Iteration 30, loss = 0.38616141\n",
      "Iteration 31, loss = 0.38627327\n",
      "Iteration 32, loss = 0.38615795\n",
      "Iteration 33, loss = 0.38605902\n",
      "Iteration 34, loss = 0.38565987\n",
      "Iteration 35, loss = 0.38611967\n",
      "Iteration 36, loss = 0.38581514\n",
      "Iteration 37, loss = 0.38591963\n",
      "Iteration 38, loss = 0.38585341\n",
      "Iteration 39, loss = 0.38572309\n",
      "Iteration 40, loss = 0.38550189\n",
      "Iteration 41, loss = 0.38576382\n",
      "Iteration 42, loss = 0.38610062\n",
      "Iteration 43, loss = 0.38636801\n",
      "Iteration 44, loss = 0.38528624\n",
      "Iteration 45, loss = 0.38538708\n",
      "Iteration 46, loss = 0.38518286\n",
      "Iteration 47, loss = 0.38559829\n",
      "Iteration 48, loss = 0.38503678\n",
      "Iteration 49, loss = 0.38537400\n",
      "Iteration 50, loss = 0.38520109\n",
      "Iteration 51, loss = 0.38536507\n",
      "Iteration 52, loss = 0.38504223\n",
      "Iteration 53, loss = 0.38489879\n",
      "Iteration 54, loss = 0.38478236\n",
      "Iteration 55, loss = 0.38509247\n",
      "Iteration 56, loss = 0.38478282\n",
      "Iteration 57, loss = 0.38462675\n",
      "Iteration 58, loss = 0.38534217\n",
      "Iteration 59, loss = 0.38495306\n",
      "Iteration 60, loss = 0.38482722\n",
      "Iteration 61, loss = 0.38448196\n",
      "Iteration 62, loss = 0.38477057\n",
      "Iteration 63, loss = 0.38457041\n",
      "Iteration 64, loss = 0.38455780\n",
      "Iteration 65, loss = 0.38471161\n",
      "Iteration 66, loss = 0.38489050\n",
      "Iteration 67, loss = 0.38473145\n",
      "Iteration 68, loss = 0.38513575\n",
      "Iteration 69, loss = 0.38450693\n",
      "Iteration 70, loss = 0.38441174\n",
      "Iteration 71, loss = 0.38519409\n",
      "Iteration 72, loss = 0.38438540\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.67232281\n",
      "Iteration 2, loss = 0.61819729\n",
      "Iteration 3, loss = 0.53839725\n",
      "Iteration 4, loss = 0.47369115\n",
      "Iteration 5, loss = 0.43838823\n",
      "Iteration 6, loss = 0.42085875\n",
      "Iteration 7, loss = 0.41220789\n",
      "Iteration 8, loss = 0.40777392\n",
      "Iteration 9, loss = 0.40417853\n",
      "Iteration 10, loss = 0.40234681\n",
      "Iteration 11, loss = 0.40175337\n",
      "Iteration 12, loss = 0.40007411\n",
      "Iteration 13, loss = 0.39943361\n",
      "Iteration 14, loss = 0.39857811\n",
      "Iteration 15, loss = 0.39813114\n",
      "Iteration 16, loss = 0.39770840\n",
      "Iteration 17, loss = 0.39730466\n",
      "Iteration 18, loss = 0.39711002\n",
      "Iteration 19, loss = 0.39656562\n",
      "Iteration 20, loss = 0.39659718\n",
      "Iteration 21, loss = 0.39597306\n",
      "Iteration 22, loss = 0.39602142\n",
      "Iteration 23, loss = 0.39547637\n",
      "Iteration 24, loss = 0.39585651\n",
      "Iteration 25, loss = 0.39567290\n",
      "Iteration 26, loss = 0.39491366\n",
      "Iteration 27, loss = 0.39485519\n",
      "Iteration 28, loss = 0.39434146\n",
      "Iteration 29, loss = 0.39436436\n",
      "Iteration 30, loss = 0.39467105\n",
      "Iteration 31, loss = 0.39428324\n",
      "Iteration 32, loss = 0.39423417\n",
      "Iteration 33, loss = 0.39396079\n",
      "Iteration 34, loss = 0.39392001\n",
      "Iteration 35, loss = 0.39345957\n",
      "Iteration 36, loss = 0.39446180\n",
      "Iteration 37, loss = 0.39358194\n",
      "Iteration 38, loss = 0.39360675\n",
      "Iteration 39, loss = 0.39347290\n",
      "Iteration 40, loss = 0.39350621\n",
      "Iteration 41, loss = 0.39323468\n",
      "Iteration 42, loss = 0.39349247\n",
      "Iteration 43, loss = 0.39343574\n",
      "Iteration 44, loss = 0.39308320\n",
      "Iteration 45, loss = 0.39361459\n",
      "Iteration 46, loss = 0.39339154\n",
      "Iteration 47, loss = 0.39303361\n",
      "Iteration 48, loss = 0.39302355\n",
      "Iteration 49, loss = 0.39340403\n",
      "Iteration 50, loss = 0.39317317\n",
      "Iteration 51, loss = 0.39388812\n",
      "Iteration 52, loss = 0.39286261\n",
      "Iteration 53, loss = 0.39293102\n",
      "Iteration 54, loss = 0.39299972\n",
      "Iteration 55, loss = 0.39266790\n",
      "Iteration 56, loss = 0.39279695\n",
      "Iteration 57, loss = 0.39303030\n",
      "Iteration 58, loss = 0.39250631\n",
      "Iteration 59, loss = 0.39299205\n",
      "Iteration 60, loss = 0.39356427\n",
      "Iteration 61, loss = 0.39316712\n",
      "Iteration 62, loss = 0.39295528\n",
      "Iteration 63, loss = 0.39271612\n",
      "Iteration 64, loss = 0.39264035\n",
      "Iteration 65, loss = 0.39263406\n",
      "Iteration 66, loss = 0.39275820\n",
      "Iteration 67, loss = 0.39297749\n",
      "Iteration 68, loss = 0.39284116\n",
      "Iteration 69, loss = 0.39267085\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.82104703\n",
      "Iteration 2, loss = 0.69656198\n",
      "Iteration 3, loss = 0.69328816\n",
      "Iteration 4, loss = 0.69315946\n",
      "Iteration 5, loss = 0.69317330\n",
      "Iteration 6, loss = 0.69318708\n",
      "Iteration 7, loss = 0.69319908\n",
      "Iteration 8, loss = 0.69322023\n",
      "Iteration 9, loss = 0.69319613\n",
      "Iteration 10, loss = 0.69320961\n",
      "Iteration 11, loss = 0.69320207\n",
      "Iteration 12, loss = 0.69320970\n",
      "Iteration 13, loss = 0.69317669\n",
      "Iteration 14, loss = 0.69320877\n",
      "Iteration 15, loss = 0.69317595\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.82323790\n",
      "Iteration 2, loss = 0.69646053\n",
      "Iteration 3, loss = 0.69324531\n",
      "Iteration 4, loss = 0.69319306\n",
      "Iteration 5, loss = 0.69318564\n",
      "Iteration 6, loss = 0.69318254\n",
      "Iteration 7, loss = 0.69319022\n",
      "Iteration 8, loss = 0.69317865\n",
      "Iteration 9, loss = 0.69318443\n",
      "Iteration 10, loss = 0.69319446\n",
      "Iteration 11, loss = 0.69318282\n",
      "Iteration 12, loss = 0.69321193\n",
      "Iteration 13, loss = 0.69317037\n",
      "Iteration 14, loss = 0.69319485\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.82250739\n",
      "Iteration 2, loss = 0.69593177\n",
      "Iteration 3, loss = 0.69324248\n",
      "Iteration 4, loss = 0.69319618\n",
      "Iteration 5, loss = 0.69319260\n",
      "Iteration 6, loss = 0.69317300\n",
      "Iteration 7, loss = 0.69318277\n",
      "Iteration 8, loss = 0.69316931\n",
      "Iteration 9, loss = 0.69320611\n",
      "Iteration 10, loss = 0.69317753\n",
      "Iteration 11, loss = 0.69319858\n",
      "Iteration 12, loss = 0.69319024\n",
      "Iteration 13, loss = 0.69317143\n",
      "Iteration 14, loss = 0.69326976\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.86566698\n",
      "Iteration 2, loss = 0.72810027\n",
      "Iteration 3, loss = 0.69586308\n",
      "Iteration 4, loss = 0.63551304\n",
      "Iteration 5, loss = 0.55470953\n",
      "Iteration 6, loss = 0.49927349\n",
      "Iteration 7, loss = 0.46053691\n",
      "Iteration 8, loss = 0.43458362\n",
      "Iteration 9, loss = 0.41777217\n",
      "Iteration 10, loss = 0.40695684\n",
      "Iteration 11, loss = 0.39963775\n",
      "Iteration 12, loss = 0.39478159\n",
      "Iteration 13, loss = 0.39112761\n",
      "Iteration 14, loss = 0.38880953\n",
      "Iteration 15, loss = 0.38710421\n",
      "Iteration 16, loss = 0.38485546\n",
      "Iteration 17, loss = 0.38477416\n",
      "Iteration 18, loss = 0.38342367\n",
      "Iteration 19, loss = 0.38244656\n",
      "Iteration 20, loss = 0.38227439\n",
      "Iteration 21, loss = 0.38132621\n",
      "Iteration 22, loss = 0.38077940\n",
      "Iteration 23, loss = 0.38087995\n",
      "Iteration 24, loss = 0.38086970\n",
      "Iteration 25, loss = 0.37967770\n",
      "Iteration 26, loss = 0.37930032\n",
      "Iteration 27, loss = 0.37974256\n",
      "Iteration 28, loss = 0.37906165\n",
      "Iteration 29, loss = 0.37889466\n",
      "Iteration 30, loss = 0.37858703\n",
      "Iteration 31, loss = 0.37893112\n",
      "Iteration 32, loss = 0.37781006\n",
      "Iteration 33, loss = 0.37873067\n",
      "Iteration 34, loss = 0.37791199\n",
      "Iteration 35, loss = 0.37817679\n",
      "Iteration 36, loss = 0.37792993\n",
      "Iteration 37, loss = 0.37763711\n",
      "Iteration 38, loss = 0.37865075\n",
      "Iteration 39, loss = 0.37761661\n",
      "Iteration 40, loss = 0.37842448\n",
      "Iteration 41, loss = 0.37731175\n",
      "Iteration 42, loss = 0.37741174\n",
      "Iteration 43, loss = 0.37730481\n",
      "Iteration 44, loss = 0.37749589\n",
      "Iteration 45, loss = 0.37744439\n",
      "Iteration 46, loss = 0.37678697\n",
      "Iteration 47, loss = 0.37834013\n",
      "Iteration 48, loss = 0.37678199\n",
      "Iteration 49, loss = 0.37726001\n",
      "Iteration 50, loss = 0.37677500\n",
      "Iteration 51, loss = 0.37699210\n",
      "Iteration 52, loss = 0.37713645\n",
      "Iteration 53, loss = 0.37665880\n",
      "Iteration 54, loss = 0.37671899\n",
      "Iteration 55, loss = 0.37702978\n",
      "Iteration 56, loss = 0.37701153\n",
      "Iteration 57, loss = 0.37695347\n",
      "Iteration 58, loss = 0.37710723\n",
      "Iteration 59, loss = 0.37614685\n",
      "Iteration 60, loss = 0.37688773\n",
      "Iteration 61, loss = 0.37666128\n",
      "Iteration 62, loss = 0.37630404\n",
      "Iteration 63, loss = 0.37681186\n",
      "Iteration 64, loss = 0.37679653\n",
      "Iteration 65, loss = 0.37710501\n",
      "Iteration 66, loss = 0.37688034\n",
      "Iteration 67, loss = 0.37696996\n",
      "Iteration 68, loss = 0.37653770\n",
      "Iteration 69, loss = 0.37649594\n",
      "Iteration 70, loss = 0.37641519\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.87102734\n",
      "Iteration 2, loss = 0.72877833\n",
      "Iteration 3, loss = 0.69751719\n",
      "Iteration 4, loss = 0.64232914\n",
      "Iteration 5, loss = 0.56467124\n",
      "Iteration 6, loss = 0.50931077\n",
      "Iteration 7, loss = 0.47051270\n",
      "Iteration 8, loss = 0.44422976\n",
      "Iteration 9, loss = 0.42721677\n",
      "Iteration 10, loss = 0.41558072\n",
      "Iteration 11, loss = 0.40787510\n",
      "Iteration 12, loss = 0.40258367\n",
      "Iteration 13, loss = 0.39953840\n",
      "Iteration 14, loss = 0.39711401\n",
      "Iteration 15, loss = 0.39477153\n",
      "Iteration 16, loss = 0.39301143\n",
      "Iteration 17, loss = 0.39202813\n",
      "Iteration 18, loss = 0.39118405\n",
      "Iteration 19, loss = 0.39034780\n",
      "Iteration 20, loss = 0.38984015\n",
      "Iteration 21, loss = 0.38901419\n",
      "Iteration 22, loss = 0.38875943\n",
      "Iteration 23, loss = 0.38856211\n",
      "Iteration 24, loss = 0.38849718\n",
      "Iteration 25, loss = 0.38786083\n",
      "Iteration 26, loss = 0.38724872\n",
      "Iteration 27, loss = 0.38770031\n",
      "Iteration 28, loss = 0.38702223\n",
      "Iteration 29, loss = 0.38677255\n",
      "Iteration 30, loss = 0.38680552\n",
      "Iteration 31, loss = 0.38653008\n",
      "Iteration 32, loss = 0.38704762\n",
      "Iteration 33, loss = 0.38713465\n",
      "Iteration 34, loss = 0.38686027\n",
      "Iteration 35, loss = 0.38604495\n",
      "Iteration 36, loss = 0.38737484\n",
      "Iteration 37, loss = 0.38585066\n",
      "Iteration 38, loss = 0.38574680\n",
      "Iteration 39, loss = 0.38578978\n",
      "Iteration 40, loss = 0.38666171\n",
      "Iteration 41, loss = 0.38607168\n",
      "Iteration 42, loss = 0.38628982\n",
      "Iteration 43, loss = 0.38493943\n",
      "Iteration 44, loss = 0.38555346\n",
      "Iteration 45, loss = 0.38527218\n",
      "Iteration 46, loss = 0.38538930\n",
      "Iteration 47, loss = 0.38540709\n",
      "Iteration 48, loss = 0.38478551\n",
      "Iteration 49, loss = 0.38497634\n",
      "Iteration 50, loss = 0.38500482\n",
      "Iteration 51, loss = 0.38501972\n",
      "Iteration 52, loss = 0.38529067\n",
      "Iteration 53, loss = 0.38462806\n",
      "Iteration 54, loss = 0.38489378\n",
      "Iteration 55, loss = 0.38537109\n",
      "Iteration 56, loss = 0.38485156\n",
      "Iteration 57, loss = 0.38512564\n",
      "Iteration 58, loss = 0.38638270\n",
      "Iteration 59, loss = 0.38520112\n",
      "Iteration 60, loss = 0.38463050\n",
      "Iteration 61, loss = 0.38456999\n",
      "Iteration 62, loss = 0.38527297\n",
      "Iteration 63, loss = 0.38504821\n",
      "Iteration 64, loss = 0.38516469\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.86991222\n",
      "Iteration 2, loss = 0.72659104\n",
      "Iteration 3, loss = 0.69463371\n",
      "Iteration 4, loss = 0.63760087\n",
      "Iteration 5, loss = 0.56390129\n",
      "Iteration 6, loss = 0.51248036\n",
      "Iteration 7, loss = 0.47583841\n",
      "Iteration 8, loss = 0.45077878\n",
      "Iteration 9, loss = 0.43420015\n",
      "Iteration 10, loss = 0.42342238\n",
      "Iteration 11, loss = 0.41600467\n",
      "Iteration 12, loss = 0.41092263\n",
      "Iteration 13, loss = 0.40754563\n",
      "Iteration 14, loss = 0.40483432\n",
      "Iteration 15, loss = 0.40331711\n",
      "Iteration 16, loss = 0.40241073\n",
      "Iteration 17, loss = 0.40074436\n",
      "Iteration 18, loss = 0.39943418\n",
      "Iteration 19, loss = 0.39846965\n",
      "Iteration 20, loss = 0.39811884\n",
      "Iteration 21, loss = 0.39749367\n",
      "Iteration 22, loss = 0.39654103\n",
      "Iteration 23, loss = 0.39627419\n",
      "Iteration 24, loss = 0.39607390\n",
      "Iteration 25, loss = 0.39581243\n",
      "Iteration 26, loss = 0.39515195\n",
      "Iteration 27, loss = 0.39543532\n",
      "Iteration 28, loss = 0.39542619\n",
      "Iteration 29, loss = 0.39476691\n",
      "Iteration 30, loss = 0.39459782\n",
      "Iteration 31, loss = 0.39499936\n",
      "Iteration 32, loss = 0.39513574\n",
      "Iteration 33, loss = 0.39378029\n",
      "Iteration 34, loss = 0.39397221\n",
      "Iteration 35, loss = 0.39374655\n",
      "Iteration 36, loss = 0.39359220\n",
      "Iteration 37, loss = 0.39368293\n",
      "Iteration 38, loss = 0.39473563\n",
      "Iteration 39, loss = 0.39356048\n",
      "Iteration 40, loss = 0.39371867\n",
      "Iteration 41, loss = 0.39337996\n",
      "Iteration 42, loss = 0.39317694\n",
      "Iteration 43, loss = 0.39299620\n",
      "Iteration 44, loss = 0.39315240\n",
      "Iteration 45, loss = 0.39291811\n",
      "Iteration 46, loss = 0.39285079\n",
      "Iteration 47, loss = 0.39294019\n",
      "Iteration 48, loss = 0.39288562\n",
      "Iteration 49, loss = 0.39365440\n",
      "Iteration 50, loss = 0.39409208\n",
      "Iteration 51, loss = 0.39349639\n",
      "Iteration 52, loss = 0.39299230\n",
      "Iteration 53, loss = 0.39272363\n",
      "Iteration 54, loss = 0.39264205\n",
      "Iteration 55, loss = 0.39371046\n",
      "Iteration 56, loss = 0.39265446\n",
      "Iteration 57, loss = 0.39328210\n",
      "Iteration 58, loss = 0.39310355\n",
      "Iteration 59, loss = 0.39294360\n",
      "Iteration 60, loss = 0.39254268\n",
      "Iteration 61, loss = 0.39317718\n",
      "Iteration 62, loss = 0.39302580\n",
      "Iteration 63, loss = 0.39298281\n",
      "Iteration 64, loss = 0.39335887\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.71068371\n",
      "Iteration 2, loss = 0.68940040\n",
      "Iteration 3, loss = 0.68271640\n",
      "Iteration 4, loss = 0.67452311\n",
      "Iteration 5, loss = 0.66265601\n",
      "Iteration 6, loss = 0.64554221\n",
      "Iteration 7, loss = 0.62208545\n",
      "Iteration 8, loss = 0.59101764\n",
      "Iteration 9, loss = 0.55473012\n",
      "Iteration 10, loss = 0.51649475\n",
      "Iteration 11, loss = 0.48142215\n",
      "Iteration 12, loss = 0.45271643\n",
      "Iteration 13, loss = 0.43164505\n",
      "Iteration 14, loss = 0.41617488\n",
      "Iteration 15, loss = 0.40561576\n",
      "Iteration 16, loss = 0.39847253\n",
      "Iteration 17, loss = 0.39347462\n",
      "Iteration 18, loss = 0.39021034\n",
      "Iteration 19, loss = 0.38744279\n",
      "Iteration 20, loss = 0.38652012\n",
      "Iteration 21, loss = 0.38466474\n",
      "Iteration 22, loss = 0.38343081\n",
      "Iteration 23, loss = 0.38302269\n",
      "Iteration 24, loss = 0.38238996\n",
      "Iteration 25, loss = 0.38205323\n",
      "Iteration 26, loss = 0.38128312\n",
      "Iteration 27, loss = 0.38118455\n",
      "Iteration 28, loss = 0.38066796\n",
      "Iteration 29, loss = 0.38071716\n",
      "Iteration 30, loss = 0.38064387\n",
      "Iteration 31, loss = 0.38004697\n",
      "Iteration 32, loss = 0.38006965\n",
      "Iteration 33, loss = 0.37965637\n",
      "Iteration 34, loss = 0.37956688\n",
      "Iteration 35, loss = 0.37961772\n",
      "Iteration 36, loss = 0.37929160\n",
      "Iteration 37, loss = 0.37934319\n",
      "Iteration 38, loss = 0.37894584\n",
      "Iteration 39, loss = 0.37873330\n",
      "Iteration 40, loss = 0.37863346\n",
      "Iteration 41, loss = 0.37892520\n",
      "Iteration 42, loss = 0.37843570\n",
      "Iteration 43, loss = 0.37827094\n",
      "Iteration 44, loss = 0.37796578\n",
      "Iteration 45, loss = 0.37807372\n",
      "Iteration 46, loss = 0.37800428\n",
      "Iteration 47, loss = 0.37820638\n",
      "Iteration 48, loss = 0.37788533\n",
      "Iteration 49, loss = 0.37803488\n",
      "Iteration 50, loss = 0.37781136\n",
      "Iteration 51, loss = 0.37758369\n",
      "Iteration 52, loss = 0.37725826\n",
      "Iteration 53, loss = 0.37751931\n",
      "Iteration 54, loss = 0.37736597\n",
      "Iteration 55, loss = 0.37721445\n",
      "Iteration 56, loss = 0.37721425\n",
      "Iteration 57, loss = 0.37707922\n",
      "Iteration 58, loss = 0.37703037\n",
      "Iteration 59, loss = 0.37735312\n",
      "Iteration 60, loss = 0.37705838\n",
      "Iteration 61, loss = 0.37691198\n",
      "Iteration 62, loss = 0.37684171\n",
      "Iteration 63, loss = 0.37660668\n",
      "Iteration 64, loss = 0.37677836\n",
      "Iteration 65, loss = 0.37683351\n",
      "Iteration 66, loss = 0.37673897\n",
      "Iteration 67, loss = 0.37644272\n",
      "Iteration 68, loss = 0.37685144\n",
      "Iteration 69, loss = 0.37668359\n",
      "Iteration 70, loss = 0.37663127\n",
      "Iteration 71, loss = 0.37698018\n",
      "Iteration 72, loss = 0.37626666\n",
      "Iteration 73, loss = 0.37643375\n",
      "Iteration 74, loss = 0.37656935\n",
      "Iteration 75, loss = 0.37649598\n",
      "Iteration 76, loss = 0.37623485\n",
      "Iteration 77, loss = 0.37623360\n",
      "Iteration 78, loss = 0.37638904\n",
      "Iteration 79, loss = 0.37626990\n",
      "Iteration 80, loss = 0.37650760\n",
      "Iteration 81, loss = 0.37628545\n",
      "Iteration 82, loss = 0.37623063\n",
      "Iteration 83, loss = 0.37621383\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.71251752\n",
      "Iteration 2, loss = 0.68897660\n",
      "Iteration 3, loss = 0.68230215\n",
      "Iteration 4, loss = 0.67376111\n",
      "Iteration 5, loss = 0.66197291\n",
      "Iteration 6, loss = 0.64465036\n",
      "Iteration 7, loss = 0.62091427\n",
      "Iteration 8, loss = 0.59012741\n",
      "Iteration 9, loss = 0.55459002\n",
      "Iteration 10, loss = 0.51756195\n",
      "Iteration 11, loss = 0.48423197\n",
      "Iteration 12, loss = 0.45710391\n",
      "Iteration 13, loss = 0.43710804\n",
      "Iteration 14, loss = 0.42316923\n",
      "Iteration 15, loss = 0.41317765\n",
      "Iteration 16, loss = 0.40625094\n",
      "Iteration 17, loss = 0.40159492\n",
      "Iteration 18, loss = 0.39842026\n",
      "Iteration 19, loss = 0.39636671\n",
      "Iteration 20, loss = 0.39471469\n",
      "Iteration 21, loss = 0.39356047\n",
      "Iteration 22, loss = 0.39238815\n",
      "Iteration 23, loss = 0.39164823\n",
      "Iteration 24, loss = 0.39128785\n",
      "Iteration 25, loss = 0.39071440\n",
      "Iteration 26, loss = 0.39068657\n",
      "Iteration 27, loss = 0.38955423\n",
      "Iteration 28, loss = 0.38950856\n",
      "Iteration 29, loss = 0.38957034\n",
      "Iteration 30, loss = 0.38971035\n",
      "Iteration 31, loss = 0.38899523\n",
      "Iteration 32, loss = 0.38869993\n",
      "Iteration 33, loss = 0.38866203\n",
      "Iteration 34, loss = 0.38825744\n",
      "Iteration 35, loss = 0.38791673\n",
      "Iteration 36, loss = 0.38799913\n",
      "Iteration 37, loss = 0.38793653\n",
      "Iteration 38, loss = 0.38774001\n",
      "Iteration 39, loss = 0.38797449\n",
      "Iteration 40, loss = 0.38730522\n",
      "Iteration 41, loss = 0.38776597\n",
      "Iteration 42, loss = 0.38716479\n",
      "Iteration 43, loss = 0.38686345\n",
      "Iteration 44, loss = 0.38685008\n",
      "Iteration 45, loss = 0.38676655\n",
      "Iteration 46, loss = 0.38649379\n",
      "Iteration 47, loss = 0.38673721\n",
      "Iteration 48, loss = 0.38644350\n",
      "Iteration 49, loss = 0.38631690\n",
      "Iteration 50, loss = 0.38625525\n",
      "Iteration 51, loss = 0.38590653\n",
      "Iteration 52, loss = 0.38619848\n",
      "Iteration 53, loss = 0.38580483\n",
      "Iteration 54, loss = 0.38600496\n",
      "Iteration 55, loss = 0.38557532\n",
      "Iteration 56, loss = 0.38563646\n",
      "Iteration 57, loss = 0.38577947\n",
      "Iteration 58, loss = 0.38543481\n",
      "Iteration 59, loss = 0.38547554\n",
      "Iteration 60, loss = 0.38580141\n",
      "Iteration 61, loss = 0.38533883\n",
      "Iteration 62, loss = 0.38534543\n",
      "Iteration 63, loss = 0.38504621\n",
      "Iteration 64, loss = 0.38530668\n",
      "Iteration 65, loss = 0.38521300\n",
      "Iteration 66, loss = 0.38551090\n",
      "Iteration 67, loss = 0.38510426\n",
      "Iteration 68, loss = 0.38498918\n",
      "Iteration 69, loss = 0.38514961\n",
      "Iteration 70, loss = 0.38501803\n",
      "Iteration 71, loss = 0.38521991\n",
      "Iteration 72, loss = 0.38466465\n",
      "Iteration 73, loss = 0.38477458\n",
      "Iteration 74, loss = 0.38485467\n",
      "Iteration 75, loss = 0.38457044\n",
      "Iteration 76, loss = 0.38539476\n",
      "Iteration 77, loss = 0.38542349\n",
      "Iteration 78, loss = 0.38501697\n",
      "Iteration 79, loss = 0.38476665\n",
      "Iteration 80, loss = 0.38496668\n",
      "Iteration 81, loss = 0.38462043\n",
      "Iteration 82, loss = 0.38454863\n",
      "Iteration 83, loss = 0.38459756\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.71406338\n",
      "Iteration 2, loss = 0.68897698\n",
      "Iteration 3, loss = 0.68241845\n",
      "Iteration 4, loss = 0.67411915\n",
      "Iteration 5, loss = 0.66242704\n",
      "Iteration 6, loss = 0.64562619\n",
      "Iteration 7, loss = 0.62250318\n",
      "Iteration 8, loss = 0.59289195\n",
      "Iteration 9, loss = 0.55842380\n",
      "Iteration 10, loss = 0.52307210\n",
      "Iteration 11, loss = 0.49053744\n",
      "Iteration 12, loss = 0.46481816\n",
      "Iteration 13, loss = 0.44553540\n",
      "Iteration 14, loss = 0.43154018\n",
      "Iteration 15, loss = 0.42196916\n",
      "Iteration 16, loss = 0.41528444\n",
      "Iteration 17, loss = 0.41104712\n",
      "Iteration 18, loss = 0.40772321\n",
      "Iteration 19, loss = 0.40616144\n",
      "Iteration 20, loss = 0.40400014\n",
      "Iteration 21, loss = 0.40278383\n",
      "Iteration 22, loss = 0.40191626\n",
      "Iteration 23, loss = 0.40140820\n",
      "Iteration 24, loss = 0.40070770\n",
      "Iteration 25, loss = 0.40046742\n",
      "Iteration 26, loss = 0.39951273\n",
      "Iteration 27, loss = 0.39953105\n",
      "Iteration 28, loss = 0.39904701\n",
      "Iteration 29, loss = 0.39865918\n",
      "Iteration 30, loss = 0.39833060\n",
      "Iteration 31, loss = 0.39812331\n",
      "Iteration 32, loss = 0.39803217\n",
      "Iteration 33, loss = 0.39754901\n",
      "Iteration 34, loss = 0.39736727\n",
      "Iteration 35, loss = 0.39753503\n",
      "Iteration 36, loss = 0.39649816\n",
      "Iteration 37, loss = 0.39750553\n",
      "Iteration 38, loss = 0.39638499\n",
      "Iteration 39, loss = 0.39633250\n",
      "Iteration 40, loss = 0.39621672\n",
      "Iteration 41, loss = 0.39611687\n",
      "Iteration 42, loss = 0.39575756\n",
      "Iteration 43, loss = 0.39583458\n",
      "Iteration 44, loss = 0.39573489\n",
      "Iteration 45, loss = 0.39575822\n",
      "Iteration 46, loss = 0.39506948\n",
      "Iteration 47, loss = 0.39559551\n",
      "Iteration 48, loss = 0.39495300\n",
      "Iteration 49, loss = 0.39486796\n",
      "Iteration 50, loss = 0.39460803\n",
      "Iteration 51, loss = 0.39472547\n",
      "Iteration 52, loss = 0.39484018\n",
      "Iteration 53, loss = 0.39446788\n",
      "Iteration 54, loss = 0.39475427\n",
      "Iteration 55, loss = 0.39457017\n",
      "Iteration 56, loss = 0.39402725\n",
      "Iteration 57, loss = 0.39404737\n",
      "Iteration 58, loss = 0.39398721\n",
      "Iteration 59, loss = 0.39396947\n",
      "Iteration 60, loss = 0.39398456\n",
      "Iteration 61, loss = 0.39380166\n",
      "Iteration 62, loss = 0.39359720\n",
      "Iteration 63, loss = 0.39372896\n",
      "Iteration 64, loss = 0.39345259\n",
      "Iteration 65, loss = 0.39329913\n",
      "Iteration 66, loss = 0.39357118\n",
      "Iteration 67, loss = 0.39365878\n",
      "Iteration 68, loss = 0.39352564\n",
      "Iteration 69, loss = 0.39340299\n",
      "Iteration 70, loss = 0.39319628\n",
      "Iteration 71, loss = 0.39341999\n",
      "Iteration 72, loss = 0.39336747\n",
      "Iteration 73, loss = 0.39347416\n",
      "Iteration 74, loss = 0.39344952\n",
      "Iteration 75, loss = 0.39294891\n",
      "Iteration 76, loss = 0.39297339\n",
      "Iteration 77, loss = 0.39315071\n",
      "Iteration 78, loss = 0.39297703\n",
      "Iteration 79, loss = 0.39289692\n",
      "Iteration 80, loss = 0.39293674\n",
      "Iteration 81, loss = 0.39314057\n",
      "Iteration 82, loss = 0.39289672\n",
      "Iteration 83, loss = 0.39278244\n",
      "Iteration 84, loss = 0.39311793\n",
      "Iteration 85, loss = 0.39272409\n",
      "Iteration 86, loss = 0.39269739\n",
      "Iteration 87, loss = 0.39272677\n",
      "Iteration 88, loss = 0.39266400\n",
      "Iteration 89, loss = 0.39290525\n",
      "Iteration 90, loss = 0.39283649\n",
      "Iteration 91, loss = 0.39323672\n",
      "Iteration 92, loss = 0.39271176\n",
      "Iteration 93, loss = 0.39245539\n",
      "Iteration 94, loss = 0.39276741\n",
      "Iteration 95, loss = 0.39311060\n",
      "Iteration 96, loss = 0.39264750\n",
      "Iteration 97, loss = 0.39278583\n",
      "Iteration 98, loss = 0.39248002\n",
      "Iteration 99, loss = 0.39293440\n",
      "Iteration 100, loss = 0.39252892\n",
      "Iteration 101, loss = 0.39233195\n",
      "Iteration 102, loss = 0.39282244\n",
      "Iteration 103, loss = 0.39203135\n",
      "Iteration 104, loss = 0.39275836\n",
      "Iteration 105, loss = 0.39239443\n",
      "Iteration 106, loss = 0.39261575\n",
      "Iteration 107, loss = 0.39242568\n",
      "Iteration 108, loss = 0.39230473\n",
      "Iteration 109, loss = 0.39271984\n",
      "Iteration 110, loss = 0.39261997\n",
      "Iteration 111, loss = 0.39233579\n",
      "Iteration 112, loss = 0.39233572\n",
      "Iteration 113, loss = 0.39225110\n",
      "Iteration 114, loss = 0.39272242\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70730568\n",
      "Iteration 2, loss = 0.65982974\n",
      "Iteration 3, loss = 0.57536827\n",
      "Iteration 4, loss = 0.47540209\n",
      "Iteration 5, loss = 0.41835570\n",
      "Iteration 6, loss = 0.39624533\n",
      "Iteration 7, loss = 0.38764082\n",
      "Iteration 8, loss = 0.38478414\n",
      "Iteration 9, loss = 0.38293782\n",
      "Iteration 10, loss = 0.38161177\n",
      "Iteration 11, loss = 0.38090249\n",
      "Iteration 12, loss = 0.38001821\n",
      "Iteration 13, loss = 0.38122330\n",
      "Iteration 14, loss = 0.37979611\n",
      "Iteration 15, loss = 0.37963374\n",
      "Iteration 16, loss = 0.37925521\n",
      "Iteration 17, loss = 0.37851144\n",
      "Iteration 18, loss = 0.37833448\n",
      "Iteration 19, loss = 0.37791879\n",
      "Iteration 20, loss = 0.37994799\n",
      "Iteration 21, loss = 0.37779829\n",
      "Iteration 22, loss = 0.37773349\n",
      "Iteration 23, loss = 0.37777843\n",
      "Iteration 24, loss = 0.37760843\n",
      "Iteration 25, loss = 0.37729932\n",
      "Iteration 26, loss = 0.37690090\n",
      "Iteration 27, loss = 0.37695266\n",
      "Iteration 28, loss = 0.37676286\n",
      "Iteration 29, loss = 0.37742958\n",
      "Iteration 30, loss = 0.37736525\n",
      "Iteration 31, loss = 0.37673586\n",
      "Iteration 32, loss = 0.37715131\n",
      "Iteration 33, loss = 0.37653155\n",
      "Iteration 34, loss = 0.37694883\n",
      "Iteration 35, loss = 0.37664413\n",
      "Iteration 36, loss = 0.37676439\n",
      "Iteration 37, loss = 0.37709582\n",
      "Iteration 38, loss = 0.37689648\n",
      "Iteration 39, loss = 0.37660153\n",
      "Iteration 40, loss = 0.37665221\n",
      "Iteration 41, loss = 0.37757913\n",
      "Iteration 42, loss = 0.37618993\n",
      "Iteration 43, loss = 0.37620680\n",
      "Iteration 44, loss = 0.37604683\n",
      "Iteration 45, loss = 0.37630043\n",
      "Iteration 46, loss = 0.37610707\n",
      "Iteration 47, loss = 0.37708048\n",
      "Iteration 48, loss = 0.37645538\n",
      "Iteration 49, loss = 0.37650981\n",
      "Iteration 50, loss = 0.37655220\n",
      "Iteration 51, loss = 0.37614381\n",
      "Iteration 52, loss = 0.37587835\n",
      "Iteration 53, loss = 0.37636444\n",
      "Iteration 54, loss = 0.37652755\n",
      "Iteration 55, loss = 0.37604922\n",
      "Iteration 56, loss = 0.37622766\n",
      "Iteration 57, loss = 0.37590195\n",
      "Iteration 58, loss = 0.37610116\n",
      "Iteration 59, loss = 0.37631339\n",
      "Iteration 60, loss = 0.37656872\n",
      "Iteration 61, loss = 0.37606349\n",
      "Iteration 62, loss = 0.37600132\n",
      "Iteration 63, loss = 0.37583004\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.71011205\n",
      "Iteration 2, loss = 0.66248316\n",
      "Iteration 3, loss = 0.58348779\n",
      "Iteration 4, loss = 0.48672760\n",
      "Iteration 5, loss = 0.42972961\n",
      "Iteration 6, loss = 0.40747688\n",
      "Iteration 7, loss = 0.39747559\n",
      "Iteration 8, loss = 0.39394810\n",
      "Iteration 9, loss = 0.39186414\n",
      "Iteration 10, loss = 0.39020452\n",
      "Iteration 11, loss = 0.38993117\n",
      "Iteration 12, loss = 0.38870941\n",
      "Iteration 13, loss = 0.38897615\n",
      "Iteration 14, loss = 0.38856317\n",
      "Iteration 15, loss = 0.38832474\n",
      "Iteration 16, loss = 0.38756745\n",
      "Iteration 17, loss = 0.38732609\n",
      "Iteration 18, loss = 0.38661317\n",
      "Iteration 19, loss = 0.38699843\n",
      "Iteration 20, loss = 0.38699301\n",
      "Iteration 21, loss = 0.38637787\n",
      "Iteration 22, loss = 0.38713393\n",
      "Iteration 23, loss = 0.38610940\n",
      "Iteration 24, loss = 0.38645273\n",
      "Iteration 25, loss = 0.38561926\n",
      "Iteration 26, loss = 0.38616132\n",
      "Iteration 27, loss = 0.38510251\n",
      "Iteration 28, loss = 0.38494718\n",
      "Iteration 29, loss = 0.38552882\n",
      "Iteration 30, loss = 0.38604251\n",
      "Iteration 31, loss = 0.38616626\n",
      "Iteration 32, loss = 0.38516675\n",
      "Iteration 33, loss = 0.38538684\n",
      "Iteration 34, loss = 0.38497933\n",
      "Iteration 35, loss = 0.38453492\n",
      "Iteration 36, loss = 0.38523377\n",
      "Iteration 37, loss = 0.38513328\n",
      "Iteration 38, loss = 0.38520018\n",
      "Iteration 39, loss = 0.38629076\n",
      "Iteration 40, loss = 0.38457135\n",
      "Iteration 41, loss = 0.38621993\n",
      "Iteration 42, loss = 0.38488350\n",
      "Iteration 43, loss = 0.38456507\n",
      "Iteration 44, loss = 0.38480763\n",
      "Iteration 45, loss = 0.38484983\n",
      "Iteration 46, loss = 0.38420232\n",
      "Iteration 47, loss = 0.38543828\n",
      "Iteration 48, loss = 0.38459090\n",
      "Iteration 49, loss = 0.38449713\n",
      "Iteration 50, loss = 0.38476072\n",
      "Iteration 51, loss = 0.38388906\n",
      "Iteration 52, loss = 0.38481635\n",
      "Iteration 53, loss = 0.38417886\n",
      "Iteration 54, loss = 0.38497412\n",
      "Iteration 55, loss = 0.38406235\n",
      "Iteration 56, loss = 0.38447725\n",
      "Iteration 57, loss = 0.38474860\n",
      "Iteration 58, loss = 0.38448288\n",
      "Iteration 59, loss = 0.38439339\n",
      "Iteration 60, loss = 0.38513190\n",
      "Iteration 61, loss = 0.38469215\n",
      "Iteration 62, loss = 0.38429960\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.71222696\n",
      "Iteration 2, loss = 0.66502711\n",
      "Iteration 3, loss = 0.59089953\n",
      "Iteration 4, loss = 0.49808334\n",
      "Iteration 5, loss = 0.44149453\n",
      "Iteration 6, loss = 0.41751896\n",
      "Iteration 7, loss = 0.40745772\n",
      "Iteration 8, loss = 0.40320036\n",
      "Iteration 9, loss = 0.40140745\n",
      "Iteration 10, loss = 0.40055625\n",
      "Iteration 11, loss = 0.39860174\n",
      "Iteration 12, loss = 0.39902877\n",
      "Iteration 13, loss = 0.39899775\n",
      "Iteration 14, loss = 0.39725590\n",
      "Iteration 15, loss = 0.39690903\n",
      "Iteration 16, loss = 0.39604341\n",
      "Iteration 17, loss = 0.39615228\n",
      "Iteration 18, loss = 0.39543853\n",
      "Iteration 19, loss = 0.39656730\n",
      "Iteration 20, loss = 0.39539758\n",
      "Iteration 21, loss = 0.39482365\n",
      "Iteration 22, loss = 0.39462831\n",
      "Iteration 23, loss = 0.39481207\n",
      "Iteration 24, loss = 0.39425332\n",
      "Iteration 25, loss = 0.39527515\n",
      "Iteration 26, loss = 0.39350234\n",
      "Iteration 27, loss = 0.39407627\n",
      "Iteration 28, loss = 0.39340031\n",
      "Iteration 29, loss = 0.39347804\n",
      "Iteration 30, loss = 0.39325784\n",
      "Iteration 31, loss = 0.39338325\n",
      "Iteration 32, loss = 0.39363828\n",
      "Iteration 33, loss = 0.39296315\n",
      "Iteration 34, loss = 0.39327293\n",
      "Iteration 35, loss = 0.39346137\n",
      "Iteration 36, loss = 0.39329880\n",
      "Iteration 37, loss = 0.39406344\n",
      "Iteration 38, loss = 0.39297005\n",
      "Iteration 39, loss = 0.39285268\n",
      "Iteration 40, loss = 0.39293585\n",
      "Iteration 41, loss = 0.39330409\n",
      "Iteration 42, loss = 0.39258767\n",
      "Iteration 43, loss = 0.39322325\n",
      "Iteration 44, loss = 0.39287440\n",
      "Iteration 45, loss = 0.39299472\n",
      "Iteration 46, loss = 0.39226133\n",
      "Iteration 47, loss = 0.39342370\n",
      "Iteration 48, loss = 0.39264990\n",
      "Iteration 49, loss = 0.39256400\n",
      "Iteration 50, loss = 0.39238480\n",
      "Iteration 51, loss = 0.39271923\n",
      "Iteration 52, loss = 0.39364001\n",
      "Iteration 53, loss = 0.39273714\n",
      "Iteration 54, loss = 0.39317526\n",
      "Iteration 55, loss = 0.39313948\n",
      "Iteration 56, loss = 0.39220479\n",
      "Iteration 57, loss = 0.39268694\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.77441521\n",
      "Iteration 2, loss = 0.66670084\n",
      "Iteration 3, loss = 0.62454795\n",
      "Iteration 4, loss = 0.57113698\n",
      "Iteration 5, loss = 0.51651598\n",
      "Iteration 6, loss = 0.47215152\n",
      "Iteration 7, loss = 0.44170458\n",
      "Iteration 8, loss = 0.42280509\n",
      "Iteration 9, loss = 0.41104931\n",
      "Iteration 10, loss = 0.40389131\n",
      "Iteration 11, loss = 0.39950219\n",
      "Iteration 12, loss = 0.39662781\n",
      "Iteration 13, loss = 0.39458994\n",
      "Iteration 14, loss = 0.39341859\n",
      "Iteration 15, loss = 0.39242633\n",
      "Iteration 16, loss = 0.39198351\n",
      "Iteration 17, loss = 0.39139433\n",
      "Iteration 18, loss = 0.39056923\n",
      "Iteration 19, loss = 0.39047608\n",
      "Iteration 20, loss = 0.39030153\n",
      "Iteration 21, loss = 0.38979570\n",
      "Iteration 22, loss = 0.38932167\n",
      "Iteration 23, loss = 0.38908437\n",
      "Iteration 24, loss = 0.38897692\n",
      "Iteration 25, loss = 0.38865575\n",
      "Iteration 26, loss = 0.38843370\n",
      "Iteration 27, loss = 0.38867706\n",
      "Iteration 28, loss = 0.38846058\n",
      "Iteration 29, loss = 0.38811941\n",
      "Iteration 30, loss = 0.38786343\n",
      "Iteration 31, loss = 0.38774650\n",
      "Iteration 32, loss = 0.38761154\n",
      "Iteration 33, loss = 0.38737708\n",
      "Iteration 34, loss = 0.38733313\n",
      "Iteration 35, loss = 0.38710430\n",
      "Iteration 36, loss = 0.38732042\n",
      "Iteration 37, loss = 0.38682934\n",
      "Iteration 38, loss = 0.38685700\n",
      "Iteration 39, loss = 0.38663364\n",
      "Iteration 40, loss = 0.38676230\n",
      "Iteration 41, loss = 0.38691735\n",
      "Iteration 42, loss = 0.38667825\n",
      "Iteration 43, loss = 0.38650354\n",
      "Iteration 44, loss = 0.38632862\n",
      "Iteration 45, loss = 0.38662285\n",
      "Iteration 46, loss = 0.38629037\n",
      "Iteration 47, loss = 0.38622683\n",
      "Iteration 48, loss = 0.38610439\n",
      "Iteration 49, loss = 0.38619829\n",
      "Iteration 50, loss = 0.38625402\n",
      "Iteration 51, loss = 0.38605427\n",
      "Iteration 52, loss = 0.38593832\n",
      "Iteration 53, loss = 0.38607939\n",
      "Iteration 54, loss = 0.38606428\n",
      "Iteration 55, loss = 0.38596636\n",
      "Iteration 56, loss = 0.38600886\n",
      "Iteration 57, loss = 0.38591173\n",
      "Iteration 58, loss = 0.38575509\n",
      "Iteration 59, loss = 0.38598944\n",
      "Iteration 60, loss = 0.38612368\n",
      "Iteration 61, loss = 0.38612336\n",
      "Iteration 62, loss = 0.38588730\n",
      "Iteration 63, loss = 0.38595664\n",
      "Iteration 64, loss = 0.38586854\n",
      "Iteration 65, loss = 0.38573331\n",
      "Iteration 66, loss = 0.38585848\n",
      "Iteration 67, loss = 0.38602176\n",
      "Iteration 68, loss = 0.38586097\n",
      "Iteration 69, loss = 0.38575208\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='tanh', hidden_layer_sizes=(2,),\n",
       "              learning_rate_init=0.01, random_state=0, solver='sgd',\n",
       "              verbose=True)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## This is the neural network trained in Lab 8\n",
    "\n",
    "def train_neural_network(X, y):\n",
    "    net = sklearn.neural_network.MLPClassifier(        \n",
    "        learning_rate_init=0.01,\n",
    "        momentum=0.9,\n",
    "        random_state=0,\n",
    "        verbose=True\n",
    "    )\n",
    "    param_grid = [\n",
    "        {\n",
    "            'activation' : ['identity', 'logistic', 'tanh', 'relu'],\n",
    "            'solver' : ['lbfgs', 'sgd', 'adam'],\n",
    "            'hidden_layer_sizes': [\n",
    "             (), (1,),(2,),(3,),\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    gscv = sklearn.model_selection.GridSearchCV(net, param_grid=param_grid, verbose=1, cv=3)\n",
    "    gscv.fit(X, y)\n",
    "\n",
    "    return gscv.best_estimator_\n",
    "train_neural_network(X_trn,y_trn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n",
      "{'mean_fit_time': array([0.21886595, 0.40351486, 0.59556667, 0.78610365, 0.25769265,\n",
      "       0.54649464, 0.83085903, 1.12065236, 0.32478468, 0.66416907,\n",
      "       0.99838114, 1.36088077, 0.34816416, 0.71597894, 1.05228972,\n",
      "       1.35137105]), 'std_fit_time': array([0.02164036, 0.01227785, 0.02530283, 0.01626275, 0.00456116,\n",
      "       0.01367742, 0.00880096, 0.02506413, 0.00310483, 0.02299487,\n",
      "       0.024675  , 0.03617472, 0.00288785, 0.0052021 , 0.00754718,\n",
      "       0.03227042]), 'mean_score_time': array([0.02196924, 0.04387776, 0.06342204, 0.07663337, 0.02781526,\n",
      "       0.05535237, 0.08046508, 0.10912172, 0.03407804, 0.06600746,\n",
      "       0.10362546, 0.13431478, 0.03797301, 0.07280588, 0.10687526,\n",
      "       0.1418616 ]), 'std_score_time': array([0.0013889 , 0.001975  , 0.00472141, 0.0009819 , 0.0017469 ,\n",
      "       0.00042229, 0.00070675, 0.00192635, 0.00254704, 0.00223741,\n",
      "       0.00449688, 0.00168412, 0.00189671, 0.0030093 , 0.00235423,\n",
      "       0.00462107]), 'param_max_depth': masked_array(data=[5, 5, 5, 5, 10, 10, 10, 10, 15, 15, 15, 15, 20, 20, 20,\n",
      "                   20],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_n_estimators': masked_array(data=[50, 100, 150, 200, 50, 100, 150, 200, 50, 100, 150,\n",
      "                   200, 50, 100, 150, 200],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'params': [{'max_depth': 5, 'n_estimators': 50}, {'max_depth': 5, 'n_estimators': 100}, {'max_depth': 5, 'n_estimators': 150}, {'max_depth': 5, 'n_estimators': 200}, {'max_depth': 10, 'n_estimators': 50}, {'max_depth': 10, 'n_estimators': 100}, {'max_depth': 10, 'n_estimators': 150}, {'max_depth': 10, 'n_estimators': 200}, {'max_depth': 15, 'n_estimators': 50}, {'max_depth': 15, 'n_estimators': 100}, {'max_depth': 15, 'n_estimators': 150}, {'max_depth': 15, 'n_estimators': 200}, {'max_depth': 20, 'n_estimators': 50}, {'max_depth': 20, 'n_estimators': 100}, {'max_depth': 20, 'n_estimators': 150}, {'max_depth': 20, 'n_estimators': 200}], 'split0_test_score': array([0.81499904, 0.8157643 , 0.8157643 , 0.81499904, 0.80945093,\n",
      "       0.81002487, 0.81002487, 0.8090683 , 0.77501435, 0.77597092,\n",
      "       0.77654486, 0.77616223, 0.75683949, 0.75358714, 0.75205663,\n",
      "       0.75263057]), 'split1_test_score': array([0.82399082, 0.82494739, 0.82494739, 0.82475607, 0.8171035 ,\n",
      "       0.81825139, 0.81806007, 0.81748613, 0.78955424, 0.79204132,\n",
      "       0.79185001, 0.7916587 , 0.77004018, 0.76927492, 0.76812703,\n",
      "       0.76889229]), 'split2_test_score': array([0.83524684, 0.83601225, 0.83601225, 0.83639495, 0.82893226,\n",
      "       0.82854956, 0.82893226, 0.82931496, 0.79850746, 0.79831611,\n",
      "       0.80156908, 0.80099502, 0.77669346, 0.77554535, 0.7765021 ,\n",
      "       0.7745886 ]), 'mean_test_score': array([0.82474557, 0.82557465, 0.82557465, 0.82538336, 0.81849556,\n",
      "       0.81894194, 0.81900574, 0.81862313, 0.78769202, 0.78877612,\n",
      "       0.78998798, 0.78960532, 0.76785771, 0.7661358 , 0.76556192,\n",
      "       0.76537049]), 'std_test_score': array([0.00828334, 0.00827808, 0.00827808, 0.0087461 , 0.0080139 ,\n",
      "       0.00757842, 0.00774782, 0.00830467, 0.009681  , 0.00941003,\n",
      "       0.01030059, 0.01024139, 0.00825096, 0.00923512, 0.01014331,\n",
      "       0.0093038 ]), 'rank_test_score': array([ 4,  1,  1,  3,  8,  6,  5,  7, 12, 11,  9, 10, 13, 14, 15, 16],\n",
      "      dtype=int32)}\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "X_trn, X_tst = scale_data(X_trn, X_tst)\n",
    "# train_neural_network(X, y)\n",
    "# train_kNN_estimators(X_trn, X_tst, y_trn, y_tst)\n",
    "rf_clf = train_random_forests(X_trn, y_trn)\n",
    "\n",
    "dm_clf = train_dummy_classifier(X_trn, y_trn)\n",
    "y_pred = dm_clf.predict(X_tst)\n",
    "stats = sklearn.metrics.precision_recall_fscore_support(y_tst, y_pred, average='binary')\n",
    "sklearn.metrics.accuracy_score(y_tst, y_pred)\n",
    "plt.figure()\n",
    "plot_data(X_trn[:300], y_trn[:300])\n",
    "plot_rf_prediction(rf_clf)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "PyCharm (comp432)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
