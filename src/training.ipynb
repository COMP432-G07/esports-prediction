{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import sklearn.model_selection\n",
    "import sklearn.dummy\n",
    "import sklearn.metrics\n",
    "import sklearn.ensemble\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>game_duration</th>\n",
       "      <th>winner</th>\n",
       "      <th>first_kill</th>\n",
       "      <th>first_tower</th>\n",
       "      <th>first_inhibitor</th>\n",
       "      <th>first_baron</th>\n",
       "      <th>first_dragon</th>\n",
       "      <th>first_rift_herald</th>\n",
       "      <th>kill_advantage</th>\n",
       "      <th>gold_advantage</th>\n",
       "      <th>cs_advantage</th>\n",
       "      <th>damage_advantage</th>\n",
       "      <th>tower_advantage</th>\n",
       "      <th>plate_advantage</th>\n",
       "      <th>inhibitor_advantage</th>\n",
       "      <th>dragon_advantage</th>\n",
       "      <th>rift_advantage</th>\n",
       "      <th>baron_advantage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1780</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>7354</td>\n",
       "      <td>72</td>\n",
       "      <td>24920</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-6</td>\n",
       "      <td>316</td>\n",
       "      <td>77</td>\n",
       "      <td>-7851</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2430</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>880</td>\n",
       "      <td>59</td>\n",
       "      <td>-4156</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1952</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-10</td>\n",
       "      <td>-5034</td>\n",
       "      <td>12</td>\n",
       "      <td>-8164</td>\n",
       "      <td>-3</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2424</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>7307</td>\n",
       "      <td>143</td>\n",
       "      <td>6028</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3915</th>\n",
       "      <td>1857</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>5743</td>\n",
       "      <td>-10</td>\n",
       "      <td>2542</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3916</th>\n",
       "      <td>1468</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>10242</td>\n",
       "      <td>151</td>\n",
       "      <td>21106</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3917</th>\n",
       "      <td>2068</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-3035</td>\n",
       "      <td>29</td>\n",
       "      <td>-3468</td>\n",
       "      <td>-1</td>\n",
       "      <td>-2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>-2</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3918</th>\n",
       "      <td>1868</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-5</td>\n",
       "      <td>-1807</td>\n",
       "      <td>-88</td>\n",
       "      <td>-12656</td>\n",
       "      <td>-3</td>\n",
       "      <td>-5</td>\n",
       "      <td>-2</td>\n",
       "      <td>-3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3919</th>\n",
       "      <td>1740</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-15</td>\n",
       "      <td>-8702</td>\n",
       "      <td>-26</td>\n",
       "      <td>-18949</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-3</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3920 rows Ã— 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      game_duration  winner  first_kill  first_tower  first_inhibitor  \\\n",
       "0              1780       1           1            1                1   \n",
       "1              2020      -1           1            1                1   \n",
       "2              2430      -1          -1            1                1   \n",
       "3              1952      -1           1           -1               -1   \n",
       "4              2424       1           1            1                1   \n",
       "...             ...     ...         ...          ...              ...   \n",
       "3915           1857       1          -1            1                1   \n",
       "3916           1468       1           1            1                1   \n",
       "3917           2068      -1           1           -1               -1   \n",
       "3918           1868      -1          -1           -1               -1   \n",
       "3919           1740      -1           1            1               -1   \n",
       "\n",
       "      first_baron  first_dragon  first_rift_herald  kill_advantage  \\\n",
       "0               1            -1                  1               3   \n",
       "1              -1            -1                  1              -6   \n",
       "2              -1            -1                 -1               2   \n",
       "3              -1            -1                 -1             -10   \n",
       "4               1            -1                  1               3   \n",
       "...           ...           ...                ...             ...   \n",
       "3915            1             1                  0               5   \n",
       "3916            1             1                  1               6   \n",
       "3917           -1             1                 -1               0   \n",
       "3918            1            -1                  1              -5   \n",
       "3919           -1            -1                  1             -15   \n",
       "\n",
       "      gold_advantage  cs_advantage  damage_advantage  tower_advantage  \\\n",
       "0               7354            72             24920                2   \n",
       "1                316            77             -7851                5   \n",
       "2                880            59             -4156                4   \n",
       "3              -5034            12             -8164               -3   \n",
       "4               7307           143              6028                3   \n",
       "...              ...           ...               ...              ...   \n",
       "3915            5743           -10              2542                2   \n",
       "3916           10242           151             21106                5   \n",
       "3917           -3035            29             -3468               -1   \n",
       "3918           -1807           -88            -12656               -3   \n",
       "3919           -8702           -26            -18949                0   \n",
       "\n",
       "      plate_advantage  inhibitor_advantage  dragon_advantage  rift_advantage  \\\n",
       "0                   8                    0                -1               1   \n",
       "1                   6                    1                 1               1   \n",
       "2                   1                    0                -1              -1   \n",
       "3                  -1                    0                -3               0   \n",
       "4                   5                    0                -1               2   \n",
       "...               ...                  ...               ...             ...   \n",
       "3915                8                    0                -2               0   \n",
       "3916                4                    1                 1               2   \n",
       "3917               -2                    0                 3              -2   \n",
       "3918               -5                   -2                -3               1   \n",
       "3919                1                    0                -3               2   \n",
       "\n",
       "      baron_advantage  \n",
       "0                   0  \n",
       "1                  -1  \n",
       "2                   0  \n",
       "3                   0  \n",
       "4                   0  \n",
       "...               ...  \n",
       "3915                0  \n",
       "3916                1  \n",
       "3917               -1  \n",
       "3918                1  \n",
       "3919               -1  \n",
       "\n",
       "[3920 rows x 18 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read both the matches and the frames\n",
    "training_df = pd.read_csv('../data/processed/diff_train.csv').drop(labels=['tier','Unnamed: 0'],axis=1)\n",
    "test_df = pd.read_csv('../data/processed/diff_test.csv').drop(labels=['tier','Unnamed: 0'],axis=1)\n",
    "# Reinterpret all values as int32s\n",
    "training_df = training_df.astype({\n",
    "    'winner': 'int32',\n",
    "    'first_kill': 'int32',\n",
    "    'first_tower': 'int32',\n",
    "    'first_inhibitor': 'int32',\n",
    "    'first_baron': 'int32',\n",
    "    'first_dragon': 'int32',\n",
    "    'first_rift_herald': 'int32',\n",
    "})\n",
    "test_df = test_df.astype({\n",
    "    'winner': 'int32',\n",
    "    'first_kill': 'int32',\n",
    "    'first_tower': 'int32',\n",
    "    'first_inhibitor': 'int32',\n",
    "    'first_baron': 'int32',\n",
    "    'first_dragon': 'int32',\n",
    "    'first_rift_herald': 'int32',\n",
    "})\n",
    "\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def scale_data(X_trn, X_tst):\n",
    "    # Scale the data with MinMax to avoid negative values\n",
    "    scaler = sklearn.preprocessing.MinMaxScaler()\n",
    "    scaler.fit(X_trn)\n",
    "    X_trn = scaler.transform(X_trn)\n",
    "    X_tst = scaler.transform(X_tst)\n",
    "\n",
    "    return X_trn, X_tst\n",
    "    \n",
    "## Here we train a dummy classifier to compare performance\n",
    "def train_dummy_classifier(X, y):\n",
    "    dummy_clf = sklearn.dummy.DummyClassifier(strategy='uniform', random_state=0)\n",
    "    # Scale the data with MinMax to avoid negative values\n",
    "    dummy_clf.fit(X, y)\n",
    "    return dummy_clf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3920,)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_trn = training_df[['kill_advantage','gold_advantage']].values\n",
    "y_trn = training_df[['winner']].values.T[0]\n",
    "X_tst = test_df[['kill_advantage','gold_advantage']].values\n",
    "y_tst = test_df[['winner']].values.T[0]\n",
    "y_tst.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING ON FRAME #25\n",
      "(26565, 20)\n",
      "(26565, 20)\n"
     ]
    }
   ],
   "source": [
    "# # Loop created to go through every frame in the data set. \n",
    "# # For each frame, pick all corresponding data points, and \n",
    "# # train both the neural network and the dummy classifier\n",
    "# # with it\n",
    "# current_frame = 25\n",
    "\n",
    "# # Convert -1 and +1 to 0 and 1 for the tensors to work\n",
    "# y = np.interp(y, (-1,+1), (0, 1)).astype(np.int32)\n",
    "\n",
    "# # Two things occur here:\n",
    "# # 1. Normalize the data from values to a distribution [0,1] in l1 norm\n",
    "# #    e.g if Red side has 11 kill, and Blue has 9, we represent it as\n",
    "# #        0.55 and 0.45, respectively.\n",
    "# # 2. Collapse each blue features to their red features by subtracting them\n",
    "# #    e.g for ourprevious example, subtracting blue from red would result\n",
    "# #        to having 0.55-0.45 = 0.1. Hence a 10% advantage for blue\n",
    "# #        FIXME: could be an arbitrary x[0,:] - 0.5 to avoid doubling up\n",
    "# def combine(x):\n",
    "#     x = sklearn.preprocessing.normalize(x.reshape(2,10), norm='l1', axis=0)\n",
    "#     return x.reshape(20)\n",
    "# # Normalize the data to have 10 features \n",
    "# # corresponding to a better comparison.\n",
    "# X = np.apply_along_axis(combine, 1, X)\n",
    "X = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def score_estimators(X, y, estimators):\n",
    "    \"\"\"Scores each estimator on (X, y), returning a list of scores.\"\"\"\n",
    "    # Your implementation here. Aim for 1-4 lines.\n",
    "    scores = [0 for _ in range(len(estimators))]\n",
    "    for x in range(len(estimators)):\n",
    "        scores[x] = sklearn.metrics.accuracy_score(y, estimators[x].predict(X))\n",
    "        print(sklearn.metrics.precision_recall_fscore_support(y, estimators[x].predict(X), average='binary'))\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train_kNN_estimators(X_trn,X_tst,y_trn,y_tst):\n",
    "    estimators = [0 for _ in range(5)]\n",
    "    param_vals=[1,5,10,15,20]\n",
    "    for x in range(5):\n",
    "        estimators[x] = sklearn.neighbors.KNeighborsClassifier(n_neighbors=param_vals[x]).fit(X_trn, y_trn)\n",
    "    trn_scores = score_estimators(X_trn, y_trn, estimators)\n",
    "    tst_scores = score_estimators(X_tst, y_tst, estimators)\n",
    "    X_axis = np.arange(0, 5)\n",
    "    plt.title(estimators[0].__class__.__name__ + \" score vs \" + \"n_neighbors\")\n",
    "    plt.xlabel(\"n_neighbors\")\n",
    "    plt.ylabel(\"score\")\n",
    "    plt.ylim(0.0, 1.05)\n",
    "    plt.plot(X, trn_scores, marker='o', color='green', markerfacecolor='green', label=\"train\")\n",
    "    plt.plot(X, tst_scores, marker='o', color='red', markerfacecolor='red', label=\"test\")\n",
    "    plt.legend()\n",
    "    plt.xticks(X_axis, param_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train_random_forests(X_trn, X_tst, y_trn, y_tst):\n",
    "    estimator = np.arange(1, 6) * 50\n",
    "    depths = np.arange(1, 6) * 5\n",
    "    estimators_meshed, depths_meshed = np.meshgrid(estimator, depths)\n",
    "\n",
    "    trn_scores = np.arange(0, 25, dtype=float)\n",
    "    tst_scores = np.arange(0, 25, dtype=float)\n",
    "    X_axis = np.arange(0, 25)\n",
    "    para_vals = np.array(\n",
    "        [\"50:5\", \"50:10\", \"50:15\", \"50:20\", \"50:25\", \"100:5\", \"100:10\", \"100:15\", \"100:20\", \"100:25\", \"150:5\", \"150:10\",\n",
    "         \"150:15\", \"150:20\",\"150:25\",\"200:5\", \"200:10\", \"200:15\", \"200:20\", \"200:25\", \"250:5\", \"250:10\", \"250:15\",\n",
    "         \"250:20\", \"250:25\"])\n",
    "    print(X_axis.shape)\n",
    "    print(para_vals.shape)\n",
    "    for i in range(5):\n",
    "        for j in range(5):\n",
    "            random_forest_model = sklearn.ensemble.RandomForestClassifier(n_estimators=estimators_meshed[j][i],\n",
    "                                                                          max_depth=depths_meshed[j][i],\n",
    "                                                                          random_state=0).fit(X_trn, y_trn)\n",
    "            trn_scores[i * 5 + j] = sklearn.metrics.accuracy_score(y_trn, random_forest_model.predict(X_trn))\n",
    "            print(\"Training Accuracy: %.2f %% with estimators = %d , depth = %d \\n\" % (\n",
    "                trn_scores[i * 5 + j] * 100, estimators_meshed[j][i], depths_meshed[j][i]))\n",
    "            tst_scores[i * 5 + j] = sklearn.metrics.accuracy_score(y_tst, random_forest_model.predict(X_tst))\n",
    "            print(\"Test Accuracy: %.2f %% with estimators = %d , depth = %d \\n\" % (\n",
    "                tst_scores[i * 5 + j] * 100, estimators_meshed[j][i], depths_meshed[j][i]))\n",
    "            print(\"Pr/Rec/Fscore: \")\n",
    "            print(sklearn.metrics.precision_recall_fscore_support(y_tst, random_forest_model.predict(X_tst), average='binary'))\n",
    "\n",
    "    plt.title(\"Random Forests score vs estimators : depth\")\n",
    "    plt.xlabel(\"estimators : depth\")\n",
    "    plt.ylabel(\"score\")\n",
    "    plt.ylim(0.0, 1.05)\n",
    "    plt.plot(X_axis, trn_scores, marker='o', color='green', markerfacecolor='green', label=\"train\")\n",
    "    plt.plot(X_axis, tst_scores, marker='o', color='red', markerfacecolor='red', label=\"test\")\n",
    "    plt.legend()\n",
    "    print(para_vals)\n",
    "    plt.xticks(X_axis, para_vals, fontsize=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This is the neural network trained in Lab 8\n",
    "\n",
    "def train_neural_network(X, y):\n",
    "    # Tensors setup\n",
    "    X_trn_torch = torch.tensor(X_trn, dtype=torch.float32)\n",
    "    y_trn_torch = torch.tensor(y_trn, dtype=torch.int64)\n",
    "    X_tst_torch = torch.tensor(X_tst, dtype=torch.float32)\n",
    "    y_tst_torch = torch.tensor(y_tst, dtype=torch.int64)\n",
    "\n",
    "    torch.manual_seed(0) # Ensure model weights initialized with same random numbers\n",
    "\n",
    "    # Create an object that holds a sequence of layers and activation functions\n",
    "    model = torch.nn.Sequential(\n",
    "        torch.nn.Linear(20, 10),   # Applies Wx+b from 10 dimensions down to 2\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(10,2)\n",
    "    )\n",
    "\n",
    "    # Create an object that can compute \"negative log likelihood of a softmax\"\n",
    "    loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # Use stochastic gradient descent to train the model\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    "\n",
    "    # Use 100 training samples at a time to compute the gradient.\n",
    "    batch_size = 200\n",
    "\n",
    "    # Make 10 passes over the training data, each time using batch_size samples to compute gradient\n",
    "    num_epoch = 10\n",
    "    next_epoch = 1\n",
    "\n",
    "    for epoch in range(next_epoch, next_epoch+num_epoch):\n",
    "        # Make an entire pass (an 'epoch') over the training data in batch_size chunks\n",
    "        for i in range(0, len(X_trn), batch_size):        \n",
    "            X_cur = X_trn_torch[i:i+batch_size]     # Slice out a mini-batch of features\n",
    "            y_cur = y_trn_torch[i:i+batch_size]     # Slice out a mini-batch of targets\n",
    "            \n",
    "            y_pred = model(X_cur)                   # Make predictions (final-layer activations)\n",
    "            l = loss(y_pred, y_cur)                 # Compute loss with respect to predictions\n",
    "            \n",
    "            model.zero_grad()                   # Reset all gradient accumulators to zero (PyTorch thing)\n",
    "            l.backward()                        # Compute gradient of loss wrt all parameters (backprop!)\n",
    "            optimizer.step()                    # Use the gradients to take a step with SGD.\n",
    "            \n",
    "        print(\"Epoch %2d: loss on final training batch: %.4f\" % (epoch, l.item()))\n",
    "        \n",
    "    print(\"Epoch %2d: loss on test set: %.4f\" % (epoch, loss(model(X_tst_torch), y_tst_torch)))\n",
    "    next_epoch = epoch+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.9941281593055911, 0.9945090026816499, 0.9943185445260134, None)\n",
      "(0.851370299553856, 0.8528923509130379, 0.8521306455728501, None)\n",
      "(0.8603670972127804, 0.8080704890818542, 0.8333991834584485, None)\n",
      "(0.8375418706518939, 0.8301621759673095, 0.8338356955043932, None)\n",
      "(0.8412095824060741, 0.8205848550632103, 0.8307692307692309, None)\n",
      "(0.7543589743589744, 0.7516607051609606, 0.7530074225748655, None)\n",
      "(0.7974554707379135, 0.8007153806847215, 0.7990821009688934, None)\n",
      "(0.8216012896292316, 0.7812979049565661, 0.8009429020429544, None)\n",
      "(0.8050377833753148, 0.8165559529892693, 0.8107559614408929, None)\n",
      "(0.8198244708311823, 0.8114460909555442, 0.8156137647663071, None)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (15680, 2) and (5,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-c909dff1bb59>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mX_trn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_tst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscale_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_trn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_tst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# train_neural_network(X, y)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_kNN_estimators\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_trn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_tst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_trn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_tst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# train_random_forests(X_trn, X_tst, y_trn, y_tst)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dummy_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_trn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_trn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-38-64e1ac7c0ac4>\u001b[0m in \u001b[0;36mtrain_kNN_estimators\u001b[0;34m(X_trn, X_tst, y_trn, y_tst)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"score\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.05\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrn_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmarker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'o'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'green'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmarkerfacecolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'green'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtst_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmarker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'o'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'red'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmarkerfacecolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'red'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"test\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3017\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0m_copy_docstring_and_deprecators\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3018\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscalex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3019\u001b[0;31m     return gca().plot(\n\u001b[0m\u001b[1;32m   3020\u001b[0m         \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscalex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscalex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscaley\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3021\u001b[0m         **({\"data\": data} if data is not None else {}), **kwargs)\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1603\u001b[0m         \"\"\"\n\u001b[1;32m   1604\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1605\u001b[0;31m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1606\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m    313\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_next_color\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, tup, kwargs, return_kwargs)\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 501\u001b[0;31m             raise ValueError(f\"x and y must have same first dimension, but \"\n\u001b[0m\u001b[1;32m    502\u001b[0m                              f\"have shapes {x.shape} and {y.shape}\")\n\u001b[1;32m    503\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (15680, 2) and (5,)"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEXCAYAAACzhgONAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAbpElEQVR4nO3debgcdZ3v8feHsAsimugIBIICahTXiN5xRmBcBlHhjguCuOAgKK4zevWijoqoc10edXQuLugoriC4RsXBBZRxAQkjIgHBGEACqAFZBJTN7/xRdUjTnlOnzyF1zkl4v54nT7qrfl317V/X6U/Xr7qrUlVIkjSRDWa7AEnS3GZQSJI6GRSSpE4GhSSpk0EhSepkUEiSOhkU66Aky5PsMWLbi5I8foJ5eyRZtTZrm64kxyR5W4/Lvy7JfdrbmyX5WpJrkpyQ5MAk3+pr3ZqeJB9O8sYR23ZuP0kqyU5rr7o7F4OiB8Nvzkn2T3JVkt2TLGo32hOHHvOZJEeMsvyqemBVfW/tVt2vNF6R5Jwk1ydZ1b5J7zoT66+qLapqZXv3GcC9gHtU1TOr6rNV9cSZqEOjq6oXV9VbZ7sOGRS9S/J84CjgyVX1/YFZj0ry17NU1lqXZMNJmrwfeCXwCuDuwC7AV4An91vZuHYALqiqW+7ogpLMWwv1zPl1qjHCdr5eMih6lORFwHuAv6+qHw3Nfhfw9o7HPiXJWUmuTvKjJA8emHfbHks7jPLJdo/lvCSvHWc46aFJzm6HWj6fZNOhdb0+yRXtcg8cmL5Vkk8lWZ3k4iT/kmSDdt5BSX6Y5H1JrgSOSLJTku+367kiyefbtjsDLwUOqKqTq+rGqrqh/ST/jnGe+9ZJvt6u96r29nYD8w9KsjLJH5JcOFbzROtv51U7/y3Am4BntcNRB7fL+8FA2/sn+XaS3yc5P8l+A/OOSfKhJCcmuR7Yc5z6x62vnXdI+zr9Icm5SR7eTn9Aku+1r/fyJPt0rTPJNkm+2PbRhUleMVxH+9hHJfnNYLgk+YckZ7e3d0uyLMm1SX6b5L0TLGePdi/w1Ul+l+TyJC8Yr+3Q445JclSSb7TP+fQk951CX79t4P5r2/VeluSF+cvhpK0nWk9r7/Z1uSLJuwe25Q3abfvi9rl9KslW7byxEYCDk/waODnJpmlGAK5sX68zktxrsr5Yp1WV/9byP+Ai4IvAb4GHDM1bBBSwJXAp8Ph2+meAI9rbDwN+BzwKmAc8v13mJgPLH3vcO4DvA1sD2wFnA6uGavkJsA3NJ/nzgBe38/YAbgHeC2wC7A5cD9yvnf8p4KttrYuAC4CD23kHtY99ObAhsBlwLPAGmg8gmwJ/07Z9MXDxJH12DPC29vY9gKcDm7frPgH4SjvvLsC1AzXeG3hge3vc9bfzCtipvX0E8JmBeQcBPxhY/iXAC9rn9TDgCmDxQJ3XAI8ZW8/Q8+iq75nta/5IIMBONHs3GwErgNcDGwN/B/xhYBnD69wcOJMm8DYG7gOspPlAMl7f/gp4wsD9E4DD29s/Bp7b3t4CePQEyxjbVo5s690buAHYeoTX9Upgt7Y/PwscN4W+Htsm9gJ+Azywff6fGXpNJ1zPwOt/Cs3fwPY02/IL23n/2Pb/fdo++BLw6aG/10+19W4GvAj4WlvHPOARwF1n+32nz3/uUfTnCcBpwM8nmP9Hmj2K8Q7AHQp8pKpOr6pbq+qTwI3Ao8dpux/wr1V1VVWtAj4wTpsPVNVlVfV7mg38oUPz31jNp/zvA98A9ms/ge4PvK6q/lBVF9HsHT134HGXVdW/V9UtVfVH4GaaN75tqupPVTX2Kf0ewOUT9MNfqKorq+qL1ex1/IGmn3YfaPJn4EFJNquqy6tqeTt9ovVPxVOAi6rqE+3z+ilN6D9zoM1Xq+qHVfXnqvrTOMuYqL4XAu+qqjOqsaKqLqZ5XbcA3lFVN1XVycDXgQPGWyewK7Cgqo5s268EPkrzeo3n2LFlJdmS5k3+2HbezcBOSeZX1XVVdVpH39wMHFlVN1fVicB1wP062o/5clX9pJqhvs+yZvsbpa/H7Ad8oqqWV9UNNGE/6nrGvLOqfl9Vvwb+jTX9eyDw3qpaWVXXAa8D9s/th5mOqKrrB7bze9CE1K1VdWZVXTtCP6yzDIr+HEYzDv+xJJmgzceAeyV56tD0HYBXt7u1Vye5GlhIs1cwbBuaT2VjLhmnzW8Gbt9A86Y05qqqun7g/sXtMufTfHK8eGjeth3rei3NJ+WftMMn/9hOv5Lmk/VIkmye5CPtUMC1wKnA3ZLMa2t9Fs1eyuXtUMP9J1n/VOxAc/xosO8PBP5qoM14fQzAJPUtpPl0P2wb4JI2BMZ09fUOwDZDNb6e5gD9eD4HPC3JJsDTgP9uAwrgYJrt9BftEMpTJnpuwJV1++M6w9vSRCba/kbp6zF3dDsffszYdj627OHtfENu35+Dj/00cBJwXDsM9q4kG41Tz3rDoOjPb4HHAX8LfHC8BlV1E/AW4K00b3BjLgHeXlV3G/i3eVUdO85iLqcZchqzcIp1bp3kLgP3twcuoxkCGPuEPjjv0sGnMPR8flNVh1TVNjS75x9sx5C/C2yXZMmINb2a5pPqo6rqrsBj2+lp13NSVT2BJnx+QfNpumv9U3EJ8P2hvt+iqg6b6HkPm6i+dtnD4+bQ9PfCsTHzVldfXwJcOFTjllW19wT1nEvz5vck4Nk0wTE275dVdQBwT+CdwBeGtoc+jdLXY+7odj78mLHtnPb/4e38Fpq/4TG39X+7R/WWqloM/DXNntHzplHPOsOg6FFVXUYTFnsled8EzT5NM56+18C0jwIvbg9EJsldkjy5HTYYdjzwujQHgLcFXjaNUt+SZOMkf0uz0Z9QVbe2y357ki2T7AC8imZseFxJnpk1B52vovnj+nNV/ZImLI9tD4pu3B4Q3D/J4eMsakuaobmrk9wdePPAOu6VZN/2zexGmuGPP3etf4p98XVglyTPTbJR+++RSR4wyoO76qPZg/w/SR7Rvq47tf16Os0n4Ne269sDeCpw3ASr+QnwhyT/N82XGeYleVCSR3aU9jmab509luYYxVi9z0myoN2bubqdPNU+m66p9PXxwAvSHPTfHBjp9xVDXtP+nSyk6YuxLzscC/xzkh2TbAH8K/D5muBbcUn2TLJrOzx7Lc0Hqpnqs1lhUPSsHQ/9O+AZSf7fOPNvpTkoefeBacuAQ4D/T/OGt4LmgOt4jgRWARcC3wG+QPMGNarftOu4jGZc98VV9Yt23stpDm6vBH5A82bz8Y5lPRI4Pcl1wFLglbXmtwuvaJ/PUTRvSL8C/oHmmMmwf6M5aHgFzXGe/xyYtwFNYF0G/J7m2MXYJ9Cu9Y+kPSbyRJrx/sto+uedNAf7RzFhfVV1As3xls/RHKz+CnD3ds/yqTSf+K+gCdXnDbwOwzXeShPoD6V53a+gCaGtOuo6tq3l5Kq6YmD6XsDyts/eD+zfjsP3bip9XVXfpDn+dgrN38PYsZSpbOtfpfkSwFk0x+L+o53+cZoPbKfS9OefaLb9ifwVzd/ZtTRfDvl++/j1Vqq8cNH6JMlhNH/su0/aWFpHtXsd59B8E/AO/x5G3dyjWMcluXeSx6T5Lvj9aMb3vzzbdUlrW5rff2ySZGuaPY+vGRIzw6BY920MfIRmKONkmt3rcQ+eS31ov2F23Tj/Dpz80VPyIprfF/0KuJU1Q47qmUNPkqRO7lFIkjqtcye4mj9/fi1atGi2y5CkdcqZZ555RVUtmM5j17mgWLRoEcuWLZvtMiRpnZLk4slbjc+hJ0lSJ4NCktTJoJAkdTIoJEmdDApJUieDQpLUqbegSPLxNNefPWeC+UnygSQr0lzP+eF91SJJmr4+9yiO4fbXWBj2JGDn9t+hwId6rEWSNE29BUVVnUpzPv6J7At8qr128Gk0l7oc+XKZkqSZMZvHKLbl9tehXcXtrxF8mySHJlmWZNnq1atnpDhJUmOdOJhdVUdX1ZKqWrJgwbROVSJJmqbZDIpLuf3Fzrfj9heTlyTNAbMZFEuB57Xffno0cE1VXT6L9UiSxtHb2WOTHAvsAcxPsgp4M7ARQFV9GDgR2JvmQuk3AC/oqxZJ0vT1FhRVdcAk8wt4aV/rlyStHevEwWxJ0uwxKCRJnQwKSVIng0KS1MmgkCR1MigkSZ0MCklSJ4NCktTJoJAkdTIoJEmdDApJUieDQpLUyaCQJHUyKCRJnQwKSVIng0KS1MmgkCR1MigkSZ0MCklSJ4NCktTJoJAkdTIoJEmdDApJUieDQpLUyaCQJHUyKCRJnQwKSVIng0KS1MmgkCR1MigkSZ0MCklSJ4NCktSp16BIsleS85OsSHL4OPO3T3JKkp8mOTvJ3n3WI0maut6CIsk84CjgScBi4IAki4ea/QtwfFU9DNgf+GBf9UiSpqfPPYrdgBVVtbKqbgKOA/YdalPAXdvbWwGX9ViPJGka+gyKbYFLBu6vaqcNOgJ4TpJVwInAy8dbUJJDkyxLsmz16tV91CpJmsBsH8w+ADimqrYD9gY+neQvaqqqo6tqSVUtWbBgwYwXKUl3Zn0GxaXAwoH727XTBh0MHA9QVT8GNgXm91iTJGmK+gyKM4Cdk+yYZGOag9VLh9r8GngcQJIH0ASFY0uSNIf0FhRVdQvwMuAk4DyabzctT3Jkkn3aZq8GDknyM+BY4KCqqr5qkiRN3YZ9LryqTqQ5SD047U0Dt88FHtNnDZKkO2a2D2ZLkuY4g0KS1MmgkCR1MigkSZ0MCklSJ4NCktTJoJAkdTIoJEmdDApJUieDQpLUyaCQJHUyKCRJnQwKSVIng0KS1MmgkCR1MigkSZ0MCklSJ4NCktTJoJAkdTIoJEmdDApJUieDQpLUyaCQJHUyKCRJnQwKSVIng0KS1MmgkCR1MigkSZ0MCklSJ4NCktTJoJAkdTIoJEmdeg2KJHslOT/JiiSHT9BmvyTnJlme5HN91iNJmroN+1pwknnAUcATgFXAGUmWVtW5A212Bl4HPKaqrkpyz77qkSRNT597FLsBK6pqZVXdBBwH7DvU5hDgqKq6CqCqftdjPZKkaegzKLYFLhm4v6qdNmgXYJckP0xyWpK9eqxHkjQNvQ09TWH9OwN7ANsBpybZtaquHmyU5FDgUIDtt99+hkuUpDu3PvcoLgUWDtzfrp02aBWwtKpurqoLgQtoguN2quroqlpSVUsWLFjQW8GSpL/UZ1CcAeycZMckGwP7A0uH2nyFZm+CJPNphqJW9liTJGmKeguKqroFeBlwEnAecHxVLU9yZJJ92mYnAVcmORc4BXhNVV3ZV02SpKlLVY3WMNkM2L6qzu+3pG5LliypZcuWzWYJkrTOSXJmVS2ZzmNH2qNI8lTgLOA/2/sPTTI8jCRJWg+NOvR0BM3vIq4GqKqzgB17qUiSNKeMGhQ3V9U1Q9NGG7OSJK3TRv0dxfIkzwbmtafdeAXwo/7KkiTNFaPuUbwceCBwI/A54Brgn3qqSZI0h0y6R9Ge3O8bVbUn8Ib+S5IkzSWT7lFU1a3An5NsNQP1SJLmmFGPUVwH/DzJt4HrxyZW1St6qUqSNGeMGhRfav9Jku5kRgqKqvpke76mXdpJ51fVzf2VJUmaK0YKiiR7AJ8ELgICLEzy/Ko6tbfKJElzwqhDT+8Bnjh2nqckuwDHAo/oqzBJ0tww6u8oNho8GWBVXQBs1E9JkqS5ZNQ9imVJPgZ8pr1/IOApXCXpTmDUoDgMeCnNqTsA/gv4YC8VSZLmlFGDYkPg/VX1Xrjt19qb9FaVJGnOGPUYxXeBzQbubwZ8Z+2XI0maa0YNik2r6rqxO+3tzfspSZI0l4waFNcnefjYnSRLgD/2U5IkaS4Z9RjFK4ETklzW3r838Kx+SpIkzSWjBsWOwMOA7YGnAY/CK9xJ0p3CqENPb6yqa4G7AXvSfDX2Q30VJUmaO0YNilvb/58MfLSqvgFs3E9JkqS5ZNSguDTJR2iOS5yYZJMpPFaStA4b9c1+P+Ak4O+r6mrg7sBr+ipKkjR3jHo9ihsYuHBRVV0OXN5XUZKkucPhI0lSJ4NCktTJoJAkdTIoJEmdDApJUieDQpLUyaCQJHXqNSiS7JXk/CQrkhze0e7pSao9fbkkaQ7pLSjay6UeBTwJWAwckGTxOO22pDmN+el91SJJmr4+9yh2A1ZU1cqqugk4Dth3nHZvBd4J/KnHWiRJ09RnUGwLXDJwf1U77TbtVfMWtmejnVCSQ5MsS7Js9erVa79SSdKEZu1gdpINgPcCr56sbVUdXVVLqmrJggUL+i9OknSbPoPiUmDhwP3t2mljtgQeBHwvyUXAo4GlHtCWpLmlz6A4A9g5yY5JNgb2B5aOzayqa6pqflUtqqpFwGnAPlW1rMeaJElT1FtQVNUtwMtormNxHnB8VS1PcmSSffparyRp7RrpehTTVVUnAicOTXvTBG336LMWSdL0+MtsSVIng0KS1MmgkCR1MigkSZ0MCklSJ4NCktTJoJAkdTIoJEmdDApJUieDQpLUyaCQJHUyKCRJnQwKSVIng0KS1MmgkCR1MigkSZ0MCklSJ4NCktTJoJAkdTIoJEmdDApJUieDQpLUyaCQJHUyKCRJnQwKSVIng0KS1MmgkCR1MigkSZ0MCklSJ4NCktTJoJAkdTIoJEmdeg2KJHslOT/JiiSHjzP/VUnOTXJ2ku8m2aHPeiRJU9dbUCSZBxwFPAlYDByQZPFQs58CS6rqwcAXgHf1VY8kaXr63KPYDVhRVSur6ibgOGDfwQZVdUpV3dDePQ3Yrsd6JEnT0GdQbAtcMnB/VTttIgcD3xxvRpJDkyxLsmz16tVrsURJ0mTmxMHsJM8BlgDvHm9+VR1dVUuqasmCBQtmtjhJupPbsMdlXwosHLi/XTvtdpI8HngDsHtV3dhjPZKkaehzj+IMYOckOybZGNgfWDrYIMnDgI8A+1TV73qsRZI0Tb0FRVXdArwMOAk4Dzi+qpYnOTLJPm2zdwNbACckOSvJ0gkWJ0maJX0OPVFVJwInDk1708Dtx/e5fknSHTcnDmZLkuYug0KS1MmgkCR1MigkSZ0MCklSJ4NCktTJoJAkdTIoJEmdDApJUieDQpLUyaCQJHUyKCRJnQwKSVIng0KS1MmgkCR1MigkSZ0MCklSJ4NCktTJoJAkdTIoJEmdDApJUieDQpLUyaCQJHUyKCRJnQwKSVIng0KS1MmgkCR1MigkSZ0MCklSJ4NCktTJoJAkdTIoJEmdDApJUqdegyLJXknOT7IiyeHjzN8kyefb+acnWdRnPZKkqestKJLMA44CngQsBg5Isnio2cHAVVW1E/A+4J191SNJmp4+9yh2A1ZU1cqqugk4Dth3qM2+wCfb218AHpckPdYkSZqiDXtc9rbAJQP3VwGPmqhNVd2S5BrgHsAVg42SHAoc2t69Mck5vVS87pnPUF/didkXa9gXa9gXa9xvug/sMyjWmqo6GjgaIMmyqloyyyXNCfbFGvbFGvbFGvbFGkmWTfexfQ49XQosHLi/XTtt3DZJNgS2Aq7ssSZJ0hT1GRRnADsn2THJxsD+wNKhNkuB57e3nwGcXFXVY02SpCnqbeipPebwMuAkYB7w8apanuRIYFlVLQX+A/h0khXA72nCZDJH91XzOsi+WMO+WMO+WMO+WGPafRE/wEuSuvjLbElSJ4NCktRpzgaFp/9YY4S+eFWSc5OcneS7SXaYjTpnwmR9MdDu6UkqyXr71chR+iLJfu22sTzJ52a6xpkywt/I9klOSfLT9u9k79mos29JPp7kdxP91iyND7T9dHaSh4+04Kqac/9oDn7/CrgPsDHwM2DxUJuXAB9ub+8PfH62657FvtgT2Ly9fdiduS/adlsCpwKnAUtmu+5Z3C52Bn4KbN3ev+ds1z2LfXE0cFh7ezFw0WzX3VNfPBZ4OHDOBPP3Br4JBHg0cPooy52rexSe/mONSfuiqk6pqhvau6fR/GZlfTTKdgHwVprzhv1pJoubYaP0xSHAUVV1FUBV/W6Ga5wpo/RFAXdtb28FXDaD9c2YqjqV5hukE9kX+FQ1TgPuluTeky13rgbFeKf/2HaiNlV1CzB2+o/1zSh9Mehgmk8M66NJ+6LdlV5YVd+YycJmwSjbxS7ALkl+mOS0JHvNWHUza5S+OAJ4TpJVwInAy2emtDlnqu8nwDpyCg+NJslzgCXA7rNdy2xIsgHwXuCgWS5lrtiQZvhpD5q9zFOT7FpVV89mUbPkAOCYqnpPkv9F8/utB1XVn2e7sHXBXN2j8PQfa4zSFyR5PPAGYJ+qunGGaptpk/XFlsCDgO8luYhmDHbpenpAe5TtYhWwtKpurqoLgQtogmN9M0pfHAwcD1BVPwY2pTlh4J3NSO8nw+ZqUHj6jzUm7YskDwM+QhMS6+s4NEzSF1V1TVXNr6pFVbWI5njNPlU17ZOhzWGj/I18hWZvgiTzaYaiVs5gjTNllL74NfA4gCQPoAmK1TNa5dywFHhe++2nRwPXVNXlkz1oTg49VX+n/1jnjNgX7wa2AE5oj+f/uqr2mbWiezJiX9wpjNgXJwFPTHIucCvwmqpa7/a6R+yLVwMfTfLPNAe2D1ofP1gmOZbmw8H89njMm4GNAKrqwzTHZ/YGVgA3AC8YabnrYV9JktaiuTr0JEmaIwwKSVIng0KS1MmgkCR1MigkSZ0MCklSJ4NCmqIk2yT5wgjtrptg+jFJnrH2K5P6YVBIU1RVl1XVrLzRt6erkWaUQaH1UpJFSc5L8tH2oj3fSrLZBG2/l+SdSX6S5IIkf9tOn5fk3UnOaC/y8qKBZZ/T3t48yfHtxYG+nOYiWksGlv32JD9rz956r4HVPj7JsnZ9T2nbbprkE0l+3l5gZ892+kFJliY5GfhuknsnOTXJWUnOGatX6otBofXZzjTXY3ggcDXw9I62G1bVbsA/0Zz2AJoTyV1TVY8EHgkckmTHoce9BLiqqhYDbwQeMTDvLsBpVfUQmgspHTIwbxHNdRSeDHw4yabAS4Gqql1pznb6yXY6NBejeUZV7Q48Gzipqh4KPAQ4a9KekO4Ad2O1Pruwqs5qb59J8+Y8kS+N0+6JwIMHjidsRRM+Fww87m+A9wNU1TlJzh6YdxPw9YHlPmFg3vHtKa5/mWQlcP92Wf/eLusXSS6mOZEfwLerauyCNGcAH0+yEfCVgeco9cI9Cq3PBk+3fivdH4xuHKddgJdX1UPbfztW1bemsP6bB048N7z+4ZOsTXbStetva9hcxeyxNKeHPibJ86ZQkzRlBoU0sZOAw9pP7iTZJcldhtr8ENivnb8Y2HXEZT8zyQZJ7ktzrefzgf8CDhxbF7B9O/12kuwA/LaqPgp8jGZYSuqNQ0/SxD5GMwz13+312FcD/3uozQdpjiWcC/wCWE5zWd7J/Br4Cc11nF9cVX9K8kHgQ0l+DtxCcyrsG/OXl4LfA3hNkpuB6wD3KNQrTzMu3QFJ5gEbtW/09wW+A9yvqm6a5dKktcY9CumO2Rw4pR2eCvASQ0LrG/codKeR5CjgMUOT319Vn5iNeqR1hUEhSerkt54kSZ0MCklSJ4NCktTJoJAkdfof0ndrMRNVyUoAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "X_trn, X_tst = scale_data(X_trn, X_tst)\n",
    "# train_neural_network(X, y)\n",
    "train_kNN_estimators(X_trn, X_tst, y_trn, y_tst)\n",
    "# train_random_forests(X_trn, X_tst, y_trn, y_tst)\n",
    "clf = train_dummy_classifier(X_trn, y_trn)\n",
    "y_pred = clf.predict(X_tst)\n",
    "stats = sklearn.metrics.precision_recall_fscore_support(y_tst, y_pred, average='binary')\n",
    "sklearn.metrics.accuracy_score(y_tst, y_pred)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "PyCharm (comp432)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
