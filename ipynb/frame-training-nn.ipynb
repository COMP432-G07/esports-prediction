{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import sklearn.model_selection\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tier</th>\n",
       "      <th>game_duration</th>\n",
       "      <th>winning_team</th>\n",
       "      <th>first_champion</th>\n",
       "      <th>first_tower</th>\n",
       "      <th>first_inhibitor</th>\n",
       "      <th>first_baron</th>\n",
       "      <th>first_dragon</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>match_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>EUW1_5479661889</th>\n",
       "      <td>BRONZE</td>\n",
       "      <td>1797</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EUW1_5479575964</th>\n",
       "      <td>BRONZE</td>\n",
       "      <td>1719</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EUW1_5479499524</th>\n",
       "      <td>BRONZE</td>\n",
       "      <td>1352</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EUW1_5479492935</th>\n",
       "      <td>BRONZE</td>\n",
       "      <td>1647</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EUW1_5479357161</th>\n",
       "      <td>BRONZE</td>\n",
       "      <td>1509</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EUW1_5544360421</th>\n",
       "      <td>GRANDMASTERS</td>\n",
       "      <td>1928</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EUW1_5544335270</th>\n",
       "      <td>GRANDMASTERS</td>\n",
       "      <td>1101</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EUW1_5544282724</th>\n",
       "      <td>GRANDMASTERS</td>\n",
       "      <td>1788</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EUW1_5544046900</th>\n",
       "      <td>GRANDMASTERS</td>\n",
       "      <td>2133</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EUW1_5543889504</th>\n",
       "      <td>GRANDMASTERS</td>\n",
       "      <td>1461</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>36340 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         tier  game_duration  winning_team  first_champion  \\\n",
       "match_id                                                                     \n",
       "EUW1_5479661889        BRONZE           1797             1               1   \n",
       "EUW1_5479575964        BRONZE           1719             1               1   \n",
       "EUW1_5479499524        BRONZE           1352            -1              -1   \n",
       "EUW1_5479492935        BRONZE           1647            -1              -1   \n",
       "EUW1_5479357161        BRONZE           1509            -1              -1   \n",
       "...                       ...            ...           ...             ...   \n",
       "EUW1_5544360421  GRANDMASTERS           1928            -1               1   \n",
       "EUW1_5544335270  GRANDMASTERS           1101            -1              -1   \n",
       "EUW1_5544282724  GRANDMASTERS           1788             1              -1   \n",
       "EUW1_5544046900  GRANDMASTERS           2133             1              -1   \n",
       "EUW1_5543889504  GRANDMASTERS           1461            -1              -1   \n",
       "\n",
       "                 first_tower  first_inhibitor  first_baron  first_dragon  \n",
       "match_id                                                                  \n",
       "EUW1_5479661889            1                1            1             1  \n",
       "EUW1_5479575964           -1                1            0             1  \n",
       "EUW1_5479499524           -1               -1            0             1  \n",
       "EUW1_5479492935            1               -1            0            -1  \n",
       "EUW1_5479357161           -1               -1            0            -1  \n",
       "...                      ...              ...          ...           ...  \n",
       "EUW1_5544360421           -1               -1           -1             1  \n",
       "EUW1_5544335270           -1                0            0            -1  \n",
       "EUW1_5544282724            1                1            1            -1  \n",
       "EUW1_5544046900            1                1            1             1  \n",
       "EUW1_5543889504            1               -1           -1            -1  \n",
       "\n",
       "[36340 rows x 8 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "match_df = pd.read_csv('../output/csv/lol-data-matches-fixed-duration.csv')\n",
    "frame_df = pd.read_csv('../output/csv/lol-data-match-frames.csv')\n",
    "match_df = match_df.set_index('match_id')\n",
    "match_df.drop(labels=['count','division','patch','region','first_rift_herald'], axis=1, inplace=True)\n",
    "match_df.fillna(0, inplace=True)\n",
    "match_df = match_df[match_df.winning_team!=0]\n",
    "match_df.replace({\n",
    "    'winning_team': {100: 1, 200: -1},\n",
    "    'first_champion': {100: 1, 200: -1},\n",
    "    'first_tower': {100: 1, 200: -1},\n",
    "    'first_inhibitor': {100: 1, 200: -1},\n",
    "    'first_baron': {100: 1, 200: -1},\n",
    "    'first_dragon': {100: 1, 200: -1},\n",
    "}, inplace=True)\n",
    "\n",
    "match_df = match_df.astype({\n",
    "    'winning_team': 'int32',\n",
    "    'first_champion': 'int32',\n",
    "    'first_tower': 'int32',\n",
    "    'first_inhibitor': 'int32',\n",
    "    'first_baron': 'int32',\n",
    "    'first_dragon': 'int32',\n",
    "})\n",
    "match_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = match_df.merge(frame_df,left_on='match_id', right_on='match_id').set_index('match_id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_neural_network(X, y):\n",
    "\n",
    "    # Split the data 2/3 to 1/3\n",
    "    X_trn, X_tst, y_trn, y_tst = sklearn.model_selection.train_test_split(X, y, test_size=0.33, random_state=0)\n",
    "    # Scale the data with MinMax to avoid negative values\n",
    "    scaler = sklearn.preprocessing.MinMaxScaler()\n",
    "    scaler.fit(X_trn)\n",
    "    X_trn = scaler.transform(X_trn)\n",
    "    X_tst = scaler.transform(X_tst)\n",
    "\n",
    "    # Tensors setup\n",
    "    X_trn_torch = torch.tensor(X_trn, dtype=torch.float32)\n",
    "    y_trn_torch = torch.tensor(y_trn, dtype=torch.int64)\n",
    "    X_tst_torch = torch.tensor(X_tst, dtype=torch.float32)\n",
    "    y_tst_torch = torch.tensor(y_tst, dtype=torch.int64)\n",
    "\n",
    "    torch.manual_seed(0) # Ensure model weights initialized with same random numbers\n",
    "\n",
    "    # Create an object that holds a sequence of layers and activation functions\n",
    "    model = torch.nn.Sequential(\n",
    "        torch.nn.Linear(10, 2)   # Applies Wx+b from 10 dimensions down to 2\n",
    "    )\n",
    "\n",
    "    # Create an object that can compute \"negative log likelihood of a softmax\"\n",
    "    loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # Use stochastic gradient descent to train the model\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "    # Use 100 training samples at a time to compute the gradient.\n",
    "    batch_size = 100\n",
    "\n",
    "    # Make 10 passes over the training data, each time using batch_size samples to compute gradient\n",
    "    num_epoch = 10\n",
    "    next_epoch = 1\n",
    "\n",
    "    for epoch in range(next_epoch, next_epoch+num_epoch):\n",
    "        # Make an entire pass (an 'epoch') over the training data in batch_size chunks\n",
    "        for i in range(0, len(X_trn), batch_size):        \n",
    "            X_cur = X_trn_torch[i:i+batch_size]     # Slice out a mini-batch of features\n",
    "            y_cur = y_trn_torch[i:i+batch_size]     # Slice out a mini-batch of targets\n",
    "            \n",
    "            y_pred = model(X_cur)                   # Make predictions (final-layer activations)\n",
    "            l = loss(y_pred, y_cur)                 # Compute loss with respect to predictions\n",
    "            \n",
    "            model.zero_grad()                   # Reset all gradient accumulators to zero (PyTorch thing)\n",
    "            l.backward()                        # Compute gradient of loss wrt all parameters (backprop!)\n",
    "            optimizer.step()                    # Use the gradients to take a step with SGD.\n",
    "            \n",
    "        print(\"Epoch %2d: loss on final training batch: %.4f\" % (epoch, l.item()))\n",
    "        \n",
    "    print(\"Epoch %2d: loss on test set: %.4f\" % (epoch, loss(model(X_tst_torch), y_tst_torch)))\n",
    "    next_epoch = epoch+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING ON FRAME #0\n",
      "Epoch  1: loss on final training batch: 0.6921\n",
      "Epoch  2: loss on final training batch: 0.6921\n",
      "Epoch  3: loss on final training batch: 0.6921\n",
      "Epoch  4: loss on final training batch: 0.6921\n",
      "Epoch  5: loss on final training batch: 0.6921\n",
      "Epoch  6: loss on final training batch: 0.6921\n",
      "Epoch  7: loss on final training batch: 0.6921\n",
      "Epoch  8: loss on final training batch: 0.6921\n",
      "Epoch  9: loss on final training batch: 0.6921\n",
      "Epoch 10: loss on final training batch: 0.6921\n",
      "Epoch 10: loss on test set: 0.6932\n",
      "TRAINING ON FRAME #1\n",
      "Epoch  1: loss on final training batch: 0.6911\n",
      "Epoch  2: loss on final training batch: 0.6902\n",
      "Epoch  3: loss on final training batch: 0.6897\n",
      "Epoch  4: loss on final training batch: 0.6894\n",
      "Epoch  5: loss on final training batch: 0.6892\n",
      "Epoch  6: loss on final training batch: 0.6890\n",
      "Epoch  7: loss on final training batch: 0.6889\n",
      "Epoch  8: loss on final training batch: 0.6888\n",
      "Epoch  9: loss on final training batch: 0.6887\n",
      "Epoch 10: loss on final training batch: 0.6886\n",
      "Epoch 10: loss on test set: 0.6922\n",
      "TRAINING ON FRAME #2\n",
      "Epoch  1: loss on final training batch: 0.6869\n",
      "Epoch  2: loss on final training batch: 0.6840\n",
      "Epoch  3: loss on final training batch: 0.6822\n",
      "Epoch  4: loss on final training batch: 0.6812\n",
      "Epoch  5: loss on final training batch: 0.6806\n",
      "Epoch  6: loss on final training batch: 0.6803\n",
      "Epoch  7: loss on final training batch: 0.6801\n",
      "Epoch  8: loss on final training batch: 0.6800\n",
      "Epoch  9: loss on final training batch: 0.6799\n",
      "Epoch 10: loss on final training batch: 0.6799\n",
      "Epoch 10: loss on test set: 0.6869\n",
      "TRAINING ON FRAME #3\n",
      "Epoch  1: loss on final training batch: 0.6870\n",
      "Epoch  2: loss on final training batch: 0.6826\n",
      "Epoch  3: loss on final training batch: 0.6794\n",
      "Epoch  4: loss on final training batch: 0.6772\n",
      "Epoch  5: loss on final training batch: 0.6755\n",
      "Epoch  6: loss on final training batch: 0.6742\n",
      "Epoch  7: loss on final training batch: 0.6733\n",
      "Epoch  8: loss on final training batch: 0.6726\n",
      "Epoch  9: loss on final training batch: 0.6720\n",
      "Epoch 10: loss on final training batch: 0.6716\n",
      "Epoch 10: loss on test set: 0.6698\n",
      "TRAINING ON FRAME #4\n",
      "Epoch  1: loss on final training batch: 0.6772\n",
      "Epoch  2: loss on final training batch: 0.6645\n",
      "Epoch  3: loss on final training batch: 0.6546\n",
      "Epoch  4: loss on final training batch: 0.6466\n",
      "Epoch  5: loss on final training batch: 0.6400\n",
      "Epoch  6: loss on final training batch: 0.6346\n",
      "Epoch  7: loss on final training batch: 0.6302\n",
      "Epoch  8: loss on final training batch: 0.6265\n",
      "Epoch  9: loss on final training batch: 0.6233\n",
      "Epoch 10: loss on final training batch: 0.6207\n",
      "Epoch 10: loss on test set: 0.6450\n",
      "TRAINING ON FRAME #5\n",
      "Epoch  1: loss on final training batch: 0.6794\n",
      "Epoch  2: loss on final training batch: 0.6738\n",
      "Epoch  3: loss on final training batch: 0.6717\n",
      "Epoch  4: loss on final training batch: 0.6706\n",
      "Epoch  5: loss on final training batch: 0.6701\n",
      "Epoch  6: loss on final training batch: 0.6701\n",
      "Epoch  7: loss on final training batch: 0.6703\n",
      "Epoch  8: loss on final training batch: 0.6708\n",
      "Epoch  9: loss on final training batch: 0.6713\n",
      "Epoch 10: loss on final training batch: 0.6720\n",
      "Epoch 10: loss on test set: 0.6341\n",
      "TRAINING ON FRAME #6\n",
      "Epoch  1: loss on final training batch: 0.6760\n",
      "Epoch  2: loss on final training batch: 0.6663\n",
      "Epoch  3: loss on final training batch: 0.6600\n",
      "Epoch  4: loss on final training batch: 0.6543\n",
      "Epoch  5: loss on final training batch: 0.6491\n",
      "Epoch  6: loss on final training batch: 0.6444\n",
      "Epoch  7: loss on final training batch: 0.6402\n",
      "Epoch  8: loss on final training batch: 0.6364\n",
      "Epoch  9: loss on final training batch: 0.6331\n",
      "Epoch 10: loss on final training batch: 0.6301\n",
      "Epoch 10: loss on test set: 0.6246\n",
      "TRAINING ON FRAME #7\n",
      "Epoch  1: loss on final training batch: 0.6513\n",
      "Epoch  2: loss on final training batch: 0.6352\n",
      "Epoch  3: loss on final training batch: 0.6270\n",
      "Epoch  4: loss on final training batch: 0.6215\n",
      "Epoch  5: loss on final training batch: 0.6173\n",
      "Epoch  6: loss on final training batch: 0.6139\n",
      "Epoch  7: loss on final training batch: 0.6111\n",
      "Epoch  8: loss on final training batch: 0.6087\n",
      "Epoch  9: loss on final training batch: 0.6067\n",
      "Epoch 10: loss on final training batch: 0.6049\n",
      "Epoch 10: loss on test set: 0.6094\n",
      "TRAINING ON FRAME #8\n",
      "Epoch  1: loss on final training batch: 0.6663\n",
      "Epoch  2: loss on final training batch: 0.6566\n",
      "Epoch  3: loss on final training batch: 0.6510\n",
      "Epoch  4: loss on final training batch: 0.6472\n",
      "Epoch  5: loss on final training batch: 0.6443\n",
      "Epoch  6: loss on final training batch: 0.6420\n",
      "Epoch  7: loss on final training batch: 0.6398\n",
      "Epoch  8: loss on final training batch: 0.6379\n",
      "Epoch  9: loss on final training batch: 0.6362\n",
      "Epoch 10: loss on final training batch: 0.6346\n",
      "Epoch 10: loss on test set: 0.5983\n",
      "TRAINING ON FRAME #9\n",
      "Epoch  1: loss on final training batch: 0.6503\n",
      "Epoch  2: loss on final training batch: 0.6478\n",
      "Epoch  3: loss on final training batch: 0.6486\n",
      "Epoch  4: loss on final training batch: 0.6499\n",
      "Epoch  5: loss on final training batch: 0.6511\n",
      "Epoch  6: loss on final training batch: 0.6520\n",
      "Epoch  7: loss on final training batch: 0.6525\n",
      "Epoch  8: loss on final training batch: 0.6527\n",
      "Epoch  9: loss on final training batch: 0.6527\n",
      "Epoch 10: loss on final training batch: 0.6524\n",
      "Epoch 10: loss on test set: 0.5827\n",
      "TRAINING ON FRAME #10\n",
      "Epoch  1: loss on final training batch: 0.6319\n",
      "Epoch  2: loss on final training batch: 0.6319\n",
      "Epoch  3: loss on final training batch: 0.6357\n",
      "Epoch  4: loss on final training batch: 0.6399\n",
      "Epoch  5: loss on final training batch: 0.6439\n",
      "Epoch  6: loss on final training batch: 0.6473\n",
      "Epoch  7: loss on final training batch: 0.6501\n",
      "Epoch  8: loss on final training batch: 0.6524\n",
      "Epoch  9: loss on final training batch: 0.6542\n",
      "Epoch 10: loss on final training batch: 0.6556\n",
      "Epoch 10: loss on test set: 0.5652\n",
      "TRAINING ON FRAME #11\n",
      "Epoch  1: loss on final training batch: 0.5706\n",
      "Epoch  2: loss on final training batch: 0.5475\n",
      "Epoch  3: loss on final training batch: 0.5387\n",
      "Epoch  4: loss on final training batch: 0.5346\n",
      "Epoch  5: loss on final training batch: 0.5324\n",
      "Epoch  6: loss on final training batch: 0.5312\n",
      "Epoch  7: loss on final training batch: 0.5303\n",
      "Epoch  8: loss on final training batch: 0.5297\n",
      "Epoch  9: loss on final training batch: 0.5293\n",
      "Epoch 10: loss on final training batch: 0.5289\n",
      "Epoch 10: loss on test set: 0.5506\n",
      "TRAINING ON FRAME #12\n",
      "Epoch  1: loss on final training batch: 0.5565\n",
      "Epoch  2: loss on final training batch: 0.5270\n",
      "Epoch  3: loss on final training batch: 0.5142\n",
      "Epoch  4: loss on final training batch: 0.5060\n",
      "Epoch  5: loss on final training batch: 0.4999\n",
      "Epoch  6: loss on final training batch: 0.4950\n",
      "Epoch  7: loss on final training batch: 0.4909\n",
      "Epoch  8: loss on final training batch: 0.4874\n",
      "Epoch  9: loss on final training batch: 0.4844\n",
      "Epoch 10: loss on final training batch: 0.4818\n",
      "Epoch 10: loss on test set: 0.5329\n",
      "TRAINING ON FRAME #13\n",
      "Epoch  1: loss on final training batch: 0.5322\n",
      "Epoch  2: loss on final training batch: 0.5203\n",
      "Epoch  3: loss on final training batch: 0.5182\n",
      "Epoch  4: loss on final training batch: 0.5170\n",
      "Epoch  5: loss on final training batch: 0.5156\n",
      "Epoch  6: loss on final training batch: 0.5141\n",
      "Epoch  7: loss on final training batch: 0.5126\n",
      "Epoch  8: loss on final training batch: 0.5113\n",
      "Epoch  9: loss on final training batch: 0.5101\n",
      "Epoch 10: loss on final training batch: 0.5091\n",
      "Epoch 10: loss on test set: 0.5149\n",
      "TRAINING ON FRAME #14\n",
      "Epoch  1: loss on final training batch: 0.5418\n",
      "Epoch  2: loss on final training batch: 0.5397\n",
      "Epoch  3: loss on final training batch: 0.5435\n",
      "Epoch  4: loss on final training batch: 0.5464\n",
      "Epoch  5: loss on final training batch: 0.5483\n",
      "Epoch  6: loss on final training batch: 0.5494\n",
      "Epoch  7: loss on final training batch: 0.5501\n",
      "Epoch  8: loss on final training batch: 0.5505\n",
      "Epoch  9: loss on final training batch: 0.5506\n",
      "Epoch 10: loss on final training batch: 0.5506\n",
      "Epoch 10: loss on test set: 0.5051\n",
      "TRAINING ON FRAME #15\n",
      "Epoch  1: loss on final training batch: 0.4599\n",
      "Epoch  2: loss on final training batch: 0.4412\n",
      "Epoch  3: loss on final training batch: 0.4341\n",
      "Epoch  4: loss on final training batch: 0.4294\n",
      "Epoch  5: loss on final training batch: 0.4258\n",
      "Epoch  6: loss on final training batch: 0.4230\n",
      "Epoch  7: loss on final training batch: 0.4208\n",
      "Epoch  8: loss on final training batch: 0.4190\n",
      "Epoch  9: loss on final training batch: 0.4176\n",
      "Epoch 10: loss on final training batch: 0.4165\n",
      "Epoch 10: loss on test set: 0.4812\n",
      "TRAINING ON FRAME #16\n",
      "Epoch  1: loss on final training batch: 0.5079\n",
      "Epoch  2: loss on final training batch: 0.5012\n",
      "Epoch  3: loss on final training batch: 0.4970\n",
      "Epoch  4: loss on final training batch: 0.4942\n",
      "Epoch  5: loss on final training batch: 0.4921\n",
      "Epoch  6: loss on final training batch: 0.4905\n",
      "Epoch  7: loss on final training batch: 0.4892\n",
      "Epoch  8: loss on final training batch: 0.4879\n",
      "Epoch  9: loss on final training batch: 0.4867\n",
      "Epoch 10: loss on final training batch: 0.4856\n",
      "Epoch 10: loss on test set: 0.4715\n",
      "TRAINING ON FRAME #17\n",
      "Epoch  1: loss on final training batch: 0.5021\n",
      "Epoch  2: loss on final training batch: 0.4937\n",
      "Epoch  3: loss on final training batch: 0.4884\n",
      "Epoch  4: loss on final training batch: 0.4834\n",
      "Epoch  5: loss on final training batch: 0.4786\n",
      "Epoch  6: loss on final training batch: 0.4744\n",
      "Epoch  7: loss on final training batch: 0.4707\n",
      "Epoch  8: loss on final training batch: 0.4675\n",
      "Epoch  9: loss on final training batch: 0.4646\n",
      "Epoch 10: loss on final training batch: 0.4622\n",
      "Epoch 10: loss on test set: 0.4675\n",
      "TRAINING ON FRAME #18\n",
      "Epoch  1: loss on final training batch: 0.4490\n",
      "Epoch  2: loss on final training batch: 0.4406\n",
      "Epoch  3: loss on final training batch: 0.4389\n",
      "Epoch  4: loss on final training batch: 0.4375\n",
      "Epoch  5: loss on final training batch: 0.4361\n",
      "Epoch  6: loss on final training batch: 0.4346\n",
      "Epoch  7: loss on final training batch: 0.4331\n",
      "Epoch  8: loss on final training batch: 0.4317\n",
      "Epoch  9: loss on final training batch: 0.4303\n",
      "Epoch 10: loss on final training batch: 0.4289\n",
      "Epoch 10: loss on test set: 0.4545\n",
      "TRAINING ON FRAME #19\n",
      "Epoch  1: loss on final training batch: 0.4773\n",
      "Epoch  2: loss on final training batch: 0.4693\n",
      "Epoch  3: loss on final training batch: 0.4643\n",
      "Epoch  4: loss on final training batch: 0.4599\n",
      "Epoch  5: loss on final training batch: 0.4563\n",
      "Epoch  6: loss on final training batch: 0.4533\n",
      "Epoch  7: loss on final training batch: 0.4507\n",
      "Epoch  8: loss on final training batch: 0.4485\n",
      "Epoch  9: loss on final training batch: 0.4466\n",
      "Epoch 10: loss on final training batch: 0.4450\n",
      "Epoch 10: loss on test set: 0.4446\n",
      "TRAINING ON FRAME #20\n",
      "Epoch  1: loss on final training batch: 0.3286\n",
      "Epoch  2: loss on final training batch: 0.3053\n",
      "Epoch  3: loss on final training batch: 0.2997\n",
      "Epoch  4: loss on final training batch: 0.2971\n",
      "Epoch  5: loss on final training batch: 0.2955\n",
      "Epoch  6: loss on final training batch: 0.2943\n",
      "Epoch  7: loss on final training batch: 0.2933\n",
      "Epoch  8: loss on final training batch: 0.2924\n",
      "Epoch  9: loss on final training batch: 0.2916\n",
      "Epoch 10: loss on final training batch: 0.2908\n",
      "Epoch 10: loss on test set: 0.4360\n",
      "TRAINING ON FRAME #21\n",
      "Epoch  1: loss on final training batch: 0.4817\n",
      "Epoch  2: loss on final training batch: 0.4745\n",
      "Epoch  3: loss on final training batch: 0.4714\n",
      "Epoch  4: loss on final training batch: 0.4687\n",
      "Epoch  5: loss on final training batch: 0.4663\n",
      "Epoch  6: loss on final training batch: 0.4643\n",
      "Epoch  7: loss on final training batch: 0.4626\n",
      "Epoch  8: loss on final training batch: 0.4612\n",
      "Epoch  9: loss on final training batch: 0.4600\n",
      "Epoch 10: loss on final training batch: 0.4591\n",
      "Epoch 10: loss on test set: 0.4231\n",
      "TRAINING ON FRAME #22\n",
      "Epoch  1: loss on final training batch: 0.5437\n",
      "Epoch  2: loss on final training batch: 0.5368\n",
      "Epoch  3: loss on final training batch: 0.5347\n",
      "Epoch  4: loss on final training batch: 0.5333\n",
      "Epoch  5: loss on final training batch: 0.5322\n",
      "Epoch  6: loss on final training batch: 0.5313\n",
      "Epoch  7: loss on final training batch: 0.5306\n",
      "Epoch  8: loss on final training batch: 0.5300\n",
      "Epoch  9: loss on final training batch: 0.5295\n",
      "Epoch 10: loss on final training batch: 0.5291\n",
      "Epoch 10: loss on test set: 0.4196\n",
      "TRAINING ON FRAME #23\n",
      "Epoch  1: loss on final training batch: 0.4439\n",
      "Epoch  2: loss on final training batch: 0.4218\n",
      "Epoch  3: loss on final training batch: 0.4141\n",
      "Epoch  4: loss on final training batch: 0.4093\n",
      "Epoch  5: loss on final training batch: 0.4057\n",
      "Epoch  6: loss on final training batch: 0.4026\n",
      "Epoch  7: loss on final training batch: 0.3998\n",
      "Epoch  8: loss on final training batch: 0.3974\n",
      "Epoch  9: loss on final training batch: 0.3952\n",
      "Epoch 10: loss on final training batch: 0.3932\n",
      "Epoch 10: loss on test set: 0.4062\n",
      "TRAINING ON FRAME #24\n",
      "Epoch  1: loss on final training batch: 0.4342\n",
      "Epoch  2: loss on final training batch: 0.3940\n",
      "Epoch  3: loss on final training batch: 0.3785\n",
      "Epoch  4: loss on final training batch: 0.3704\n",
      "Epoch  5: loss on final training batch: 0.3654\n",
      "Epoch  6: loss on final training batch: 0.3619\n",
      "Epoch  7: loss on final training batch: 0.3594\n",
      "Epoch  8: loss on final training batch: 0.3574\n",
      "Epoch  9: loss on final training batch: 0.3558\n",
      "Epoch 10: loss on final training batch: 0.3544\n",
      "Epoch 10: loss on test set: 0.4041\n",
      "TRAINING ON FRAME #25\n",
      "Epoch  1: loss on final training batch: 0.4112\n",
      "Epoch  2: loss on final training batch: 0.3749\n",
      "Epoch  3: loss on final training batch: 0.3640\n",
      "Epoch  4: loss on final training batch: 0.3598\n",
      "Epoch  5: loss on final training batch: 0.3580\n",
      "Epoch  6: loss on final training batch: 0.3573\n",
      "Epoch  7: loss on final training batch: 0.3572\n",
      "Epoch  8: loss on final training batch: 0.3573\n",
      "Epoch  9: loss on final training batch: 0.3575\n",
      "Epoch 10: loss on final training batch: 0.3578\n",
      "Epoch 10: loss on test set: 0.4074\n",
      "TRAINING ON FRAME #26\n",
      "Epoch  1: loss on final training batch: 0.4655\n",
      "Epoch  2: loss on final training batch: 0.4370\n",
      "Epoch  3: loss on final training batch: 0.4253\n",
      "Epoch  4: loss on final training batch: 0.4178\n",
      "Epoch  5: loss on final training batch: 0.4121\n",
      "Epoch  6: loss on final training batch: 0.4073\n",
      "Epoch  7: loss on final training batch: 0.4031\n",
      "Epoch  8: loss on final training batch: 0.3993\n",
      "Epoch  9: loss on final training batch: 0.3959\n",
      "Epoch 10: loss on final training batch: 0.3927\n",
      "Epoch 10: loss on test set: 0.4033\n",
      "TRAINING ON FRAME #27\n",
      "Epoch  1: loss on final training batch: 0.4512\n",
      "Epoch  2: loss on final training batch: 0.4215\n",
      "Epoch  3: loss on final training batch: 0.4120\n",
      "Epoch  4: loss on final training batch: 0.4071\n",
      "Epoch  5: loss on final training batch: 0.4038\n",
      "Epoch  6: loss on final training batch: 0.4011\n",
      "Epoch  7: loss on final training batch: 0.3989\n",
      "Epoch  8: loss on final training batch: 0.3968\n",
      "Epoch  9: loss on final training batch: 0.3950\n",
      "Epoch 10: loss on final training batch: 0.3933\n",
      "Epoch 10: loss on test set: 0.4027\n",
      "TRAINING ON FRAME #28\n",
      "Epoch  1: loss on final training batch: 0.5094\n",
      "Epoch  2: loss on final training batch: 0.4821\n",
      "Epoch  3: loss on final training batch: 0.4723\n",
      "Epoch  4: loss on final training batch: 0.4668\n",
      "Epoch  5: loss on final training batch: 0.4631\n",
      "Epoch  6: loss on final training batch: 0.4601\n",
      "Epoch  7: loss on final training batch: 0.4576\n",
      "Epoch  8: loss on final training batch: 0.4553\n",
      "Epoch  9: loss on final training batch: 0.4533\n",
      "Epoch 10: loss on final training batch: 0.4515\n",
      "Epoch 10: loss on test set: 0.4189\n",
      "TRAINING ON FRAME #29\n",
      "Epoch  1: loss on final training batch: 0.4409\n",
      "Epoch  2: loss on final training batch: 0.3882\n",
      "Epoch  3: loss on final training batch: 0.3697\n",
      "Epoch  4: loss on final training batch: 0.3606\n",
      "Epoch  5: loss on final training batch: 0.3551\n",
      "Epoch  6: loss on final training batch: 0.3511\n",
      "Epoch  7: loss on final training batch: 0.3480\n",
      "Epoch  8: loss on final training batch: 0.3453\n",
      "Epoch  9: loss on final training batch: 0.3429\n",
      "Epoch 10: loss on final training batch: 0.3407\n",
      "Epoch 10: loss on test set: 0.4124\n",
      "TRAINING ON FRAME #30\n",
      "Epoch  1: loss on final training batch: 0.4536\n",
      "Epoch  2: loss on final training batch: 0.3983\n",
      "Epoch  3: loss on final training batch: 0.3746\n",
      "Epoch  4: loss on final training batch: 0.3604\n",
      "Epoch  5: loss on final training batch: 0.3505\n",
      "Epoch  6: loss on final training batch: 0.3430\n",
      "Epoch  7: loss on final training batch: 0.3369\n",
      "Epoch  8: loss on final training batch: 0.3318\n",
      "Epoch  9: loss on final training batch: 0.3273\n",
      "Epoch 10: loss on final training batch: 0.3234\n",
      "Epoch 10: loss on test set: 0.4311\n",
      "TRAINING ON FRAME #31\n",
      "Epoch  1: loss on final training batch: 0.4282\n",
      "Epoch  2: loss on final training batch: 0.3701\n",
      "Epoch  3: loss on final training batch: 0.3516\n",
      "Epoch  4: loss on final training batch: 0.3433\n",
      "Epoch  5: loss on final training batch: 0.3389\n",
      "Epoch  6: loss on final training batch: 0.3363\n",
      "Epoch  7: loss on final training batch: 0.3347\n",
      "Epoch  8: loss on final training batch: 0.3337\n",
      "Epoch  9: loss on final training batch: 0.3330\n",
      "Epoch 10: loss on final training batch: 0.3326\n",
      "Epoch 10: loss on test set: 0.4472\n",
      "TRAINING ON FRAME #32\n",
      "Epoch  1: loss on final training batch: 0.5560\n",
      "Epoch  2: loss on final training batch: 0.5361\n",
      "Epoch  3: loss on final training batch: 0.5347\n",
      "Epoch  4: loss on final training batch: 0.5355\n",
      "Epoch  5: loss on final training batch: 0.5363\n",
      "Epoch  6: loss on final training batch: 0.5367\n",
      "Epoch  7: loss on final training batch: 0.5368\n",
      "Epoch  8: loss on final training batch: 0.5366\n",
      "Epoch  9: loss on final training batch: 0.5362\n",
      "Epoch 10: loss on final training batch: 0.5356\n",
      "Epoch 10: loss on test set: 0.4460\n",
      "TRAINING ON FRAME #33\n",
      "Epoch  1: loss on final training batch: 0.6531\n",
      "Epoch  2: loss on final training batch: 0.6448\n",
      "Epoch  3: loss on final training batch: 0.6425\n",
      "Epoch  4: loss on final training batch: 0.6407\n",
      "Epoch  5: loss on final training batch: 0.6386\n",
      "Epoch  6: loss on final training batch: 0.6365\n",
      "Epoch  7: loss on final training batch: 0.6343\n",
      "Epoch  8: loss on final training batch: 0.6322\n",
      "Epoch  9: loss on final training batch: 0.6302\n",
      "Epoch 10: loss on final training batch: 0.6284\n",
      "Epoch 10: loss on test set: 0.4778\n",
      "TRAINING ON FRAME #34\n",
      "Epoch  1: loss on final training batch: 0.6247\n",
      "Epoch  2: loss on final training batch: 0.6067\n",
      "Epoch  3: loss on final training batch: 0.6101\n",
      "Epoch  4: loss on final training batch: 0.6162\n",
      "Epoch  5: loss on final training batch: 0.6218\n",
      "Epoch  6: loss on final training batch: 0.6264\n",
      "Epoch  7: loss on final training batch: 0.6300\n",
      "Epoch  8: loss on final training batch: 0.6327\n",
      "Epoch  9: loss on final training batch: 0.6347\n",
      "Epoch 10: loss on final training batch: 0.6361\n",
      "Epoch 10: loss on test set: 0.4865\n",
      "TRAINING ON FRAME #35\n",
      "Epoch  1: loss on final training batch: 0.6497\n",
      "Epoch  2: loss on final training batch: 0.6277\n",
      "Epoch  3: loss on final training batch: 0.6208\n",
      "Epoch  4: loss on final training batch: 0.6162\n",
      "Epoch  5: loss on final training batch: 0.6120\n",
      "Epoch  6: loss on final training batch: 0.6078\n",
      "Epoch  7: loss on final training batch: 0.6037\n",
      "Epoch  8: loss on final training batch: 0.5997\n",
      "Epoch  9: loss on final training batch: 0.5959\n",
      "Epoch 10: loss on final training batch: 0.5923\n",
      "Epoch 10: loss on test set: 0.5262\n",
      "TRAINING ON FRAME #36\n",
      "Epoch  1: loss on final training batch: 0.6181\n",
      "Epoch  2: loss on final training batch: 0.5681\n",
      "Epoch  3: loss on final training batch: 0.5374\n",
      "Epoch  4: loss on final training batch: 0.5187\n",
      "Epoch  5: loss on final training batch: 0.5063\n",
      "Epoch  6: loss on final training batch: 0.4973\n",
      "Epoch  7: loss on final training batch: 0.4906\n",
      "Epoch  8: loss on final training batch: 0.4854\n",
      "Epoch  9: loss on final training batch: 0.4813\n",
      "Epoch 10: loss on final training batch: 0.4780\n",
      "Epoch 10: loss on test set: 0.5259\n",
      "TRAINING ON FRAME #37\n",
      "Epoch  1: loss on final training batch: 0.6549\n",
      "Epoch  2: loss on final training batch: 0.6072\n",
      "Epoch  3: loss on final training batch: 0.5834\n",
      "Epoch  4: loss on final training batch: 0.5698\n",
      "Epoch  5: loss on final training batch: 0.5612\n",
      "Epoch  6: loss on final training batch: 0.5551\n",
      "Epoch  7: loss on final training batch: 0.5505\n",
      "Epoch  8: loss on final training batch: 0.5469\n",
      "Epoch  9: loss on final training batch: 0.5441\n",
      "Epoch 10: loss on final training batch: 0.5417\n",
      "Epoch 10: loss on test set: 0.5539\n",
      "TRAINING ON FRAME #38\n",
      "Epoch  1: loss on final training batch: 0.6781\n",
      "Epoch  2: loss on final training batch: 0.6521\n",
      "Epoch  3: loss on final training batch: 0.6192\n",
      "Epoch  4: loss on final training batch: 0.6031\n",
      "Epoch  5: loss on final training batch: 0.5909\n",
      "Epoch  6: loss on final training batch: 0.5820\n",
      "Epoch  7: loss on final training batch: 0.5750\n",
      "Epoch  8: loss on final training batch: 0.5693\n",
      "Epoch  9: loss on final training batch: 0.5644\n",
      "Epoch 10: loss on final training batch: 0.5601\n",
      "Epoch 10: loss on test set: 0.5733\n",
      "TRAINING ON FRAME #39\n",
      "Epoch  1: loss on final training batch: 0.6831\n",
      "Epoch  2: loss on final training batch: 0.5958\n",
      "Epoch  3: loss on final training batch: 0.5395\n",
      "Epoch  4: loss on final training batch: 0.5023\n",
      "Epoch  5: loss on final training batch: 0.4768\n",
      "Epoch  6: loss on final training batch: 0.4585\n",
      "Epoch  7: loss on final training batch: 0.4445\n",
      "Epoch  8: loss on final training batch: 0.4336\n",
      "Epoch  9: loss on final training batch: 0.4246\n",
      "Epoch 10: loss on final training batch: 0.4172\n",
      "Epoch 10: loss on test set: 0.5726\n",
      "TRAINING ON FRAME #40\n",
      "Epoch  1: loss on final training batch: 0.6986\n",
      "Epoch  2: loss on final training batch: 0.6782\n",
      "Epoch  3: loss on final training batch: 0.6622\n",
      "Epoch  4: loss on final training batch: 0.6517\n",
      "Epoch  5: loss on final training batch: 0.6445\n",
      "Epoch  6: loss on final training batch: 0.6394\n",
      "Epoch  7: loss on final training batch: 0.6358\n",
      "Epoch  8: loss on final training batch: 0.6331\n",
      "Epoch  9: loss on final training batch: 0.6309\n",
      "Epoch 10: loss on final training batch: 0.6292\n",
      "Epoch 10: loss on test set: 0.5852\n",
      "TRAINING ON FRAME #41\n",
      "Epoch  1: loss on final training batch: 0.7107\n",
      "Epoch  2: loss on final training batch: 0.6950\n",
      "Epoch  3: loss on final training batch: 0.6632\n",
      "Epoch  4: loss on final training batch: 0.6442\n",
      "Epoch  5: loss on final training batch: 0.6310\n",
      "Epoch  6: loss on final training batch: 0.6185\n",
      "Epoch  7: loss on final training batch: 0.6081\n",
      "Epoch  8: loss on final training batch: 0.5999\n",
      "Epoch  9: loss on final training batch: 0.5929\n",
      "Epoch 10: loss on final training batch: 0.5869\n",
      "Epoch 10: loss on test set: 0.6160\n",
      "TRAINING ON FRAME #42\n",
      "Epoch  1: loss on final training batch: 0.7039\n",
      "Epoch  2: loss on final training batch: 0.6812\n",
      "Epoch  3: loss on final training batch: 0.6658\n",
      "Epoch  4: loss on final training batch: 0.6536\n",
      "Epoch  5: loss on final training batch: 0.6416\n",
      "Epoch  6: loss on final training batch: 0.6306\n",
      "Epoch  7: loss on final training batch: 0.6217\n",
      "Epoch  8: loss on final training batch: 0.6143\n",
      "Epoch  9: loss on final training batch: 0.6078\n",
      "Epoch 10: loss on final training batch: 0.6020\n",
      "Epoch 10: loss on test set: 0.6229\n",
      "TRAINING ON FRAME #43\n",
      "Epoch  1: loss on final training batch: 0.7086\n",
      "Epoch  2: loss on final training batch: 0.6942\n",
      "Epoch  3: loss on final training batch: 0.6859\n",
      "Epoch  4: loss on final training batch: 0.6781\n",
      "Epoch  5: loss on final training batch: 0.6713\n",
      "Epoch  6: loss on final training batch: 0.6652\n",
      "Epoch  7: loss on final training batch: 0.6597\n",
      "Epoch  8: loss on final training batch: 0.6551\n",
      "Epoch  9: loss on final training batch: 0.6510\n",
      "Epoch 10: loss on final training batch: 0.6474\n",
      "Epoch 10: loss on test set: 0.6319\n",
      "TRAINING ON FRAME #44\n",
      "Epoch  1: loss on final training batch: 0.7076\n",
      "Epoch  2: loss on final training batch: 0.6950\n",
      "Epoch  3: loss on final training batch: 0.6889\n",
      "Epoch  4: loss on final training batch: 0.6843\n",
      "Epoch  5: loss on final training batch: 0.6803\n",
      "Epoch  6: loss on final training batch: 0.6772\n",
      "Epoch  7: loss on final training batch: 0.6744\n",
      "Epoch  8: loss on final training batch: 0.6717\n",
      "Epoch  9: loss on final training batch: 0.6694\n",
      "Epoch 10: loss on final training batch: 0.6677\n",
      "Epoch 10: loss on test set: 0.6527\n",
      "TRAINING ON FRAME #45\n",
      "Epoch  1: loss on final training batch: 0.7183\n",
      "Epoch  2: loss on final training batch: 0.7102\n",
      "Epoch  3: loss on final training batch: 0.7032\n",
      "Epoch  4: loss on final training batch: 0.6976\n",
      "Epoch  5: loss on final training batch: 0.6922\n",
      "Epoch  6: loss on final training batch: 0.6868\n",
      "Epoch  7: loss on final training batch: 0.6816\n",
      "Epoch  8: loss on final training batch: 0.6770\n",
      "Epoch  9: loss on final training batch: 0.6728\n",
      "Epoch 10: loss on final training batch: 0.6689\n",
      "Epoch 10: loss on test set: 0.6645\n",
      "TRAINING ON FRAME #46\n",
      "Epoch  1: loss on final training batch: 0.7130\n",
      "Epoch  2: loss on final training batch: 0.7088\n",
      "Epoch  3: loss on final training batch: 0.7052\n",
      "Epoch  4: loss on final training batch: 0.7026\n",
      "Epoch  5: loss on final training batch: 0.6997\n",
      "Epoch  6: loss on final training batch: 0.6956\n",
      "Epoch  7: loss on final training batch: 0.6902\n",
      "Epoch  8: loss on final training batch: 0.6841\n",
      "Epoch  9: loss on final training batch: 0.6780\n",
      "Epoch 10: loss on final training batch: 0.6724\n",
      "Epoch 10: loss on test set: 0.6804\n",
      "TRAINING ON FRAME #47\n",
      "Epoch  1: loss on final training batch: 0.7235\n",
      "Epoch  2: loss on final training batch: 0.7171\n",
      "Epoch  3: loss on final training batch: 0.7092\n",
      "Epoch  4: loss on final training batch: 0.7022\n",
      "Epoch  5: loss on final training batch: 0.6972\n",
      "Epoch  6: loss on final training batch: 0.6940\n",
      "Epoch  7: loss on final training batch: 0.6919\n",
      "Epoch  8: loss on final training batch: 0.6903\n",
      "Epoch  9: loss on final training batch: 0.6890\n",
      "Epoch 10: loss on final training batch: 0.6878\n",
      "Epoch 10: loss on test set: 0.6816\n",
      "TRAINING ON FRAME #48\n",
      "Epoch  1: loss on final training batch: 0.7293\n",
      "Epoch  2: loss on final training batch: 0.7259\n",
      "Epoch  3: loss on final training batch: 0.7213\n",
      "Epoch  4: loss on final training batch: 0.7168\n",
      "Epoch  5: loss on final training batch: 0.7130\n",
      "Epoch  6: loss on final training batch: 0.7105\n",
      "Epoch  7: loss on final training batch: 0.7091\n",
      "Epoch  8: loss on final training batch: 0.7086\n",
      "Epoch  9: loss on final training batch: 0.7084\n",
      "Epoch 10: loss on final training batch: 0.7081\n",
      "Epoch 10: loss on test set: 0.6933\n",
      "TRAINING ON FRAME #49\n",
      "Epoch  1: loss on final training batch: 0.7190\n",
      "Epoch  2: loss on final training batch: 0.7170\n",
      "Epoch  3: loss on final training batch: 0.7142\n",
      "Epoch  4: loss on final training batch: 0.7114\n",
      "Epoch  5: loss on final training batch: 0.7091\n",
      "Epoch  6: loss on final training batch: 0.7074\n",
      "Epoch  7: loss on final training batch: 0.7063\n",
      "Epoch  8: loss on final training batch: 0.7056\n",
      "Epoch  9: loss on final training batch: 0.7051\n",
      "Epoch 10: loss on final training batch: 0.7046\n",
      "Epoch 10: loss on test set: 0.6987\n",
      "TRAINING ON FRAME #50\n",
      "Epoch  1: loss on final training batch: 0.8427\n",
      "Epoch  2: loss on final training batch: 0.8030\n",
      "Epoch  3: loss on final training batch: 0.7396\n",
      "Epoch  4: loss on final training batch: 0.6667\n",
      "Epoch  5: loss on final training batch: 0.5958\n",
      "Epoch  6: loss on final training batch: 0.5340\n",
      "Epoch  7: loss on final training batch: 0.4844\n",
      "Epoch  8: loss on final training batch: 0.4469\n",
      "Epoch  9: loss on final training batch: 0.4196\n",
      "Epoch 10: loss on final training batch: 0.4006\n",
      "Epoch 10: loss on test set: 0.8060\n",
      "TRAINING ON FRAME #51\n",
      "Epoch  1: loss on final training batch: 0.7433\n",
      "Epoch  2: loss on final training batch: 0.7423\n",
      "Epoch  3: loss on final training batch: 0.7404\n",
      "Epoch  4: loss on final training batch: 0.7378\n",
      "Epoch  5: loss on final training batch: 0.7347\n",
      "Epoch  6: loss on final training batch: 0.7313\n",
      "Epoch  7: loss on final training batch: 0.7278\n",
      "Epoch  8: loss on final training batch: 0.7242\n",
      "Epoch  9: loss on final training batch: 0.7208\n",
      "Epoch 10: loss on final training batch: 0.7176\n",
      "Epoch 10: loss on test set: 0.7081\n",
      "TRAINING ON FRAME #52\n",
      "Epoch  1: loss on final training batch: 0.7458\n",
      "Epoch  2: loss on final training batch: 0.7446\n",
      "Epoch  3: loss on final training batch: 0.7423\n",
      "Epoch  4: loss on final training batch: 0.7392\n",
      "Epoch  5: loss on final training batch: 0.7356\n",
      "Epoch  6: loss on final training batch: 0.7316\n",
      "Epoch  7: loss on final training batch: 0.7274\n",
      "Epoch  8: loss on final training batch: 0.7233\n",
      "Epoch  9: loss on final training batch: 0.7194\n",
      "Epoch 10: loss on final training batch: 0.7157\n",
      "Epoch 10: loss on test set: 0.7117\n",
      "TRAINING ON FRAME #53\n",
      "Epoch  1: loss on final training batch: 0.7361\n",
      "Epoch  2: loss on final training batch: 0.7340\n",
      "Epoch  3: loss on final training batch: 0.7300\n",
      "Epoch  4: loss on final training batch: 0.7247\n",
      "Epoch  5: loss on final training batch: 0.7183\n",
      "Epoch  6: loss on final training batch: 0.7115\n",
      "Epoch  7: loss on final training batch: 0.7044\n",
      "Epoch  8: loss on final training batch: 0.6976\n",
      "Epoch  9: loss on final training batch: 0.6911\n",
      "Epoch 10: loss on final training batch: 0.6852\n",
      "Epoch 10: loss on test set: 0.7297\n",
      "TRAINING ON FRAME #54\n",
      "Epoch  1: loss on final training batch: 0.7285\n",
      "Epoch  2: loss on final training batch: 0.7263\n",
      "Epoch  3: loss on final training batch: 0.7222\n",
      "Epoch  4: loss on final training batch: 0.7166\n",
      "Epoch  5: loss on final training batch: 0.7100\n",
      "Epoch  6: loss on final training batch: 0.7026\n",
      "Epoch  7: loss on final training batch: 0.6949\n",
      "Epoch  8: loss on final training batch: 0.6872\n",
      "Epoch  9: loss on final training batch: 0.6797\n",
      "Epoch 10: loss on final training batch: 0.6727\n",
      "Epoch 10: loss on test set: 0.7407\n",
      "TRAINING ON FRAME #55\n",
      "Epoch  1: loss on final training batch: 0.7730\n",
      "Epoch  2: loss on final training batch: 0.7696\n",
      "Epoch  3: loss on final training batch: 0.7632\n",
      "Epoch  4: loss on final training batch: 0.7545\n",
      "Epoch  5: loss on final training batch: 0.7443\n",
      "Epoch  6: loss on final training batch: 0.7331\n",
      "Epoch  7: loss on final training batch: 0.7216\n",
      "Epoch  8: loss on final training batch: 0.7103\n",
      "Epoch  9: loss on final training batch: 0.6996\n",
      "Epoch 10: loss on final training batch: 0.6899\n",
      "Epoch 10: loss on test set: 0.6802\n",
      "TRAINING ON FRAME #56\n",
      "Epoch  1: loss on final training batch: 0.7485\n",
      "Epoch  2: loss on final training batch: 0.7461\n",
      "Epoch  3: loss on final training batch: 0.7417\n",
      "Epoch  4: loss on final training batch: 0.7356\n",
      "Epoch  5: loss on final training batch: 0.7282\n",
      "Epoch  6: loss on final training batch: 0.7199\n",
      "Epoch  7: loss on final training batch: 0.7110\n",
      "Epoch  8: loss on final training batch: 0.7018\n",
      "Epoch  9: loss on final training batch: 0.6926\n",
      "Epoch 10: loss on final training batch: 0.6835\n",
      "Epoch 10: loss on test set: 0.7004\n",
      "TRAINING ON FRAME #57\n",
      "Epoch  1: loss on final training batch: 0.7561\n",
      "Epoch  2: loss on final training batch: 0.7519\n",
      "Epoch  3: loss on final training batch: 0.7441\n",
      "Epoch  4: loss on final training batch: 0.7335\n",
      "Epoch  5: loss on final training batch: 0.7209\n",
      "Epoch  6: loss on final training batch: 0.7072\n",
      "Epoch  7: loss on final training batch: 0.6929\n",
      "Epoch  8: loss on final training batch: 0.6788\n",
      "Epoch  9: loss on final training batch: 0.6653\n",
      "Epoch 10: loss on final training batch: 0.6528\n",
      "Epoch 10: loss on test set: 0.6037\n",
      "TRAINING ON FRAME #58\n",
      "Epoch  1: loss on final training batch: 0.8262\n",
      "Epoch  2: loss on final training batch: 0.8203\n",
      "Epoch  3: loss on final training batch: 0.8093\n",
      "Epoch  4: loss on final training batch: 0.7938\n",
      "Epoch  5: loss on final training batch: 0.7747\n",
      "Epoch  6: loss on final training batch: 0.7526\n",
      "Epoch  7: loss on final training batch: 0.7283\n",
      "Epoch  8: loss on final training batch: 0.7024\n",
      "Epoch  9: loss on final training batch: 0.6755\n",
      "Epoch 10: loss on final training batch: 0.6481\n",
      "Epoch 10: loss on test set: 0.6119\n",
      "TRAINING ON FRAME #59\n",
      "Epoch  1: loss on final training batch: 0.8407\n",
      "Epoch  2: loss on final training batch: 0.8342\n",
      "Epoch  3: loss on final training batch: 0.8220\n",
      "Epoch  4: loss on final training batch: 0.8050\n",
      "Epoch  5: loss on final training batch: 0.7840\n",
      "Epoch  6: loss on final training batch: 0.7599\n",
      "Epoch  7: loss on final training batch: 0.7333\n",
      "Epoch  8: loss on final training batch: 0.7051\n",
      "Epoch  9: loss on final training batch: 0.6759\n",
      "Epoch 10: loss on final training batch: 0.6463\n",
      "Epoch 10: loss on test set: 0.6200\n",
      "TRAINING ON FRAME #60\n",
      "Epoch  1: loss on final training batch: 0.6220\n",
      "Epoch  2: loss on final training batch: 0.6177\n",
      "Epoch  3: loss on final training batch: 0.6097\n",
      "Epoch  4: loss on final training batch: 0.5984\n",
      "Epoch  5: loss on final training batch: 0.5843\n",
      "Epoch  6: loss on final training batch: 0.5681\n",
      "Epoch  7: loss on final training batch: 0.5500\n",
      "Epoch  8: loss on final training batch: 0.5307\n",
      "Epoch  9: loss on final training batch: 0.5104\n",
      "Epoch 10: loss on final training batch: 0.4896\n",
      "Epoch 10: loss on test set: 0.2999\n",
      "TRAINING ON FRAME #61\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "With n_samples=1, test_size=0.33 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-dafff6e8ad2f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_along_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombine\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0mtrain_neural_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-43-f213ccb1549b>\u001b[0m in \u001b[0;36mtrain_neural_network\u001b[0;34m(X, y)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# Split the data 2/3 to 1/3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mX_trn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_tst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_trn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_tst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.33\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;31m# Scale the data with MinMax to avoid negative values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mscaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMinMaxScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2421\u001b[0m     \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2422\u001b[0;31m     n_train, n_test = _validate_shuffle_split(\n\u001b[0m\u001b[1;32m   2423\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault_test_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2424\u001b[0m     )\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36m_validate_shuffle_split\u001b[0;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[1;32m   2096\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2097\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mn_train\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2098\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m   2099\u001b[0m             \u001b[0;34m\"With n_samples={}, test_size={} and train_size={}, the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2100\u001b[0m             \u001b[0;34m\"resulting train set will be empty. Adjust any of the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: With n_samples=1, test_size=0.33 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
     ]
    }
   ],
   "source": [
    "# print(df.keys())\n",
    "for current_frame in df['frame'].unique():\n",
    "    print(\"TRAINING ON FRAME #%i\" % current_frame)\n",
    "    # Get the values of X and y for a given frame\n",
    "    X = df[df.frame==current_frame][[\n",
    "        'blue_total_kills',\n",
    "        'blue_total_gold',\n",
    "        'blue_total_cs',\n",
    "        'blue_total_damage',\n",
    "        'blue_towers', \n",
    "        'blue_plates',\n",
    "        'blue_inhibitors', \n",
    "        'blue_barons', \n",
    "        'blue_dragons', \n",
    "        'blue_rift_heralds',\n",
    "        'red_total_kills', \n",
    "        'red_total_gold', \n",
    "        'red_total_cs', \n",
    "        'red_total_damage',\n",
    "        'red_towers', \n",
    "        'red_plates', \n",
    "        'red_inhibitors', \n",
    "        'red_barons',\n",
    "        'red_dragons', \n",
    "        'red_rift_heralds',\n",
    "    ]].values\n",
    "    y = df[df.frame==current_frame]['winning_team'].values\n",
    "\n",
    "    # Convert -1 and +1 to 0 and 1 for the tensors\n",
    "    y = np.interp(y, (-1,+1), (0, 1)).astype(np.int32)\n",
    "\n",
    "    # Reshape the values one next to eachother\n",
    "    def combine(x):\n",
    "        x = sklearn.preprocessing.normalize(x.reshape(2,10), norm='l1', axis=0)\n",
    "        return x[0,:] - x[1,:]\n",
    "    # Normalize the data to have a set of 10 features that better correspond to a better comparison\n",
    "    X = np.apply_along_axis(combine, 1, X)\n",
    "\n",
    "    train_neural_network(X, y)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
